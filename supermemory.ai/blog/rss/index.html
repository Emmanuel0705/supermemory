<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[supermemory - Blog]]></title><description><![CDATA[How we build supermemory - best memory engine on the planet.]]></description><link>https://blog.supermemory.ai/</link><image><url>https://blog.supermemory.ai/favicon.png</url><title>supermemory - Blog</title><link>https://blog.supermemory.ai/</link></image><generator>Ghost 5.130</generator><lastBuildDate>Fri, 01 Aug 2025 01:32:54 GMT</lastBuildDate><atom:link href="https://blog.supermemory.ai/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Unified Memory That Works Where You Work: Your Second Brain With Supermemory]]></title><description><![CDATA[<p>Hi everyone, I&#x2019;m Dhravya, the founder of Supermemory.</p><p>I want to start with a little story behind why this product means so much to me. You can also skip straight to what it is and how it works below.&#xA0;</p><p>Anyways, when I started Supermemory one year ago,</p>]]></description><link>https://blog.supermemory.ai/unified-memory-that-works-where-you-work-your-second-brain-with-supermemory/</link><guid isPermaLink="false">6883ade2e3025a0001da6199</guid><dc:creator><![CDATA[Dhravya Shah]]></dc:creator><pubDate>Fri, 25 Jul 2025 17:16:18 GMT</pubDate><media:content url="https://blog.supermemory.ai/content/images/2025/07/Frame_2147223412.webp" medium="image"/><content:encoded><![CDATA[<img src="https://blog.supermemory.ai/content/images/2025/07/Frame_2147223412.webp" alt="Unified Memory That Works Where You Work: Your Second Brain With Supermemory"><p>Hi everyone, I&#x2019;m Dhravya, the founder of Supermemory.</p><p>I want to start with a little story behind why this product means so much to me. You can also skip straight to what it is and how it works below.&#xA0;</p><p>Anyways, when I started Supermemory one year ago, we were a consumer app that acted like a second brain. You could add your memories, documents, and notes, and chat with them.</p><p>We had a big community and a shit ton of users, but, frankly, our product&#x2019;s retention was terrible. People stopped using it, and there were still several technological challenges I had to figure out.</p><p>So, we pivoted to B2B and set our heads down for one year.&#xA0;</p><p>Now, we have a lot of businesses using Supermemory to build memory into their apps, and in the process, we figured out some of the hardest underlying technological problems. You can look at our releases of the <a href="https://supermemory.ai/blog/the-ux-and-technicalities-of-awesome-mcps/?ref=blog.supermemory.ai" rel="noreferrer">MCP</a>, <a href="https://supermemory.ai/blog/we-solved-ai-api-interoperability/?ref=blog.supermemory.ai" rel="noreferrer">llm-bridge</a>, <a href="https://blog.supermemory.ai/faster-smarter-reliable-infinite-chat-supermemory-is-context-engineering/" rel="noreferrer">infinite chat</a>, and more.</p><p>I was finally starting to see AI memory in the form I imagined it to be.&#xA0;</p><p>Although watching the revenue graph on my Stripe dashboard go berserk is very fun, I felt it was time to go after our initial vision: a memory engine that <em>automatically </em>(pay attention to this) stores information from all your apps, like Google Drive, Notion, Claude, ChatGPT, etc., organizes them into sexy graphs, and allows you to chat with them.&#xA0;</p><p>This launch has been a whole year in the making. And, I think the Dhravya of one year ago would be proud as hell.&#xA0;</p><p>This kind of memory has always been my real mission: bringing effortless knowledge capture to everyone, changing the way we live, work, and think. Selling B2B licenses is a noble pursuit, lol, but <em>this</em> is what I live for.</p><p>And today, I&#x2019;m beyond excited to finally fulfill that promise.</p><p>Thanks for sticking around. I can&#x2019;t wait for you to try it.</p><p>Best,</p><p>Dhravya</p><h2 id="what-is-unified-memory-and-how-does-it-work">What is unified memory, and how does it work?</h2><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfFzD0j1ie-1TVHTV5H2EbTK7ov26xcVggQoqbeOQ20jk3F42UppYbDFtXgfE-8NLXCuOmaoWzoM-LdInZW1KPLotnvQoikZr3v-ohCZFp5vIRqE2S3r-kr0RkA1ub7Mx3SUacp3w?key=cM0AgtU4PqZfJJjReCqVcQ" class="kg-image" alt="Unified Memory That Works Where You Work: Your Second Brain With Supermemory" loading="lazy" width="624" height="337"><figcaption><span style="white-space: pre-wrap;">The app&apos;s dashboard</span></figcaption></figure><p>When I first launched Supermemory a year ago, the biggest flaw and the reason we saw so many people stop using it was that it relied entirely on manual input.</p><p>You literally had to open our app, manually add memories or documents, and then actively chat with them later.</p><p>Over the past year, I realized something crucial: memory only works when it&#x2019;s automatic. When it lives quietly in the background, seamlessly integrated into the way you already work.</p><blockquote>Memory only works when it&#x2019;s automatic. When it lives quietly in the background, seamlessly integrated into the way you already work.</blockquote><p>Here&#x2019;s what you can expect from our app:</p><ul><li><strong>Automatic, real-time memory updates:</strong> The app now lives where your AI does. It automatically captures and updates your interactions from ChatGPT, Claude, Cursor, or any other AI client supporting MCP in real-time.</li><li><strong>Effortless document integration:</strong> Connect seamlessly to Google Drive, Notion, and OneDrive using our connectors, instantly importing your documents and making them available for immediate conversation. <a href="https://blog.supermemory.ai/building-an-ai-compliance-chatbot-with-supermemory-and-google-drive/" rel="noreferrer">Learn how.</a></li><li><strong>Intelligent knowledge graphs:</strong> Automatically create intuitive, visual graphs of your knowledge, interactions, and memories, bringing clarity to the way you think and remember.</li><li><strong>Project-based organization:</strong> Easily segment your memories into different projects, securely isolating work from personal knowledge, or dividing topics clearly, however you see fit.</li></ul><p>Unlike other apps, the Unified Memory Engine is designed to be accessible for normal everyday users like you and me, and not just developers. For instance, platforms like OpenMemory by mem0 require a complex Docker setup, familiarity with Python, and technical know-how to even get started.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXebLf7x4AjbjQIa8ZYzq0qUG-kkzOCV4lpsi1_fHVOGvHwnlF-4I3GwrIDyUM3tuOOOi1K_L4u2D08YmIE7AFyxkcf61eYmceF6WRPcVZXDCAWWuahXHIWKAqq-uVlJLaDSR-PPpw?key=cM0AgtU4PqZfJJjReCqVcQ" class="kg-image" alt="Unified Memory That Works Where You Work: Your Second Brain With Supermemory" loading="lazy" width="624" height="423"><figcaption><span style="white-space: pre-wrap;">A demo graph we created</span></figcaption></figure><p>Arguably, the graph feature is one of our coolest additions. Right now, for every raw document that is added, Supermemory not only understands and extracts things from the document to search through them, but also makes connections between two or three items.</p><p>It is also capable of making implicit connections - stuff that the user never explicitly said, but Supermemory understood.</p><p>For you, the benefit is that our system is MUCH smarter than everything else out there. You can move beyond surface-level questions like &#x2018;when did xyz happen,&#x2019; and do actual complex analysis and querying.</p><p>The app also forgets memories that aren&#x2019;t used very frequently, <a href="https://blog.supermemory.ai/memory-engine/" rel="noreferrer">based on how our brain works</a>. Other memory systems don&#x2019;t have that temporal understanding, and this intelligent understanding of when to overwrite and rewrite things makes for faster performance.</p><p>I also talked to the founder of <a href="http://spawn.co/?ref=blog.supermemory.ai"><u>spawn.co</u></a> and he told me they do this at their company. They&#x2019;ve been racking every interaction on Slack and documents on Notion, adding it to the company&#x2019;s knowledge graph.&#xA0;</p><h2 id="how-to-use-the-app">How to use the app?</h2><p>We&#x2019;re releasing the app to only a select few people right now. To sign up for the waitlist, head over to <a href="http://app.supermemory.ai/?ref=blog.supermemory.ai"><u>app.supermemory.ai</u></a></p><p>Once you&#x2019;re in, start by manually adding memories in the form of notes, links, files, or connections to Google Drive, Notion, or OneDrive.</p><figure class="kg-card kg-image-card"><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXdTBmu1SirZ340WfJ4xbtzg9nqpUahgw_OjJ6virNmqBatFxXvbU950DJZBv7ulcWWorK1Kml-BGy1XbyqWD_TUc9aSfESY__HTMz77eBgDNgcDlriLmM5cHhDS6ZrMyUf7d-RO8Q?key=cM0AgtU4PqZfJJjReCqVcQ" class="kg-image" alt="Unified Memory That Works Where You Work: Your Second Brain With Supermemory" loading="lazy" width="624" height="232"></figure><p>You can also install it using the MCP for automatic syncing. For Claude Desktop, follow the following steps:</p><ol><li>Open the <strong>Claude Desktop app</strong>.</li><li>Go to <strong>Developer &#x2192; Edit Config</strong> to open or generate the file:</li></ol><ul><li>macOS: ~/Library/Application Support/Claude/claude_desktop_config.json</li><li>Windows: %APPDATA%\Claude\claude_desktop_config.json<a href="https://modelcontextprotocol.io/quickstart/user?utm_source=chatgpt.com"><u> </u></a></li></ul><ol start="3"><li>Add the Supermemory MCP server entry under &quot;mcpServers&quot;. For example:</li></ol><pre><code>&quot;mcpServers&quot;: {
  &quot;my-mcp&quot;: {
    &quot;command&quot;: &quot;npx&quot;,
    &quot;args&quot;: [ &quot;-y&quot;, &quot;mcp-remote&quot;, &quot;https://api.supermemory.ai/mcp&quot; ]
  }
}
</code></pre><ol start="4"><li>Save the file and restart Claude Desktop. After reboot, your MCP server should be available in the UI.</li></ol><p>Here&apos;s how it works:</p><figure class="kg-card kg-image-card"><img src="https://blog.supermemory.ai/content/images/2025/07/image-5.png" class="kg-image" alt="Unified Memory That Works Where You Work: Your Second Brain With Supermemory" loading="lazy" width="1850" height="1208" srcset="https://blog.supermemory.ai/content/images/size/w600/2025/07/image-5.png 600w, https://blog.supermemory.ai/content/images/size/w1000/2025/07/image-5.png 1000w, https://blog.supermemory.ai/content/images/size/w1600/2025/07/image-5.png 1600w, https://blog.supermemory.ai/content/images/2025/07/image-5.png 1850w" sizes="(min-width: 720px) 720px"></figure><p>You can also add it to Cursor with just one click.</p><h2 id="pricing">Pricing</h2><p>The free plan allows you to create up to 10 memories and has basic search. For 500 memories, 10 connections, advanced search, and priority support, the price is $9/month for the first 100 users.</p><h2 id="what%E2%80%99s-next">What&#x2019;s next?</h2><p>This launch is just the beginning. Next up, we&#x2019;re working on a browser extension to make capturing memories across the web feel super natural. You can carry your memory engine with you everywhere.</p><p>Feel free to send us your feedback or any ideas that you&#x2019;d like to see implemented.</p><p>If you&#x2019;ve made it this far, thank you. I genuinely can&#x2019;t wait to see what you build, recall, and rediscover with it. <a href="https://app.supermemory.ai/?ref=blog.supermemory.ai" rel="noreferrer">Sign up for a free account now!</a></p>]]></content:encoded></item><item><title><![CDATA[Building an AI Compliance Chatbot With Supermemory and Google Drive]]></title><description><![CDATA[<p>Contract compliance reviews are a serious drain on time and focus. It&#x2019;s a repetitive process that takes away from actual legal thinking, and the workflow is absolutely broken.</p><p>Files live in different places. You&#x2019;re never sure if you&#x2019;re reading the latest version. And no</p>]]></description><link>https://blog.supermemory.ai/building-an-ai-compliance-chatbot-with-supermemory-and-google-drive/</link><guid isPermaLink="false">687d4a1e8578750001471aeb</guid><dc:creator><![CDATA[Naman Bansal]]></dc:creator><pubDate>Sun, 20 Jul 2025 20:18:56 GMT</pubDate><media:content url="https://blog.supermemory.ai/content/images/2025/07/Frame_2147223301.webp" medium="image"/><content:encoded><![CDATA[<img src="https://blog.supermemory.ai/content/images/2025/07/Frame_2147223301.webp" alt="Building an AI Compliance Chatbot With Supermemory and Google Drive"><p>Contract compliance reviews are a serious drain on time and focus. It&#x2019;s a repetitive process that takes away from actual legal thinking, and the workflow is absolutely broken.</p><p>Files live in different places. You&#x2019;re never sure if you&#x2019;re reading the latest version. And no one has time to manually track every obligation, clause, or renewal date buried inside a hundred PDFs.</p><p>But what if you had an AI assistant to automatically handle it? One that connects directly to your (or your client&#x2019;s) Google Drive, stays in sync automatically, and lets you chat with your documents.</p><p>In this guide, we&#x2019;ll walk through building that AI assistant. It connects directly to your Google Drive, keeps itself updated as documents change, and lets you ask questions or run checks over the entire set of contracts without manual uploads or setup.</p><p>You&#x2019;ll use<a href="https://supermemory.ai/?ref=blog.supermemory.ai"><u> Supermemory</u></a> to handle document syncing and memory, with a TypeScript backend and a Vite React frontend. You can also access all the code here:<a href="https://github.com/namancoderpro/ai-drive-chat?ref=blog.supermemory.ai"><u> GitHub repo</u></a></p><h2 id="planning-the-app">Planning the app</h2><p>Before jumping into the code, it helps to get clear on what we&#x2019;re actually building and why. Here&#x2019;s how I thought through the design:</p><h3 id="1-what%E2%80%99s-the-assistant-actually-doing">1. What&#x2019;s the Assistant Actually Doing?</h3><p>At a high level, it needs to:</p><ul><li>Connect to a Google Drive and access all relevant legal docs</li><li>Automatically sync new or updated files</li><li>Let the user ask questions about any of the documents</li><li>Respond using context pulled from those files, plus memory of past chats</li></ul><p>So the three core jobs are: connect, sync, and chat.</p><h3 id="2-why-supermemory">2. Why Supermemory?</h3><p>Most &#x201C;chat with your documents&#x201D; setups require manually uploading files or rebuilding context every time. I wanted something that just stayed in sync without needing to re-upload or reprocess every time someone edits a contract.</p><p>Supermemory handles all that for me. It:</p><ul><li>Connects directly to Google Drive</li><li>Watches for file changes and syncs them</li><li>Adds memory and document context automatically to each AI call</li></ul><p>That&#x2019;s huge because I don&#x2019;t have to manually store, chunk, or retrieve anything.</p><h3 id="3-tools-breakdown">3. Tools Breakdown</h3><p>Here&#x2019;s how I picked the stack:</p><ul><li><strong>Memory + File Syncing:</strong> Supermemory handles both, so no need to build my own database or embedding system.</li><li><strong>Chat + AI Responses:</strong> OpenAI (GPT-4o) via Supermemory, so memory is already baked into the API call.</li><li><strong>Backend:</strong> TypeScript with Express as it&#x2019;s simple and flexible for API routes.</li><li><strong>Frontend:</strong> Vite + React + Tailwind because it&#x2019;s fast to build and easy to style.</li></ul><h3 id="4-assistant-workflow">4. Assistant Workflow</h3><p>Here&#x2019;s the basic flow this app follows:</p><ol><li><strong>Connect</strong> &#x2013; User clicks a button &#x2192; initiates Google Drive OAuth via Supermemory</li><li><strong>Sync</strong> &#x2013; After uploading contracts, user clicks sync &#x2192; Supermemory indexes the new files</li><li><strong>Chat</strong> &#x2013; User sends a message &#x2192; app hits the /chat API &#x2192; Supermemory auto-adds document context &#x2192; GPT-4o generates a useful, formatted reply</li></ol><p>That&#x2019;s the thinking. Now let&#x2019;s dive right into the code.</p><h2 id="building-the-app-step-by-step">Building the app: step-by-step</h2><h3 id="project-setup">Project Setup</h3><p>The first step in the tutorial is to set up our project. First, create a general directory that will house the project&apos;s backend and frontend files.</p><pre><code>mkdir compliance-checker-assistant
</code></pre>
<p>Navigate to the compliance-checker-assistant directory in your terminal to continue with the tutorial.</p><pre><code>cd compliance-checker-assistant
</code></pre>
<h3 id="create-assistant-backend">Create Assistant Backend</h3><p>To set up our AI assistant backend. First, add a backend folder to your compliance-checker-assistant folder and navigate to the folder.</p><pre><code>mkdir backend
cd backend
</code></pre>
<p>Initialize Node in the backend folder.</p><pre><code>npm init -y
</code></pre>
<p>For the backend, we will be installing the following packages.&#xA0;</p><ol><li>express:&#xA0; To create our backend routes</li><li>dotenv:&#xA0; To pass env variables</li><li>openai:&#xA0; The OpenAI Typescript SDK</li><li>supermemory: Supermemory AI Typescript SDK</li></ol><p>Install the required packages with the following command -</p><pre><code>npm install express supermemory openai
</code></pre>
<p>Install TypeScript and add types for Express.js and Node.</p><pre><code>npm install --save-dev typescript @types/node @types/express
</code></pre>
<p>Initialize TypeScript:</p><pre><code>npx tsc --init
</code></pre>
<p>This command will add a tsconfig.json file to your backend folder. Open the tsconfig.json file and add the following code to it:</p><pre><code>&quot;outDir&quot;: &quot;./dist/&quot;,
</code></pre>
<p>Add the following scripts to your package.json file.</p><pre><code>&quot;scripts&quot;: {
   &quot;build&quot;: &quot;tsc&quot;,
   &quot;start&quot;: &quot;node dist/app.js&quot;,
   &quot;dev&quot;: &quot;ts-node src/app.ts&quot;
 },
</code></pre>
<p>Create a .env file and add the following keys and their values to the file.</p><pre><code>SUPERMEMORY_API_KEY=
OPENAI_API_KEY=
</code></pre>
<p>For instructions on how to retrieve your Supermemory AI api key, see - https://supermemory.ai/docs/model-enhancement/getting-started#get-your-supermemory-api-key</p><p>For instructions on how to retrieve your OpenAI&#xA0; api key, see - https://platform.openai.com/api-keys</p><h3 id="create-your-assistant-server">Create your Assistant server</h3><p>After setting up our backend, the next step is to create the assistant server. For this tutorial, we will be adding three routes to our backend.</p><ol><li>api/connect - This route will be used to connect to our Google Drive application with Supermemory AI. This will enable Supermemory AI to access the contract and compliance files stored in the drive.</li><li>api/sync - This route will be used to sync Supermemory AI and our Google Drive. This is useful when we add new files to our drive and we need to keep our Supermemory files up to date.</li><li>api/chat - This route will be used to chat with our AI assistant.&#xA0;</li></ol><p>Add a src folder to your backend folder, and add an app.ts file to the folder.</p><pre><code>mkdir src
touch app.ts
</code></pre>
<p>Add the following code sample to the app.ts&#xA0; file</p><pre><code>
import express from &quot;express&quot;;
import &quot;dotenv/config&quot;;
import chatRoutes from &quot;./routes/chatRoutes&quot;;

const app = express();
const PORT = 3100;

app.use(express.json());

app.use(&quot;/api&quot;, chatRoutes);

app.listen(PORT, () =&gt; {
 console.log(`Server running on http://localhost:${PORT}`);
});
</code></pre>
<p>In this code, we created our backend server using Express.js. We imported our .env file parameters using dotenv, imported our routes, and configured our server to listen on port 3100.&#xA0;</p><p>To create the required routes, add a routes folder to your src folder and add a chatRoutes.ts file to the folder.</p><pre><code>mkdir routes
touch routes/chatRoutes.ts
</code></pre>
<p>Add the following code to the chatRoutes.ts file</p><pre><code>
import express, {
 Request,
 Response,
 NextFunction,
 RequestHandler,
} from &quot;express&quot;;
import Supermemory from &quot;supermemory&quot;;
import OpenAI from &quot;openai&quot;;
const router = express.Router();
const client = new Supermemory({
 apiKey: process.env[&quot;SUPERMEMORY_API_KEY&quot;], // This is the default and can be omitted
});

const openai = new OpenAI({
 apiKey: process.env.OPENAI_API_KEY,
 baseURL: &quot;https://api.supermemory.ai/v3/https://api.openai.com/v1&quot;,
 defaultHeaders: {
   &quot;x-api-key&quot;: process.env.SUPERMEMORY_API_KEY,
 },
});
</code></pre>
<p>In this code, we created an Express router, initialized a Supermemory client, and an OpenAI client. Note the OpenAI baseUrl and default headers; with this configuration, Supermemory automatically appends relevant memory and context to our conversation when interacting with OpenAI APIs.</p><p>Add the following code to the chatRoutes.ts file to create the api/connect route -</p><pre><code>router.get(&quot;/connect&quot;, async (req: Request, res: Response) =&gt; {
 try {
   const connection = await client.connections.create(&quot;google-drive&quot;, {
     redirectUrl: &quot;http://localhost:5173&quot;,
   });
   res.status(201).json({
     message: &quot;authenticate&quot;,
     url: connection.authLink,
   });
 } catch (error) {
   console.error(&quot;Connection creation failed:&quot;, error);
   res.status(500).json({
     message: &quot;Failed to initiate connection&quot;,
     error: (error as Error).message,
   });
 }
});
</code></pre>
<p>First, we created a new Google Drive connection using the Suprememory SDK. An authentication link is then collected from the response and sent back as a response for the API.&#xA0;</p><p>Note that we added a redirectUrl parameter to the request. The authentication link will redirect to the redirect URL after the authentication process is completed.</p><p>Add the following code to the chatRoutes.ts file to create the api/sync route -</p><pre><code>router.get(&quot;/sync&quot;, async (req: Request, res: Response) =&gt; {
 try {
   await client.connections.import(&quot;google-drive&quot;);
   res.status(201).json({
     message: &quot;synced&quot;,
   });
 } catch (error) {
   console.error(&quot;Sync failed:&quot;, error);
   res.status(500).json({
     message: &quot;Failed to initiate sync&quot;,
     error: (error as Error).message,
   });
 }
});
</code></pre>
<p>This code performs a connection sync between Supermemory and Google Drive. After completing the sync, the api returns a `synced` response.</p><p>Add the following code to the chatRoutes.ts file to create the api/chat route -</p><pre><code>interface ChatRequestBody {
 message: string;
}
interface ChatResponse {
 response: string | null;
}
interface ErrorResponse {
 error: string;
}

const chatHandler: RequestHandler&lt;
 {},
 ChatResponse | ErrorResponse,
 ChatRequestBody
&gt; = async (req, res, next) =&gt; {
 const { message } = req.body;
 if (!message) {
   res
     .status(400)
     .json({ error: &apos;Request body must include a &quot;message&quot; field.&apos; });
   return;
 }

 try {
   const completion = await openai.chat.completions.create({
     model: &quot;gpt-4o&quot;,
     user: &quot;sheriff&quot;,
     messages: [
       {
         role: &quot;system&quot;,
         content:
           &quot;You are ContractGuard, an AI assistant for contract and compliance reviews. When given any contract text, you will:\n\n1. Highlight key obligations and risks.\n2. Point out missing or non-standard clauses.\n3. Recommend fixes or best-practice language.\n4. Always format your answer in Markdown with headings and bullet points.&quot;,
       },
       { role: &quot;user&quot;, content: message },
     ],
   });

   res.status(200).json({
     response: completion.choices[0].message.content,
   });
   return;
 } catch (err) {
   console.error(&quot;Chat failed:&quot;, err);
   res.status(500).json({ error: &quot;Failed to initiate AI chat.&quot; });
   return;
 }
};

router.post(&quot;/chat&quot;, chatHandler);
</code></pre>
<p>The `chatHandler` function first extracts the user&#x2019;s message from the request body. It then passes the system our AI assistant system&apos;s prompt and the extracted user message to the OpenAI SDK to get a response.&#xA0;</p><p>Note that we are not adding our previous conversation manually to the messages; this is handled automatically by Supermemory. In addition to that, Supermemory also handles the AI&#x2019;s interaction with your Google Drive files.</p><p>Add the following to the end of the chatRoutes.ts file to export your routes.</p><pre><code>export default router;</code></pre><p>Start your backend server with the following command.</p><pre><code>npm run build
npm run start</code></pre><p>You can access your backend server at http://localhost:3100.</p><h3 id="create-your-app%E2%80%99s-frontend">Create your app&#x2019;s frontend</h3><p>After completing the assistant&apos;s backend, the next step is to create a chat interface frontend for the assistant. We will be using Vite React, TypeScript, and Tailwind CSS for our frontend code.</p><p>Create a Vite React TypeScript project named frontend.</p><pre><code>npm create vite@latest frontend -- --template react-ts</code></pre><p>Navigate to the project and install the required dependencies</p><pre><code>cd frontend
npm install</code></pre><p>Install Tailwind CSS Vite</p><pre><code>npm install tailwindcss @tailwindcss/vite</code></pre><p>Open your vite.config.ts and update it to the following code.</p><pre><code>import tailwindcss from &apos;@tailwindcss/vite&apos;
import react from &apos;@vitejs/plugin-react&apos;

// https://vite.dev/config/
export default defineConfig({
 plugins: [react(), tailwindcss()],
 })
</code></pre><p>Open your src/index.css file and import Tailwind CSS at the top of the file</p><pre><code>@import &quot;tailwindcss&quot;;</code></pre><p>Start your frontend server.</p><pre><code>npm run dev</code></pre><p>You should be able to access your app at http://localhost:5173/.</p><p>The next step is to implement the chat interface, open your App.tsx file, and replace the code with the following code.</p><p>Let&#x2019;s start with the necessary imports:</p><pre><code>import type React from &apos;react&apos;

import { useState, useRef, useEffect } from &apos;react&apos;
import ReactMarkdown from &apos;react-markdown&apos;
import { Send, Bot, User, Loader2 } from &apos;lucide-react&apos;</code></pre><p>At the top of the file, we&#x2019;ll now define TypeScript interfaces for our chat messages and API responses:</p><pre><code class="language-javascript">interface Message {
 id: string
 role: &apos;user&apos; | &apos;assistant&apos;
 content: string
 timestamp: Date
}

interface ConnectResponse {
 message: string;
 url?: string;
 error?: string;
}

interface SyncResponse {
 message: string;
 error?: string;
}</code></pre><p>Since this project is built with TypeScript, defining interfaces upfront helps make the entire frontend codebase easier to reason about and catch bugs early. We only define a few interfaces here, but they&#x2019;re essential to how the chat system operates.</p>
<p>The <code>Message</code> interface represents a single message in the conversation. It includes an <code>id</code> (used as a unique key when rendering messages), a <code>role</code> (either&#xA0; <code>user</code> or <code>assistant</code> so we can style them differently), the <code>content</code> (which is the actual message text), and a <code>timestamp</code>.</p>
<p>Every time a user sends a message or the assistant replies, we create a new <code>Message</code> object and add it to our state.</p>
<p>The <code>ConnectResponse</code> and <code>SyncResponse</code> types are used to type-check the data we get back from the backend API when connecting to Google Drive or syncing files to avoid runtime errors.</p><p>Now, let&#x2019;s set up the states and refs:</p><pre><code>function App () {
 const [messages, setMessages] = useState&lt;Message[]&gt;([])
 const [input, setInput] = useState(&apos;&apos;)
 const [isLoading, setIsLoading] = useState(false)
 const messagesEndRef = useRef&lt;HTMLDivElement&gt;(null)</code></pre><p><code>messages</code> is the core of the chat experience. It&#x2019;s an array that holds every message in the order they were sent.</p>
<p><code>input</code> stores whatever the user is currently typing. As the user types in the input box, this gets updated in real-time.</p>
<p><code>isLoading</code> is a simple flag to tell us when the assistant is &#x201C;thinking.&#x201D;<br>
It&#x2019;s used to disable the input field and show a spinner in the chat UI so users don&#x2019;t accidentally submit multiple messages or get confused by lag.</p>
<p><code>messagesRef</code> is a reference to an invisible div at the bottom of the chat window. We use this to automatically scroll the chat down to the latest message every time a new one is added.</p>
<p>Next, we&#x2019;ll write functions to connect to Google Drive and Sync the content, sending requests to our backend APIs:</p>
<pre><code>const handleConnect = async () =&gt; {
   try {
     const res = await fetch(&apos;http://localhost:3100/api/connect&apos;, {
       method: &apos;GET&apos;,
       credentials: &apos;include&apos;,
     });
     const data: ConnectResponse = await res.json();

     if (!res.ok) {
       console.log(data.error ?? data.message);
     } else if (data.url) {
       // redirect to the OAuth URL
       window.location.href = data.url;
     }
   } catch (err) {
     console.log((err as Error).message);
   }
 };

 const handleSync = async () =&gt; {
   try {
     const res = await fetch(&apos;http://localhost:3100/api/sync&apos;, {
       method: &apos;GET&apos;,
       credentials: &apos;include&apos;,
     });
     const data: SyncResponse = await res.json();

     if (!res.ok) {
       console.log(data.error ?? data.message);
     } else {
       console.log(data.message);
     }
   } catch (err) {
     console.log((err as Error).message);
   } 
 };
</code></pre>
<p>The <code>handleConnect</code> function makes a GET request to our backend route at <code>/api/connect</code>, it gets the <code>authLink</code> returned from the backend, and redirects the user to it. This is how the user grants permissions to Supermemory to read our GDrive files.</p>
<p>The <code>handleSync</code> function explicitly tells Supermemory to sync to Google Drive. Supermemory automatically does it whenever a new doc is uploaded, but in case you need manual refreshing, this function will work.</p>
<p>It also sends a GET request to our backend /api/sync endpoint and when it&#x2019;s successfully done, we log a simple confirmation message <code>{message: &#x2018;synced&#x2019;}</code> in the console.</p>
<p>Now, let&#x2019;s handle the main logic of sending messages and receiving responses:</p>
<pre><code> const scrollToBottom = () =&gt; {
   messagesEndRef.current?.scrollIntoView({ behavior: &apos;smooth&apos; })
 }

 useEffect(() =&gt; {
   scrollToBottom()
 }, [messages])
</code></pre>
<p>The <code>scrollToBottom</code> function is a little helper that scrolls automatically down when a new message loads.</p>
<p>Now, the <code>chatHandler</code> is the main function that encompasses the logic of our app:</p>
<pre><code>const chatHandler: RequestHandler&lt;{}, ChatResponse | ErrorResponse, ChatRequestBody&gt; = async (req, res) =&gt; {
 const { message } = req.body;
 if (!message) {
   res.status(400).json({ error: &apos;Request body must include a &quot;message&quot; field.&apos; });
   return;
 }

 try {
   // Initial response: model decides if it should use the search tool
   const initial = await openai.responses.create({
     model: &quot;gpt-4o&quot;,
     instructions: `You are ContractGuard, an assistant for contract and compliance reviews.
Decide if you need to search documents to answer the user&apos;s question.
If yes, call the &apos;supermemory_search&apos; tool with the query parameter.
Otherwise, provide an answer directly.`,
     input: message,
     tools: [
       {
         type: &quot;function&quot;,
         name: &quot;supermemory_search&quot;,
         description: &quot;Search documents in Google Drive&quot;,
         strict: true,
         parameters: {
           type: &quot;object&quot;,
           properties: {
             query: {
               type: &quot;string&quot;,
               description: &quot;The search query for finding relevant documents&quot;
             },
           },
           required: [&quot;query&quot;],
           additionalProperties: false,
         },
       },
     ],
     tool_choice: &quot;auto&quot;,
   });

   // Check if there&apos;s a function call in the output
   const functionCall = initial.output.find(item =&gt; item.type === &quot;function_call&quot;);
  
   if (functionCall &amp;&amp; functionCall.name === &quot;supermemory_search&quot;) {
     const args = JSON.parse(functionCall.arguments || &quot;{}&quot;);
    
            // Perform the search using supermemory client
      let docs;
      try {
        // Use the correct Supermemory API method
        docs = await client.search.execute({
          q: args.query,
          limit: 5
        });
       
        console.log(`&#x1F50D; Search executed: &quot;${args.query}&quot;`);
      } catch (searchError) {
        console.error(&quot;Supermemory search failed:&quot;, searchError);
        res.status(500).json({ error: &quot;Failed to search documents. Please try again.&quot; });
        return;
      }

     // Second response: provide search results for final answer
     const followUp = await openai.responses.create({
       model: &quot;gpt-4o&quot;,
       instructions: `You are ContractGuard. Use the provided search results to answer the user&apos;s question about contracts and compliance.
Be specific and cite relevant information from the documents when possible.`,
       input: [
         {
           type: &quot;function_call&quot;,
           call_id: functionCall.call_id,
           name: functionCall.name,
           arguments: functionCall.arguments,
         },
         {
           type: &quot;function_call_output&quot;,
           call_id: functionCall.call_id,
           output: JSON.stringify(docs),
         },
         {
           role: &quot;user&quot;,
           content: message,
         }
       ],
     });
    
     res.status(200).json({
       response: followUp.output_text || &quot;I found some documents but couldn&apos;t generate a response. Please try rephrasing your question.&quot;
     });
     return;
   }

   // No tool call needed: return the direct answer
   res.status(200).json({
     response: initial.output_text || &quot;I&apos;m ready to help with contract and compliance questions. Please ask me anything!&quot;
   });
   return;

 } catch (err) {
   console.error(&quot;Chat handler error:&quot;, err);
   res.status(500).json({
     error: &quot;An error occurred while processing your request. Please try again.&quot;
   });
   return;
 }
};
</code></pre>
<p>First, as soon as a request hits &#x2018;/chat&#x2019;, we pul message out of the JSON body. If it&#x2019;s missing, we return a 400 error. Then, we use OpenAI&#x2019;s Responses API with three key pieces:</p>
<ul>
<li><code>instructions</code> that tell the model it&#x2019;s ContractGuard and should decide whether it needs to search the Drive documents.</li>
<li>A single tool definition named &#x2018;supermemory_search&#x2019;, which strictly accepts an object with a <code>query</code> string.</li>
<li><code>tool_choice: &#x201C;auto&#x201D;</code>, so the model can either answer directly or invoke our search tool.</li>
</ul>
<p>At this point, the model returns a sequence of output items, which may include a <code>function_call</code> if it thinks a search is required. If there&#x2019;s a function called <code>supermemory_search</code> in the output, it means the model wants us to run a document search.</p>
<p>So, we use Supermemory&#x2019;s <code>.search.execute()</code> method, we run that <code>query</code> against the user&#x2019;s connected Drive docs, limiting the results to the top 5.</p>
<p>Then, we hit the Responses API again, with the original function_call object, the search results, and the user&#x2019;s original message.</p>
<p>Lastly, we have a simple utility to wipe the entire chat history.</p>
<pre><code>&#xA0;const clearChat = () =&gt; { &#xA0; setMessages([]) }
</code></pre>
<p>We&#x2019;ve also created the UI for the app below and linked the functions to distinct buttons:</p><pre><code> return (
   &lt;div className=&apos;flex flex-col h-screen w-screen bg-gray-50&apos;&gt;
     {/* Header */}
     &lt;header className=&apos;bg-white shadow-sm border-b px-4 py-3&apos;&gt;
       &lt;div className=&apos;max-w-4xl mx-auto flex items-center justify-between&apos;&gt;
         &lt;h1 className=&apos;text-xl font-semibold text-gray-800 flex items-center gap-2&apos;&gt;
         ContractGuard
         &lt;/h1&gt;
         &lt;div className=&apos;flex items-center gap-2&apos;&gt;
           &lt;button
             onClick={handleConnect}
             className=&apos;px-3 py-1 text-sm bg-gray-100 hover:bg-gray-200 rounded-md transition-colors&apos;
           &gt;
             Connect to Google Drive
           &lt;/button&gt;

           &lt;button
             onClick={handleSync}
             className=&apos;px-3 py-1 text-sm bg-gray-100 hover:bg-gray-200 rounded-md transition-colors&apos;
           &gt;
             Sync Google Drive
           &lt;/button&gt;
           &lt;button
             onClick={clearChat}
             className=&apos;px-3 py-1 text-sm bg-gray-100 hover:bg-gray-200 rounded-md transition-colors&apos;
           &gt;
             Clear chats
           &lt;/button&gt;
         &lt;/div&gt;
       &lt;/div&gt;
     &lt;/header&gt;

     {/* Messages */}
     &lt;div className=&apos;flex-1 overflow-y-auto px-4 py-6&apos;&gt;
       &lt;div className=&apos;max-w-4xl mx-auto space-y-6&apos;&gt;
         {messages.length === 0 &amp;&amp; (
           &lt;div className=&apos;text-center text-gray-500 py-12&apos;&gt;
             &lt;Bot className=&apos;w-12 h-12 mx-auto mb-4 text-gray-300&apos; /&gt;
             &lt;p className=&apos;text-lg&apos;&gt;Start a conversation with the AI&lt;/p&gt;
             &lt;p className=&apos;text-sm&apos;&gt;Type your message below to get started&lt;/p&gt;
           &lt;/div&gt;
         )}

         {messages.map(message =&gt; (
           &lt;div
             key={message.id}
             className={`flex gap-3 ${
               message.role === &apos;user&apos; ? &apos;justify-end&apos; : &apos;justify-start&apos;
             }`}
           &gt;
             {message.role === &apos;assistant&apos; &amp;&amp; (
               &lt;div className=&apos;flex-shrink-0&apos;&gt;
                 &lt;div className=&apos;w-8 h-8 bg-blue-600 rounded-full flex items-center justify-center&apos;&gt;
                   &lt;Bot className=&apos;w-4 h-4 text-white&apos; /&gt;
                 &lt;/div&gt;
               &lt;/div&gt;
             )}

             &lt;div
               className={`max-w-3xl rounded-lg px-4 py-3 ${
                 message.role === &apos;user&apos;
                   ? &apos;bg-blue-600 text-white&apos;
                   : &apos;bg-white border border-gray-200&apos;
               }`}
             &gt;
               {message.role === &apos;user&apos; ? (
                 &lt;p className=&apos;whitespace-pre-wrap&apos;&gt;{message.content}&lt;/p&gt;
               ) : (
                 &lt;div className=&apos;prose prose-sm max-w-none text-black&apos;&gt;
                   &lt;ReactMarkdown
                     components={{
                       code ({ inline, className, children, ...props }) {
                         if (inline) {
                           return (
                             &lt;code
                               className=&apos;bg-gray-100 px-1 py-0.5 rounded text-sm&apos;
                               {...props}
                             &gt;
                               {children}
                             &lt;/code&gt;
                           )
                         }
                         return (
                           &lt;pre className=&apos;bg-gray-100 rounded-lg p-4 overflow-x-auto text-sm&apos;&gt;
                             &lt;code className={className} {...props}&gt;
                               {children}
                             &lt;/code&gt;
                           &lt;/pre&gt;
                         )
                       }
                     }}
                   &gt;
                     {message.content}
                   &lt;/ReactMarkdown&gt;
                 &lt;/div&gt;
               )}
               &lt;div className=&apos;text-xs opacity-70 mt-2&apos;&gt;
                 {message.timestamp.toLocaleTimeString()}
               &lt;/div&gt;
             &lt;/div&gt;

             {message.role === &apos;user&apos; &amp;&amp; (
               &lt;div className=&apos;flex-shrink-0&apos;&gt;
                 &lt;div className=&apos;w-8 h-8 bg-gray-600 rounded-full flex items-center justify-center&apos;&gt;
                   &lt;User className=&apos;w-4 h-4 text-white&apos; /&gt;
                 &lt;/div&gt;
               &lt;/div&gt;
             )}
           &lt;/div&gt;
         ))}

         {isLoading &amp;&amp; (
           &lt;div className=&apos;flex gap-3 justify-start&apos;&gt;
             &lt;div className=&apos;flex-shrink-0&apos;&gt;
               &lt;div className=&apos;w-8 h-8 bg-blue-600 rounded-full flex items-center justify-center&apos;&gt;
                 &lt;Bot className=&apos;w-4 h-4 text-white&apos; /&gt;
               &lt;/div&gt;
             &lt;/div&gt;
             &lt;div className=&apos;bg-white border border-gray-200 rounded-lg px-4 py-3&apos;&gt;
               &lt;div className=&apos;flex items-center gap-2 text-gray-500&apos;&gt;
                 &lt;Loader2 className=&apos;w-4 h-4 animate-spin&apos; /&gt;
                 &lt;span&gt;AI is thinking...&lt;/span&gt;
               &lt;/div&gt;
             &lt;/div&gt;
           &lt;/div&gt;
         )}

         &lt;div ref={messagesEndRef} /&gt;
       &lt;/div&gt;
     &lt;/div&gt;

     {/* Input */}
     &lt;div className=&apos;bg-white border-t px-4 py-4 text-black&apos;&gt;
       &lt;div className=&apos;max-w-4xl mx-auto&apos;&gt;
         &lt;form onSubmit={sendMessage} className=&apos;flex gap-3&apos;&gt;
           &lt;input
             type=&apos;text&apos;
             value={input}
             onChange={e =&gt; setInput(e.target.value)}
             placeholder=&apos;Type your message...&apos;
             disabled={isLoading}
             className=&apos;flex-1 px-4 py-3 border border-gray-300 rounded-lg focus:outline-none focus:ring-2 focus:ring-blue-500 focus:border-transparent disabled:opacity-50&apos;
           /&gt;
           &lt;button
             type=&apos;submit&apos;
             disabled={!input.trim() || isLoading}
             className=&apos;px-6 py-3 bg-blue-600 text-white rounded-lg hover:bg-blue-700 focus:outline-none focus:ring-2 focus:ring-blue-500 focus:ring-offset-2 disabled:opacity-50 disabled:cursor-not-allowed transition-colors flex items-center gap-2&apos;
           &gt;
             {isLoading ? (
               &lt;Loader2 className=&apos;w-4 h-4 animate-spin&apos; /&gt;
             ) : (
               &lt;Send className=&apos;w-4 h-4&apos; /&gt;
             )}
             Send
           &lt;/button&gt;
         &lt;/form&gt;
       &lt;/div&gt;
     &lt;/div&gt;
   &lt;/div&gt;
 )
}

export default App
</code></pre>
<p>This code implements a connect button, sync button, and a chat interface. Your frontend app should change to the following.</p><figure class="kg-card kg-image-card"><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXcBqV4rsY4idtlZOzEORhc7uuHl4LBCYNmY1ll5axq-aokiuEbS6Tno2Wuz1DqY4ZlLsavT7yZdPbr0ZND2lP64Ew7SQOkH9iWH-b9k74GBYcHmWWiyGunFtUMppMD4dvNgM5kmPw?key=yaSvOVpdgQgQh3vtNv08hg" class="kg-image" alt="Building an AI Compliance Chatbot With Supermemory and Google Drive" loading="lazy" width="624" height="352"></figure><h3 id="chat-with-your-assistant">Chat with your assistant</h3><p>To chat with your AI assistant, first, click <strong>Connect to Google Drive</strong>, you will be redirected to an OAuth URL to complete your authorization process that wil grant Supermemory access to files in your Google Drive, then upload your Contract and Compliance files to your google drive and click<strong> Sync Google Drive</strong> to bring Supermemory AI up to date with the new files.</p><p>Ask a question to your AI via the chat interface, Supermemory will automatically insert context from the contract and compliance files stored in your Google Drive and your previous conversations with the AI into the AI chat, helping your AI give you the correct answers.</p><h2 id="conclusion">Conclusion</h2><p>And that&#x2019;s a wrap. You now have a contract compliance chatbot that:</p><ul><li>Connects itself straight to Google Drive without any manual uploads</li><li>Automatically keeps its document &#x201C;memory&#x201D; up to date</li><li>Decides when to dig into your contracts and brings back precise, cited answers</li></ul><p>By leaning on Supermemory as your memory layer, you never have to build complex ingestion pipelines or worry about stitching context back into your prompts. Supermemory handles the Drive sync, the embedding, and the context injection so you can focus on tweaking your assistant&#x2019;s prompts, refining its compliance checks, and rolling it out to real users.</p><p>Head over to <a href="http://supermemory.ai/?ref=blog.supermemory.ai"><u>supermemory.ai</u></a> and grab your API key now, it&#x2019;s free.</p>]]></content:encoded></item><item><title><![CDATA[Supermemory just got faster on PlanetScale]]></title><description><![CDATA[<h2 id="what-is-supermemory"><strong>What is Supermemory?</strong></h2><p>Supermemory completes the missing part of the LLM puzzle: memory. Just as memory is crucial for human intelligence, it&apos;s essential for truly intelligent AI systems. We&apos;ve built a portable memory engine that works seamlessly across different LLMs through multiple interfaces, including an <a href="https://supermemory.ai/docs/memory-api/introduction?ref=blog.supermemory.ai"><u>API</u></a></p>]]></description><link>https://blog.supermemory.ai/supermemory-just-got-faster-on-planetscale/</link><guid isPermaLink="false">687a92188578750001471aac</guid><dc:creator><![CDATA[Dhravya Shah]]></dc:creator><pubDate>Fri, 18 Jul 2025 18:38:14 GMT</pubDate><media:content url="https://blog.supermemory.ai/content/images/2025/07/new1.webp" medium="image"/><content:encoded><![CDATA[<h2 id="what-is-supermemory"><strong>What is Supermemory?</strong></h2><img src="https://blog.supermemory.ai/content/images/2025/07/new1.webp" alt="Supermemory just got faster on PlanetScale"><p>Supermemory completes the missing part of the LLM puzzle: memory. Just as memory is crucial for human intelligence, it&apos;s essential for truly intelligent AI systems. We&apos;ve built a portable memory engine that works seamlessly across different LLMs through multiple interfaces, including an <a href="https://supermemory.ai/docs/memory-api/introduction?ref=blog.supermemory.ai"><u>API</u></a>, <a href="https://supermemory.ai/docs/memory-api/sdks/typescript?ref=blog.supermemory.ai"><u>SDKs</u></a>, <a href="https://supermemory.ai/docs/model-enhancement/context-extender?ref=blog.supermemory.ai"><u>base URL swap (Infinite Chat)</u></a>, and <a href="https://mcp.supermemory.ai/?ref=blog.supermemory.ai"><u>MCP</u></a>.</p><h2 id="our-database-problems"><strong>Our Database Problems</strong></h2><p>Our previous database solution, TigerData (previously Timescale), worked okay for us, but there were a few issues that we encountered:</p><ul><li>No branching or forking made development and testing risky</li><li>Performance was okay, but nothing exceptional, especially given the cost</li><li>The dashboard experience was horrendous, making monitoring painful</li><li>Logs were nearly impossible to navigate and often went missing entirely - When things went wrong, we were often dumbstruck and it took longer to debug critical errors.</li><li>Analytics were fairly basic, with no query optimization suggestions to help us improve performance</li></ul><p>These issues weren&apos;t just minor inconveniences. Supermemory works with vectors and massive amounts of user documents. </p><p>We&apos;re constantly ingesting and retrieving huge volumes of data. Every millisecond counts for our <a href="https://supermemory.ai/docs/model-enhancement/context-extender?ref=blog.supermemory.ai"><u>Infinite Chat product</u></a>, where users expect instant responses. Meanwhile, our TigerData costs exploded as we scaled.</p><h2 id="migrating-to-planetscale"><strong>Migrating to PlanetScale</strong></h2><p>PlanetScale didn&apos;t just provide a database; they provided a partnership. </p><p>We met their team in person at their office, where they worked with us closely and gave us hands-on support for a smooth migration. Beyond that, they provided amazing resources for a no downtime migration. </p><p>We used their <a href="https://github.com/planetscale/migration-scripts/tree/main/postgres-direct?ref=blog.supermemory.ai"><u>proxy script</u></a> to perform the migration from TigerData to PlanetScale with no interruption to our users, which is what mattered to us most.</p><h2 id="the-results-speak-for-themselves"><strong>The Results Speak for Themselves</strong></h2><p>The impact was immediate and dramatic:</p><ul><li>Costs dropped from $900/mo to $90/mo (90% reduction!)</li><li><strong>QPS jumped from 20 to 1000 (50x improvement!)</strong></li><li><strong>Our p99 for the database is always less than 6ms - making our services much faster than before </strong></li><li>Affordable and pre-validated backups that have already saved us once</li></ul><p>Another huge win was PlanetScale&#x2019;s Insights dashboard, which gave us an in-depth look at our query performance. This deep level of observability is crucial to us as we scale.</p><p>For a company building the future of AI memory, finding a database that could keep up with our ambitions was crucial. We&#x2019;re excited that we no longer have to worry about handling scale as we grow. We can now focus on building more incredible features for our users, which is our top priority.</p>]]></content:encoded></item><item><title><![CDATA[Knowledge Graph For RAG: Step-by-Step Tutorial]]></title><description><![CDATA[<p>If you&#x2019;ve ever built a retrieval-augmented generation (RAG) system using embeddings and vector databases, you already know the drill: you turn your data into vectors, stuff them into a store like <a href="https://github.com/facebookresearch/faiss?ref=blog.supermemory.ai">FAISS</a>, and let your model retrieve similar chunks during inference.</p><p>And it works, until it doesn&#x2019;</p>]]></description><link>https://blog.supermemory.ai/knowledge-graph-for-rag-step-by-step-tutorial/</link><guid isPermaLink="false">6871ea978578750001471a59</guid><dc:creator><![CDATA[Naman Bansal]]></dc:creator><pubDate>Sat, 12 Jul 2025 07:56:27 GMT</pubDate><media:content url="https://blog.supermemory.ai/content/images/2025/07/11.webp" medium="image"/><content:encoded><![CDATA[<img src="https://blog.supermemory.ai/content/images/2025/07/11.webp" alt="Knowledge Graph For RAG: Step-by-Step Tutorial"><p>If you&#x2019;ve ever built a retrieval-augmented generation (RAG) system using embeddings and vector databases, you already know the drill: you turn your data into vectors, stuff them into a store like <a href="https://github.com/facebookresearch/faiss?ref=blog.supermemory.ai">FAISS</a>, and let your model retrieve similar chunks during inference.</p><p>And it works, until it doesn&#x2019;t.</p><h3 id="why-vector-search-alone-falls-short">Why Vector Search Alone Falls Short</h3><p>Embeddings are great at catching semantic similarity. But what they don&#x2019;t give you is structure. They don&#x2019;t know that &#x201C;Supplier A ships to Germany&#x201D; or that &#x201C;Product X requires a temperature-controlled warehouse.&#x201D; They don&#x2019;t understand relationships.</p><p>This becomes a real problem when:</p><ul><li>You need to enforce business logic<br>(e.g., &#x201C;<em>Only show suppliers who are certified and in the same region</em>&#x201D;)</li><li>You want to combine structured facts with natural text<br>(e.g., &#x201C;<em>Summarize my top 5 suppliers in Europe for Product X</em>&#x201D;)</li><li>You need explainability<br>(e.g., &#x201C;<em>Why was this supplier recommended?</em>&#x201D;)</li></ul><p>In short, vector search is like a very smart blur, which can be really helpful, but sometimes too vague.</p><h3 id="why-knowledge-graphs-are-catching-on">Why Knowledge Graphs Are Catching On</h3><p>That&#x2019;s where knowledge graphs come in. Unlike vectors, graphs give you explicit relationships between entities. A product has a supplier. A supplier serves a location. A warehouse stores certain categories. And once that structure exists, querying becomes way more powerful and intuitive.</p><p>Here&#x2019;s what makes knowledge graphs so useful in AI-powered apps:</p><ul><li><strong>Structured retrieval</strong>: You can write precise queries instead of relying on fuzzy matches.</li><li><strong>Explainability</strong>: You can show exactly why a result was retrieved.</li><li><strong>Domain alignment</strong>: Graphs reflect real-world relationships, just like your business rules do.</li></ul><figure class="kg-card kg-image-card"><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXebTSWwC1IrgIU8dkccq9lAUPrt-S7eaTz12oAiyTt4jLmytGTuC8HgbpI_pf5lvWnNLLxYurrIQA_RnznCaNobLpEvkW9_Y3vAUI9IScE44qaBv50XiE_bt5Aho9qoEB9lUI1duw?key=lqZSFvuWmXmvGZnyQ5f1lQ" class="kg-image" alt="Knowledge Graph For RAG: Step-by-Step Tutorial" loading="lazy" width="1024" height="1024"></figure><p>By the end of this article, you&#x2019;ll build a working question-answering app that combines knowledge graphs with language models. Specifically:</p><ul><li>You&#x2019;ll load supply chain data (product catalogs, suppliers, purchase orders)</li><li>Convert unstructured descriptions into structured relationships</li><li>Build a knowledge graph using <a href="https://neo4j.com/?ref=blog.supermemory.ai">Neo4j</a></li><li>Query the graph with user questions like &#x201C;Which suppliers in Europe provide Product X?&#x201D;</li><li>Generate natural-language answers based on retrieved graph data</li></ul><p>This is a very interesting real-world implementation of knowledge graph-based RAG.</p><h2 id="how-knowledge-graphs-work">How Knowledge Graphs Work</h2><p>Working with vector search, you already know that you take some text, embed it into a vector, and then look for other vectors that are close. The moment you need to reason over structured facts, relationships, or apply logic like &#x201C;filter by supplier region,&#x201D; vector search starts to feel like duct tape.</p><p>That&#x2019;s where knowledge graphs step in.</p><p>A knowledge graph (KG) is a structured representation of facts. Instead of storing data as documents or raw text, it organizes the world as entities (like &#x201C;<strong>Supplier A</strong>&#x201D; or &#x201C;<strong>Product X</strong>&#x201D;) and relationships (like &#x201C;supplies&#x201D; or &#x201C;located_in&#x201D;) between them.</p><h3 id="the-building-blocks-of-a-knowledge-graph">The Building Blocks of a Knowledge Graph</h3><p>At the core, a KG is just a set of triples:</p><p><strong>(Entity A) &#x2014; [Relation] &#x2192; (Entity B)</strong></p><p>For example:<br><strong>(Supplier A) &#x2014; [supplies] &#x2192; (Product X)<br>(Supplier A) &#x2014; [located_in] &#x2192; (Germany)</strong></p><p>These triples are stored as nodes and edges, which together form a graph. Each node represents a unique concept or object, while each edge defines how two nodes are connected.</p><figure class="kg-card kg-image-card"><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXeAnMTCZSXUR_Viin6XZBsfnsZMCbzlwCKdHeFW-KZ39Ee07B-WO78aIpwJ11yoE0CMvNtc7cMR6NdOXumcD2A3kPoCm_djE158Dwbw9VErtbolJ6xcUhCh1IK-wON1LvcjdHEJqg?key=lqZSFvuWmXmvGZnyQ5f1lQ" class="kg-image" alt="Knowledge Graph For RAG: Step-by-Step Tutorial" loading="lazy" width="1024" height="1024"></figure><p>Once you have this graph, retrieval becomes more than just blurry similarity. You can follow paths, apply constraints, and answer precise questions. For instance:</p><ul><li>Which suppliers in Europe provide electronic parts?</li><li>What products did Supplier B deliver last quarter?</li></ul><p>The graph understands structure and hierarchy. Because of the nodes and edges, it knows that &#x201C;Supplier B&#x201D; is related to certain purchase orders and that &#x201C;Europe&#x201D; contains Germany, France, etc.</p><h3 id="why-not-just-use-vectors">Why Not Just Use Vectors?</h3><p>Good question. Vector search engines like <a href="https://www.pinecone.io/?ref=blog.supermemory.ai">Pinecone</a> or <a href="https://weaviate.io/?ref=blog.supermemory.ai">Weaviate</a> are powerful for semantic matching. They shine when you&#x2019;re dealing with natural language, fuzzy synonyms, or vague queries like &#x201C;cheapest product with fast delivery.&#x201D;</p><p>But they fall short when:</p><ul><li>You need exact business logic (&#x201C;only suppliers who passed our compliance checks&#x201D;).</li><li>You want explainability (how did the system decide this was the best answer?).</li><li>You care about relational context (&#x201C;show suppliers that deliver Product X and are located in Germany&#x201D;).</li></ul><p>With vector embeddings, the relationships between entities are implicit. With knowledge graphs, they&#x2019;re explicit and that changes everything.</p><h3 id="how-knowledge-graphs-power-retrieval">How Knowledge Graphs Power Retrieval</h3><p>Imagine you get a user query like:<br>&#x201C;Which suppliers in Europe provide lithium-ion batteries?&#x201D;</p><p>A knowledge graph can:</p><ol><li>Identify &quot;<strong>lithium-ion batteries</strong>&quot; as a product.</li><li>Traverse the graph to find all suppliers linked to that product.</li><li>Filter by region to return only those with a located_in relationship pointing to European countries.</li></ol><p>And that&#x2019;s not hypothetical. In the next section, you&#x2019;ll build exactly that.</p><blockquote>Want to learn how memory improves LLM performance? <a href="https://supermemory.ai/blog/the-wow-factor-of-memory-how-flow-used-supermemory-to-build-smarter-stickier-products?ref=blog.supermemory.ai">See how Flow built smarter products with Supermemory</a></blockquote><p>By combining graphs and LLMs, you get the best of both worlds: structured, reliable facts with the flexibility of natural language understanding. It&#x2019;s a big reason why memory-based platforms like <a href="https://supermemory.ai/?ref=blog.supermemory.ai">supermemory.ai</a> are designed to integrate with both vector and graph retrieval patterns.</p><h2 id="step-by-step-tutorial-supply-chain-qa-with-knowledge-graph-rag">Step-by-Step Tutorial: Supply Chain QA with Knowledge Graph RAG</h2><p>In this section, we will get practical. We will build a local knowledge graph using Neo4j, convert structured CSV fields into knowledge triples using GPT, query the graph to answer domain-specific questions and finally, generate a readable answer using a template.</p><h3 id="step-1-setup-and-install-dependencies">Step 1. Setup and Install Dependencies</h3><p>In this section, we will ensure that our local environment is ready for building and querying the Knowledge Graph.</p><p>We will need the following:</p><ul><li>Python 3.8+</li><li>Neo4j Desktop (<a href="https://neo4j.com/download/?ref=blog.supermemory.ai">Download it here</a>)</li><li>OpenAI account (for triple extraction)</li><li>Jupyter Notebook or any Python IDE (e.g., VS Code)</li><li><a href="https://www.kaggle.com/datasets/shahriarkabir/procurement-kpi-analysis-dataset?ref=blog.supermemory.ai">Procurement KPI Analysis Dataset</a></li></ul><p>Open your terminal or notebook and install the following:</p><pre><code class="language-python">pip install pandas py2neo openai neo4j datasets
</code></pre><p><code>py2neo</code> is the official library to work with Neo4j in Python, <code>openai</code> helps us make requests to the LLM, and <code>datasets</code> would let us download the Procurement KPI Analysis Dataset listed above.</p><h3 id="step-2-launch-neo4j">Step 2. Launch Neo4j</h3><p>This will prepare Neo4j to accept incoming connections and display the graph. Neo4j is a graph database, designed specifically to store and query data structured as <strong>nodes</strong> (entities) and <strong>edges</strong> (relationships). To run the Neo4j desktop locally, you can follow this step-by-step process:</p><h3 id="step-1-create-a-new-project">Step 1: Create a New Project</h3><ol><li>After this, you will find your local instance running. You&#x2019;ll need the <strong>Connection URI</strong> for your Python app to connect. Click on your running DB, where you will be able to find the following:</li></ol><p>Create an instance, you can name it <strong>SupplyChainKG</strong>. Inside the instance, create a database user and send the password(you&#x2019;ll use this in your code).<br></p><figure class="kg-card kg-image-card"><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfpcVetsogbOWvfK6o7H3kUz6GckE_GPT9esEow35_IijSGcuE58yEkefcZF5sdxKjGyLe-QYcN3OyykkX1Xxkxcy857HhaXrs5aRaDNGKY-IpIwXrh9b47hWUMlVtqLOZGk7TY?key=lqZSFvuWmXmvGZnyQ5f1lQ" class="kg-image" alt="Knowledge Graph For RAG: Step-by-Step Tutorial" loading="lazy" width="1600" height="1010"></figure><p>Open Neo4j Desktop<br></p><figure class="kg-card kg-image-card"><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXd1LpvMZvsJy93yQeVaB7C6edgfa8Hw5jAh_TJU5zj46k71Q2I8lbZwKPBhkjPcL9mpuV_gGfxf1GO1TjNYLQ2SJt57luFf9G6FkTqL_xE8R3WX7DHXbVNNvWlVxMAMIQ6QlWtX?key=lqZSFvuWmXmvGZnyQ5f1lQ" class="kg-image" alt="Knowledge Graph For RAG: Step-by-Step Tutorial" loading="lazy" width="1600" height="1010"></figure><ul><li>Connection URI: <code>neo4j://127.0.0.1:7687</code></li><li>Username: <code>neo4j</code></li><li>Password: whatever you set (e.g., test)</li></ul><figure class="kg-card kg-image-card"><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXei76CYbLUsDo7n4r-VRXu1hGZwXIP9WL2LHX4k8o1wN9YjUGukujHAK2mQ5I1Q0RZ75LZWAWM222EJel8JTlcH7BgCwFh5Pb_r5lOy6_s9aUI8u9zPoqQMDeBJ4mSu_YpAzuaxdw?key=lqZSFvuWmXmvGZnyQ5f1lQ" class="kg-image" alt="Knowledge Graph For RAG: Step-by-Step Tutorial" loading="lazy" width="1600" height="1010"></figure><h3 id="step-3-import-required-libraries-and-connect-to-neo4j">Step 3. Import Required Libraries and Connect to Neo4j</h3><p>Before we build the knowledge graph, we need to import the necessary libraries and connect to a local Neo4j database. Make sure Neo4j is running and your credentials are correct.</p><pre><code class="language-python">import ast 
from py2neo import Graph, Node, Relationship # To interact with Neo4j graph  
  
# Connect to your running Neo4j database instance  
graph = Graph(&quot;neo4j://localhost:7687&quot;, auth=(&quot;neo4j&quot;, &quot;testpassword&quot;))
</code></pre><p><code>ast</code> has been used for safely evaluating stringified list of tuples.</p><h3 id="step-4-define-the-graph-construction-function">Step 4. Define the Graph Construction Function</h3><p>Our app will work by using GPT to convert the rows in our dataset into strings of triples that we can use to construct nodes and relationships in the graph database. This function takes GPT&apos;s response (a string of triples), parses it, and constructs nodes and relationships inside the Neo4j graph.</p><p>Each triple is expected to follow the (subject, predicate, object) structure.</p><p>Declare a function in your file:</p><pre><code class="language-python">def build_graph_from_gpt_response(gpt_response):  
</code></pre><p>Use a try-catch block inside it to parse the GPT output:</p><pre><code class="language-python">	try:  
        # Safely parse the GPT response (which is a string of list of tuples)  
        triples = ast.literal_eval(gpt_response)  
    except Exception as e:  
        print(&quot;Parsing Error:&quot;, e)
</code></pre><p>Awesome, now let&apos;s extract the subject, predicate, and object from the GPT output and convert it to a graph as follows:</p><pre><code class="language-python">	for triple in triples:  
    # Ensure the triple is valid and contains exactly 3 items  
        if len(triple) != 3:  
            continue  
        # Strip and convert all parts to string (to avoid TypeErrors)  
        subject, predicate, obj = [str(x).strip() for x in triple]  
        
        # Create graph nodes for subject and object  
        subj_node = Node(&quot;Entity&quot;, name=subject)  
        obj_node = Node(&quot;Entity&quot;, name=obj)  
        
        # Create the relationship between the nodes  
        relationship = Relationship(subj_node, predicate, obj_node)  
        
        # Merge ensures no duplicates; updates if nodes/edges exist  
        graph.merge(subj_node, &quot;Entity&quot;, &quot;name&quot;)  
        graph.merge(obj_node, &quot;Entity&quot;, &quot;name&quot;)  
        graph.merge(relationship)
</code></pre><p>The loop only executes if the tuple has exactly 3 entities. Using the <code>strip()</code> function, the entites are extracted.</p><p>Nodes are constructed for the subject and object, wherease the predicate is used to create a relationship, and these are merged into the graph.</p><h3 id="step-5-import-openai-and-load-dataset">Step 5. Import OpenAI and Load Dataset</h3><p>We&apos;ll use the OpenAI API to extract structured knowledge from rows in the procurement CSV dataset. We&#x2019;re using a <a href="https://www.kaggle.com/datasets/shahriarkabir/procurement-kpi-analysis-dataset?ref=blog.supermemory.ai">Procurement KPI Analysis Dataset</a> because it includes product, supplier, pricing, and compliance information.</p><pre><code class="language-python">import pandas as pd  
from openai import OpenAI # Ensure you&apos;ve installed the OpenAI package  
  
# Initialize OpenAI client with your API key  
client = OpenAI(api_key=&quot;OPENAI_API_KEY&quot;)
  
# Load the Procurement KPI Analysis Dataset  
df = pd.read_csv(&quot;Procurement KPI Analysis Dataset.csv&quot;)
</code></pre><p>Initialize the OpenAI client with your OpenAI API Key and make sure you&apos;ve downloaded the CSV from the link and stored it in the same folder as the file you&apos;re writing the code in.</p><h3 id="step-6-loop-over-a-few-rows-and-send-them-to-gpt">Step 6. Loop Over a Few Rows and Send Them to GPT</h3><p>We&apos;ll use GPT to extract <strong>subject-predicate-object</strong> triples from 3 sample rows in the dataset. Once you confirm that it&apos;s working, you can increase the number of rows.<br>Now we loop through a few rows of the dataset, construct a GPT prompt for each, extract triples, and pass them to the graph builder.</p><pre><code class="language-python">for i in range(5):  # Start with 5 rows
    row = df.iloc[i]
    
    # Clear prompt with structure and examples
    prompt = f&quot;&quot;&quot;
Extract subject-predicate-object triples from the following structured purchase order. Return the result as a valid Python list of 3-item tuples. Use proper literals &#x2014; avoid vague terms like &apos;N/A&apos;.

Example format:
[
  (&quot;Delta Logistics&quot;, &quot;supplies&quot;, &quot;Raw Materials&quot;),
  (&quot;Raw Materials&quot;, &quot;has_quantity&quot;, &quot;1180&quot;),
  (&quot;Raw Materials&quot;, &quot;has_price&quot;, &quot;64.07&quot;)
]

Purchase Order:
Supplier: {row[&apos;Supplier&apos;]}
Item Category: {row[&apos;Item_Category&apos;]}
Quantity: {row[&apos;Quantity&apos;]}
Unit Price: {row[&apos;Unit_Price&apos;]}
Order Status: {row[&apos;Order_Status&apos;]}
Compliance: {row[&apos;Compliance&apos;]}

I need this to construct a knowledge graph from the data in the spreadsheet. Please ensure the triples are accurate and complete, reflecting the relationships in the purchase order.
&quot;&quot;&quot;

    # Send prompt to GPT
    response = client.chat.completions.create(
        model=&quot;gpt-3.5-turbo&quot;,
        messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}],
        temperature=0
    )

    content = response.choices[0].message.content.strip()
    print(f&quot;Triples for row {i}:&quot;, content)

    # Build graph from the returned triples
    build_graph_from_gpt_response(content)
</code></pre><p>You should get an output that looks like this:</p><figure class="kg-card kg-image-card"><img src="https://i.postimg.cc/85VXrwms/Screenshot-2025-07-12-at-09-40-45.png" class="kg-image" alt="Knowledge Graph For RAG: Step-by-Step Tutorial" loading="lazy" width="1192" height="800"></figure><h3 id="step-7-querying-the-knowledge-graph-for-answers-rag">Step 7: Querying the Knowledge Graph for Answers (RAG)</h3><p>Now we test a natural language question: <em>&quot;Which suppliers provide Raw Materials?&quot;</em></p><p>Neo4J provides their own graph query language called Cypher which we&apos;ll use to run this natural language query as follows:</p><pre><code class="language-python">question_query = &quot;&quot;&quot;
MATCH (s:Entity)-[:supplies]-&gt;(c:Entity {name: &quot;Raw Materials&quot;})
RETURN s.name AS supplier
&quot;&quot;&quot;

# Run the Cypher query
suppliers = graph.run(question_query).data()
print(&quot;Suppliers of Raw Materials:&quot;, suppliers)
</code></pre><p>Round brackets are used to represent <code>(:Nodes)</code> and <code>-[:ARROWS]-&gt;</code> represent relationships between the nodes. In the above query, we check for nodes that have the label &quot;Entity&quot;, which supply to other nodes labelled &quot;Entity&quot; with the name &quot;Raw Materials&quot;.</p><p>The output is as follows:</p><figure class="kg-card kg-image-card"><img src="https://i.postimg.cc/26bHJMjH/Screenshot-2025-07-12-at-09-59-58.png" class="kg-image" alt="Knowledge Graph For RAG: Step-by-Step Tutorial" loading="lazy" width="968" height="48"></figure><h3 id="step-8-generate-a-natural-language-answer">Step 8: Generate a Natural Language Answer</h3><p>You can turn the Cypher output into human-readable text &#x2014; a critical part of the RAG pipeline.</p><pre><code class="language-python">def format_answer(suppliers):
    if not suppliers:
        return &quot;No compliant suppliers found for Raw Materials.&quot;
    names = [s[&quot;supplier&quot;] for s in suppliers]
    return f&quot;The following compliant suppliers provide Raw Materials: {&apos;, &apos;.join(names)}.&quot;

# Print final answer
print(format_answer(suppliers))

</code></pre><p>The function simply access the array of objects that the graph returns, extracts the name of the supplier from it, and appends it to a string.</p><h3 id="step-9-visualizing-the-results-in-neo4j-desktop">Step 9. Visualizing the results in Neo4j Desktop</h3><p>Go to Neo4j Desktop and click on Query. Select your instance, and then run the queries in the text box provided.</p><p>First, let&apos;s check the number of nodes in our database with this query:</p><pre><code class="language-python">MATCH (n) RETURN COUNT(n);
</code></pre><p>Output:</p><figure class="kg-card kg-image-card"><img src="https://i.postimg.cc/3R5p8cCf/Screenshot-2025-07-12-at-10-07-19.png" class="kg-image" alt="Knowledge Graph For RAG: Step-by-Step Tutorial" loading="lazy" width="1280" height="484"></figure><p>You should see a <strong>number &gt; 0</strong> in this case, we have <strong>21</strong></p><p>If you want to check for a sample of how the data is stored, run the following:</p><pre><code class="language-python">MATCH (a)-[r]-&gt;(b) RETURN a.name, type(r), b.name LIMIT 10;
</code></pre><p>Output:</p><figure class="kg-card kg-image-card"><img src="https://i.postimg.cc/brdqKzD7/Screenshot-2025-07-12-at-10-08-10.png" class="kg-image" alt="Knowledge Graph For RAG: Step-by-Step Tutorial" loading="lazy" width="1033" height="800"></figure><p>To see the entire graph visualized: entities as circles, relationships as arrows, run this cypher:</p><pre><code class="language-python">MATCH (n)-[r]-&gt;(m) RETURN n, r, m LIMIT 100
</code></pre><figure class="kg-card kg-image-card"><img src="https://i.postimg.cc/sDQVmgRS/Screenshot-2025-07-12-at-10-08-54.png" class="kg-image" alt="Knowledge Graph For RAG: Step-by-Step Tutorial" loading="lazy" width="1280" height="779"></figure><h2 id="how-do-you-measure-knowledge-graph-quality">How Do You Measure Knowledge Graph Quality?</h2><p>Building a knowledge graph is great. But how do you know if it&#x2019;s actually any good?</p><p>This is where a lot of beginner AI engineers hit a wall. The graph looks fine, maybe even visualizes nicely in Neo4j, but under the hood, it could be missing key connections, introducing wrong relationships, or lacking coverage. And if you&apos;re plugging that graph into a retrieval-augmented generation (RAG) system, a weak graph means weak answers.</p><p>So let&#x2019;s break down the core ways to evaluate the quality of your knowledge graph, especially the one we just built for the supply chain question-answering use case.</p><h3 id="coverage">Coverage</h3><p>Coverage refers to how much of the actual data your graph managed to capture in the form of nodes and edges. If your dataset contains 100 suppliers but only 40 show up as nodes in the graph, you&apos;re leaving insight on the table.</p><p>You can calculate this manually by comparing the number of entities/relations in your source data versus the ones that made it into the graph.</p><pre><code class="language-python"># Total number of unique suppliers in the CSV
total_suppliers = df[&apos;Supplier&apos;].nunique()

# How many of those suppliers exist as named nodes in the graph?
query = &quot;&quot;&quot;
MATCH (s:Entity)
WHERE EXISTS {
  MATCH (s)-[]-&gt;()
}
RETURN count(DISTINCT s.name) AS linked_suppliers
&quot;&quot;&quot;
linked_suppliers = graph.run(query, parameters={&quot;supplier_list&quot;: df[&apos;Supplier&apos;].dropna().unique().tolist()}).data()[0][&apos;linked_suppliers&apos;]
coverage_percent = (linked_suppliers / total_suppliers) * 100

print(f&quot;Supplier Node Coverage: {coverage_percent:.2f}%&quot;)
</code></pre><p>Why does this matter? Low coverage means your graph can&#x2019;t support many types of queries, it limits what your app can answer.</p><h3 id="accuracy">Accuracy</h3><p>Accuracy means the relationships in the graph are actually correct. For instance, if &#x201C;Delta Logistics&#x201D; is shown supplying &#x201C;Office Supplies&#x201D; when they really supply &#x201C;IT Equipment,&#x201D; that&#x2019;s a faulty edge.</p><p>To check for accuracy, spot-check your triples. A good practice is to manually review a subset of extracted triples and compare them with the original row in the dataset. If you&#x2019;ve extracted 10 triples from GPT, verify if each subject-predicate-object makes sense contextually.</p><pre><code class="language-python">sample_check = graph.run(&quot;&quot;&quot;  
MATCH (a:Entity)-[r]-&gt;(b:Entity)  
RETURN a.name AS subject, type(r) AS relation, b.name AS object  
LIMIT 10  
&quot;&quot;&quot;).to_data_frame()  
  
print(sample_check)
</code></pre><h3 id="completeness">Completeness</h3><p>A graph can be accurate and still incomplete. Completeness checks whether all relevant connections were extracted and represented.</p><p>Checking this is trickier because it often involves domain knowledge. For example, if your business rule says every supplier must have a &#x201C;compliance&#x201D; rating, but your graph has some suppliers without any compliance edges, you&#x2019;ve got gaps.</p><pre><code class="language-python"># Find suppliers without compliance links  
query = &quot;&quot;&quot;  
MATCH (s:Entity)
WHERE NOT (s)-[:Compliance]-&gt;()
RETURN s.name AS MissingCompliance
&quot;&quot;&quot;  

missing_compliance = graph.run(query, parameters={&quot;supplier_list&quot;: df[&apos;Supplier&apos;].dropna().unique().tolist()}).to_data_frame()  
print(missing_compliance)

</code></pre><h3 id="explainability">Explainability</h3><p>One of the strongest benefits of knowledge graphs over vector search is explainability. Can you trace a clear path between entities? Can you see why a fact was returned?</p><p>To check for this, fire up <a href="https://neo4j.com/bloom/?ref=blog.supermemory.ai">Neo4j Bloom</a> and inspect the subgraphs visually. When you ask a question like &#x201C;Which non-compliant suppliers provide raw materials?&#x201D;, can you see the chain of edges that explain the result?</p><p>Alternatively, export a few subgraphs and explain them in plain English. That&#x2019;s a human-readable test of graph logic.</p><p>You can get more scientific with:</p><ul><li>Precision &amp; Recall for named entity recognition (NER) and relation extraction (RE) stages &#x2014; if your pipeline does this</li><li>Manual Subgraph Audits: Pick 10 nodes at random and audit their relationships</li><li>Consistency Checks: No dangling edges, no duplicate nodes with the same label, etc.</li></ul><p>If you want to go deeper, here are some tools that can be of help:</p><ul><li><a href="https://neo4j.com/bloom/?ref=blog.supermemory.ai">Neo4j Bloom</a>: Perfect for graph exploration without writing queries</li><li>Evaluation scripts: Write custom Python scripts that check for coverage, duplicate edges, or logical inconsistencies</li></ul><figure class="kg-card kg-image-card"><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXcXrUdHYzwlUesACSYBDl4PlcaD-kRkNfNvftzWQwF0FS2Ugel0OuYH4UQ2rVJFoZZYwrHFqVbu-bStHobh2aw9w9ONc5qgjLG4Qk0Zvmccgl2hZxEvDnew6x-HEJ3FC13_SVrUKA?key=lqZSFvuWmXmvGZnyQ5f1lQ" class="kg-image" alt="Knowledge Graph For RAG: Step-by-Step Tutorial" loading="lazy" width="1536" height="1024"></figure><h2 id="how-good-is-the-knowledge-graph-we-built">How Good Is the Knowledge Graph We Built?</h2><p>Now that you&apos;ve built our graph using Neo4j and GPT-generated triples, we can test it in real time. Using this code:</p><pre><code class="language-python"># What % of suppliers in CSV have at least one edge in the graph?  
total_suppliers = df[&apos;Supplier&apos;].nunique()  
  
query = &quot;&quot;&quot;  
MATCH (s:Entity)  
WHERE EXISTS {  
MATCH (s)-[]-&gt;()  
}  
RETURN count(DISTINCT s.name) AS linked_suppliers  
&quot;&quot;&quot;  
  
linked_suppliers = graph.run(query).data()[0][&apos;linked_suppliers&apos;]  
coverage_percent = (linked_suppliers / total_suppliers) * 100  
  
print(f&quot;Supplier Node Coverage: {coverage_percent:.2f}%&quot;)
</code></pre><p>Here is our result:</p><figure class="kg-card kg-image-card"><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXf1gJP7Y99P8YxdapUn_kw1nmKxrOGHAXkbeSfr_KSihCEx7UHKDie1lfywCUPhhAAaK6zeFPR6pKjF1wFie9DZlH5F-gbwXUaLm-Rm8bI3s7eS0jZGrcbI2ZLhCiZX5wLnmNEn?key=lqZSFvuWmXmvGZnyQ5f1lQ" class="kg-image" alt="Knowledge Graph For RAG: Step-by-Step Tutorial" loading="lazy" width="1544" height="100"></figure><p>If you&#x2019;re getting low numbers, maybe GPT missed some triples or they weren&#x2019;t added due to formatting errors, and that is your signal to iterate.</p><h2 id="conclusion">Conclusion</h2><p>Knowledge Graph RAG is a real-world solution when vector search alone doesn&#x2019;t cut it. If you&#x2019;ve ever struggled to answer structured questions or enforce logic across messy data, this combo brings sanity and structure to your pipeline.</p><p>And the best part? You don&#x2019;t have to build it all from scratch.</p><p><a href="https://supermemory.ai/?ref=blog.supermemory.ai">Supermemory</a> gives you plug-and-play memory APIs that support both vector and graph-style retrieval, with built-in evaluation, flexible integrations, and long-term memory that scales. Whether you&#x2019;re working with PDFs, emails, supply chains, or anything in between, it just works.</p><p>Ready to build smarter AI memory? <a href="https://supermemory.ai/docs/memory-api/introduction?ref=blog.supermemory.ai">Start here</a>.</p>]]></content:encoded></item><item><title><![CDATA[Faster, smarter, reliable infinite chat: Supermemory IS context engineering.]]></title><description><![CDATA[<p>People are obsessed with prompts and prompt engineering. Sure, what you say is important, but <em>what the model know</em>s when you say it is the difference between a stateless text generator and an intelligent AI system.</p><p>In short, context is the most crucial component. Karpathy&#x2019;s viral tweet</p>]]></description><link>https://blog.supermemory.ai/faster-smarter-reliable-infinite-chat-supermemory-is-context-engineering/</link><guid isPermaLink="false">686dfa7a8578750001471a31</guid><category><![CDATA[News]]></category><dc:creator><![CDATA[Dhravya Shah]]></dc:creator><pubDate>Wed, 09 Jul 2025 07:13:42 GMT</pubDate><media:content url="https://blog.supermemory.ai/content/images/2025/07/Frame_2147223295.png" medium="image"/><content:encoded><![CDATA[<img src="https://blog.supermemory.ai/content/images/2025/07/Frame_2147223295.png" alt="Faster, smarter, reliable infinite chat: Supermemory IS context engineering."><p>People are obsessed with prompts and prompt engineering. Sure, what you say is important, but <em>what the model know</em>s when you say it is the difference between a stateless text generator and an intelligent AI system.</p><p>In short, context is the most crucial component. Karpathy&#x2019;s viral tweet called it out:</p><figure class="kg-card kg-embed-card"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">+1 for &quot;context engineering&quot; over &quot;prompt engineering&quot;.<br><br>People associate prompts with short task descriptions you&apos;d give an LLM in your day-to-day use. When in every industrial-strength LLM app, context engineering is the delicate art and science of filling the context window&#x2026; <a href="https://t.co/Ne65F6vFcf?ref=blog.supermemory.ai">https://t.co/Ne65F6vFcf</a></p>&#x2014; Andrej Karpathy (@karpathy) <a href="https://twitter.com/karpathy/status/1937902205765607626?ref_src=twsrc%5Etfw&amp;ref=blog.supermemory.ai">June 25, 2025</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></figure><p>We thought to ourselves: our current infinite chat <em>is </em>context engineering! However, it had its fair share of issues. It didn&#x2019;t work with all AI providers like Gemini and Anthropic, couldn&#x2019;t support multi-modal memory, didn&#x2019;t have tool call support, and more.&#xA0;</p><p>So, we fixed it.</p><p>With today&#x2019;s release, our <strong>Infinite Chat</strong> becomes a full-blown context engineering engine.&#xA0;</p><p>The TL;DR is: it&#x2019;s faster, smarter, and more reliable.</p><p>Here&#x2019;s everything that&#x2019;s new:</p><ul><li>Multi-modal memory support (images, audio, structured data, etc.)</li><li>Works with all LLMs like Gemini and Anthropic, not just OpenAI.</li><li>Tool calls awareness and memory</li><li>Higher context limits</li><li>Long-context RAG</li><li>Implicit few-shot learning</li><li>Smarter summarization and pruning to avoid context pollution</li><li>A new architecture for better reliability and response quality</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXc4COBeYcr8MqTMJLnbpSf-Y-eHyxZJPp0c_yZsprSzkUigYnzHTTpAj4l1_Qrv0R0Hm3iQ-ZLNK76nm7zu_w-nNVz-q0tqE39ONnH0kNn6NirH3eLopQsmvFMrUN-N1jFI4lbAvg?key=vzthXSB6B4ZCgX8Pt04FZw" class="kg-image" alt="Faster, smarter, reliable infinite chat: Supermemory IS context engineering." loading="lazy" width="624" height="351"><figcaption><a href="https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-03-own-your-context-window.md?ref=blog.supermemory.ai"><u><i><em class="italic underline" style="white-space: pre-wrap;">Source</em></i></u></a></figcaption></figure><h2 id="multi-step-agent-support">Multi-step agent support</h2><p>Agents are a bit messy. Sometimes, they stop mid-task and need to pick up where they left off, without getting confused or forgetting everything.</p><p>Say you&apos;re building a coding co-pilot. You tell it: &#x201C;Edit 100 files and add 1 new feature.&#x201D;</p><p>It gets through 50, then the call times out or the model hits a limit. Now what? All 50 interactions would be polluting the context.</p><p>Our new infrastructure solves it.&#xA0;</p><p>If you provide the conversation-id, Infinite Chat removes the unnecessary context and automatically backfills it, making the network requests much faster, even on extremely long contexts:</p><pre><code>// if you provide conversation ID, You do not need to send all the messages every single time. Supermemory automatically backfills it. 
const client = new OpenAI({
    baseURL:
&quot;https://api.supermemory.ai/v3/https://api.openai.com/v1&quot;,
    defaultHeaders: {
        &quot;x-supermemory-api-key&quot;:
            &quot;&quot;,
        &quot;x-sm-user-id&quot;: `dhravya`,
        &quot;x-sm-conversation-id&quot;: &quot;conversation-id&quot;
    },
})

const messages = [
{&quot;role&quot; : &quot;user&quot;, &quot;text&quot;: &quot;SOme long thing&quot;},
// .... 50 other messages
{&quot;role&quot; : &quot;user&quot;, &quot;text&quot;: &quot;new message&quot;},
]

const client.generateText(messages)

// Next time, you dont need to send more.
const messages2 = [{&quot;role&quot; : &quot;user&quot;, &quot;text&quot;: &quot;What did we talk about in this conversation, and the one we did last year?&quot;}]

const client.generateText(messages2)
</code></pre>
<p>That includes:</p><ul><li>A summary of what was done. For instance, &#x201C;50 files were changed to add feature XYZ.&#x201D;&#xA0;</li><li>A diff of what&#x2019;s pending: &#x201C;These 12 are still left.&#x201D;</li><li>And even implicit few-shot grounding from earlier turns: &#x201C;Here&#x2019;s how you&#x2019;ve answered similar requests before.&#x201D;</li></ul><p>It&#x2019;s also cheaper, cleaner, and easier to reason about than long chat dumps.</p><h2 id="works-with-all-llm-providers">Works with all LLM providers</h2><p>Initially, Infinite Chat only supported the OpenAI API format. But, our customers asked for more - Gemini, Anthropic, Langchain, AI SDK, etc.</p><p>It was a pain in the ass to implement a system with multiple different providers, each provider having different ways to do multi-modal, tool call chains, calculating token counts, etc. for all their different models.</p><p>So,<a href="https://supermemory.ai/blog/we-solved-ai-api-interoperability/?ref=blog.supermemory.ai"><u> we built llm-bridge</u></a>, an open-source package, to solve this issue, and used it in Infinite Chat. This makes adoption seamless: all you have to do as a developer is prepend your provider&apos;s base URL with our API&apos;s URL, add your Supermemory API key, and we take care of the rest. That is, there is no change to the client interface. You can keep using your SDK of choice no matter what!</p><pre><code>const client = new OpenAI({
&#xA0;&#xA0;&#xA0;&#xA0;baseURL:
&quot;https://api.supermemory.ai/v3/https://api.openai.com/v1&quot;,
&#xA0;&#xA0;&#xA0;&#xA0;defaultHeaders: {
&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&quot;x-supermemory-api-key&quot;:
&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&quot;&quot;,
&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&quot;x-sm-user-id&quot;: `dhravya`,
&#xA0;&#xA0;&#xA0;&#xA0;},
})
/// or new Anthropic() or new GoogleGenAI()
</code></pre>
<p>Read about it in more detail <a href="https://supermemory.ai/blog/we-solved-ai-api-interoperability/?ref=blog.supermemory.ai"><u>here.</u></a></p><h2 id="smarter-memory-management">Smarter Memory Management</h2><p>Beyond simple message history, the new Infinite Chat lets your models remember more than just words and do more with them.</p><p>This includes:</p><ul><li><strong>Multimodal memory:</strong> Images, audio, structured data, etc. Anything your agent sees can now persist across calls. A screenshot sent two messages ago? Still in context. A chart from last week? Still referenced.</li><li><strong>Tool call tracking:</strong> We store tool invocations and outputs as first-class context objects. The model knows what tools it has used, what the results were, and doesn&#x2019;t blindly repeat actions.</li><li><strong>Long-context RAG:</strong> Instead of stuffing 10 retrieved passages into every prompt, Supermemory now decides what&#x2019;s already been seen, summarizes it when needed, and surfaces <em>only</em> new or relevant chunks.</li></ul><p>In essence, we&#x2019;ve taken Karpathy&#x2019;s point seriously:</p><p><em>&quot;Context engineering is the delicate art and science of filling the context window with just the right information for the next step.&quot;</em></p><p>What you really want is:</p><ul><li>Just enough memory to make the next decision smart</li><li>Just enough pruning to keep token usage efficient</li><li>Just enough scaffolding to avoid re-teaching the model what it already knows</li></ul><p>That&#x2019;s what the new Infinite Chat does.</p><p><a href="https://supermemory.ai/docs/model-enhancement/context-extender?ref=blog.supermemory.ai"><u>Take it out for a spin today!</u></a></p>]]></content:encoded></item><item><title><![CDATA[We solved AI API interoperability]]></title><description><![CDATA[<blockquote>One API to rule them all, One spec to find them,<br>One library to bring them all and in the TypeScript, bind them.</blockquote><p>When we were building the the Infinite Chat API, initially, we only supported the OpenAI format. This was fine, until a lot of our customers started asking,</p>]]></description><link>https://blog.supermemory.ai/we-solved-ai-api-interoperability/</link><guid isPermaLink="false">686ad459857875000147198c</guid><dc:creator><![CDATA[Alex Foster]]></dc:creator><pubDate>Mon, 07 Jul 2025 02:35:52 GMT</pubDate><media:content url="https://blog.supermemory.ai/content/images/2025/07/9.png" medium="image"/><content:encoded><![CDATA[<blockquote>One API to rule them all, One spec to find them,<br>One library to bring them all and in the TypeScript, bind them.</blockquote><img src="https://blog.supermemory.ai/content/images/2025/07/9.png" alt="We solved AI API interoperability"><p>When we were building the the Infinite Chat API, initially, we only supported the OpenAI format. This was fine, until a lot of our customers started asking, asking for more.</p><div class="kg-card kg-cta-card kg-cta-bg-grey kg-cta-immersive  kg-cta-has-img  " data-layout="immersive">
            
                <div class="kg-cta-sponsor-label-wrapper">
                    <div class="kg-cta-sponsor-label">
                        <span style="white-space: pre-wrap;">Skip to the repo</span>
                    </div>
                </div>
            
            <div class="kg-cta-content">
                
                    <div class="kg-cta-image-container">
                        <a href="https://github.com/supermemoryai/llm-bridge?ref=blog.supermemory.ai"><img src="https://blog.supermemory.ai/content/images/2025/07/llm-bridge.png" alt="We solved AI API interoperability" data-image-dimensions="1200x600"></a>
                    </div>
                
                
                    <div class="kg-cta-content-inner">
                    
                        <div class="kg-cta-text">
                            <p dir="ltr"><i><b><strong class="italic" style="white-space: pre-wrap;">LLM Bridge</strong></b></i><span style="white-space: pre-wrap;"> - Interoperability between input formats of various LLMs, with observability, error handling, etc. built in.</span></p>
                        </div>
                    
                    
                        <a href="https://github.com/supermemoryai/llm-bridge?ref=blog.supermemory.ai" class="kg-cta-button " style="background-color: #000000; color: #ffffff;">
                            REPOSITORY
                        </a>
                        
                    </div>
                
            </div>
        </div><p>Many customers were using Anthropic SDKs, Gemini pro models, AI SDK, langchain, etc. We wanted to keep our developer experience as beautiful as it currently is - one universal &apos;routing&apos; layer, and proxy, that also serves as a memory provider. </p><p>Now, we were stuck implementing a complex system, with multiple different providers, each provider having different ways to do multi-modal, tool call chains, calculating token counts, etc. etc. for all these different models and functionalities that they provide.</p><p>Almost immediately, our straightforward logic splinted into a mountain of complexity. Instead of a clean pipeline, we were left inundated with branching logic, special-case conditionals, and provider-specific exceptions. Where there used to be a simple flow, we now had three to six different diverging code paths, each adding friction, increasing the maintenance burden, and slowly draining our sanity. We find ourselves wondering: &quot;How is there not a universal standard for handling this already?&quot;</p><h2 id="introducing-llm-bridge">Introducing LLM-Bridge </h2><p>We&apos;ve gone through the pain, and this is why we&apos;re so excited to announce a brand new open-source package designed to streamline exactly these types of workflows: <code>llm-bridge</code>. </p><p>The package aims to be the bridge between the different LLM API formats by implementing a standard, universal format that losslessly represents any well-formed OpenAI, Anthropic, or Gemini API payload, as well as helper functions to convert our universal representation to and from any of these API providers&apos; formats. </p><p>We&apos;ve recently been dogfooding this package in the newest version of our Infinite Chat offering, and since it&apos;s been such a useful tool for us internally, we&apos;ve open-sourced it on <a href="https://github.com/supermemoryai/llm-bridge?ref=blog.supermemory.ai">Github</a> so that everyone in the community can contribute to and benefit from it.</p><h2 id="why-did-we-build-this">Why did we build this?</h2><p>We currently offer a product called Infinite Chat, a model-enhancing service which allows developers to extend the context window of any model far beyond its technical context limit. At a high level, we&apos;ve designed it to act as a proxy for LLM APIs, intercepting requests to the OpenAI, Anthropic, and Gemini APIs, extracting relevant data from them, and feeding additional context back into the requests as needed. This makes adoption seamless: all you have to do as a developer is prepend your provider&apos;s base URL with our API&apos;s URL, add your Supermemory API key, and we take care of the rest. That is, there is <em>no change</em> to the client interface&#x2014;you can keep using your SDK of choice no matter what.</p><pre><code class="language-JS">const client = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
  baseURL: &quot;https://api.supermemory.ai/v3/https://api.openai.com/v1&quot;,
  headers: {
    &quot;x-api-key&quot;: process.env.SUPERMEMORY_API_KEY,
  },
});

/// or new Anthropic() or new GoogleGenAI()</code></pre><p>This is a huge win for our users but it comes at a cost for us: we have to be able to process every single possible request, in any state, to any AI provider&apos;s API, in a way that is completely invisible to the client. As you might expect, this is not an easy problem to solve and a major pain point we encountered was simply in processing the different request shapes for each of the target APIs. The current state of the ecosystem is such that the OpenAI, Anthropic, and Google APIs are not cross-compatible. They all demand data to be sent in similar but slightly different formats. While they do all offer specific OpenAI-compatible endpoints, their compatibility isn&apos;t perfect and they don&apos;t always offer full feature parity with the proprietary endpoints. More importantly though, as developers, we shouldn&apos;t have to force our users to modify their choice of API to use our service.</p><p>As a result, the status quo is that anybody wishing to be able to accept any of these formats has to implement their own logic for each of them, or write their own ad-hoc and often leaky abstractions narrowly tailored to their needs. Unfortunately, all of that glue quickly balloons in complexity and silently drifts as vendors add features. This is the problem <code>llm-bridge</code> was built to solve.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.supermemory.ai/content/images/2025/07/standards_2x.png" class="kg-image" alt="We solved AI API interoperability" loading="lazy" width="1000" height="567" srcset="https://blog.supermemory.ai/content/images/size/w600/2025/07/standards_2x.png 600w, https://blog.supermemory.ai/content/images/2025/07/standards_2x.png 1000w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">From xkcd&apos;s &quot;Standards&quot; (</span><a href="https://xkcd.com/927?ref=blog.supermemory.ai"><span style="white-space: pre-wrap;">https://xkcd.com/927</span></a><span style="white-space: pre-wrap;">)</span></figcaption></figure><p>If you&apos;ve been active in this space recently, you may already be thinking of a few offerings that share the goal of unifying interactions with the different provider APIs (inb4 Vercel&apos;s AI SDK). These all have their place, but none of them quite fit our needs. Specifically, our solution had to:</p><ol><li>Be universal</li><li>Be lightweight</li><li>Retain full feature parity (multimodality, tool calls)</li><li>Support lossless transformations, and translations</li><li>Not require changes to the client interface</li></ol><p>There are plenty of products which meet 3 or 4 of these criteria, but none that meet all 5. So, we built our own.</p><h2 id="how-did-we-solve-it">How did we solve it?</h2><p>Our solution is <code>llm-bridge</code>, a library which defines a universal format for representing API requests for any of the major providers. With <code>llm-bridge</code>, you can do any processing you need on any request intended for any of the major providers, like changing the messages array, updating the system prompt, extracting multimodal data, and more. When you&apos;re done, you can use one of our helper functions to convert it into the right format for your target API, even if the target is different than that of the original payload. Because every vendor-specific oddity is stashed intact under original, you never lose multimodal blobs or tool definitions; because the same helpers wrap and translate errors, you can bubble one canonical exception type up your stack instead of juggling three dialects. In effect, three diverging code paths shrink to a symmetrical &#x201C;normalize &#x2192; process &#x2192; emit&#x201D; pipeline that&#x2019;s less than a dozen lines long yet preserves full feature parity across OpenAI, Claude, and Gemini.</p><figure class="kg-card kg-image-card"><img src="https://blog.supermemory.ai/content/images/2025/07/Screenshot-2025-07-06-at-14.41.01.png" class="kg-image" alt="We solved AI API interoperability" loading="lazy" width="2000" height="660" srcset="https://blog.supermemory.ai/content/images/size/w600/2025/07/Screenshot-2025-07-06-at-14.41.01.png 600w, https://blog.supermemory.ai/content/images/size/w1000/2025/07/Screenshot-2025-07-06-at-14.41.01.png 1000w, https://blog.supermemory.ai/content/images/size/w1600/2025/07/Screenshot-2025-07-06-at-14.41.01.png 1600w, https://blog.supermemory.ai/content/images/2025/07/Screenshot-2025-07-06-at-14.41.01.png 2156w" sizes="(min-width: 720px) 720px"></figure><p><code>llm-bridge</code> is designed to make working with these APIs simpler for all use-cases, whether you run a proxy for LLM APIs, are building middleware for LLMs, or are building consumer-facing apps. Here are some of the things it can do for you:</p><h3 id="transform-between-request-formats">Transform between request formats</h3><p>Transformations in <code>llm-bridge</code> are lossless, meaning that round-trip transformations preserve all the information from the original request. Every original key is tucked under a <code>_original</code> field, so <code>fromUniversal</code> can reproduce the exact JSON you fed in. You can use the universal format as your base too, if you&apos;re building a consumer-facing app and just need a way to construct and represent an abstract LLM API call.</p><pre><code class="language-JS">import { toUniversal, fromUniversal } from &quot;llm-bridge&quot;

const uni = toUniversal(&quot;openai&quot;, openaiReq)   // &#x2199; normalise
// ...inspect / mutate...
// add messages in the middle, edit messages, etc.

const claudeReq = fromUniversal(&quot;anthropic&quot;, uni)  // &#x2197; emit
</code></pre><p>You can also &quot;edit&quot; messages mid-conversion, for functionality like:</p><pre><code class="language-typescript">async function editMessages(universal: UniversalBody): Promise&lt;{
  request: UniversalBody
  contextModified: boolean
}&gt; {
  let processed = universal
  let contextModified = false
  
  // 1. Convert multimodal if needed
  if (hasMultimodalContent(universal)) {
    processed = await enhancedMultimodalProcessor(processed, {
      imageDescriptionService: async (data) =&gt; {
        // Call your vision API here
        return &quot;A photo of a sunset over mountains&quot;
      },
      documentParser: async (data, mimeType) =&gt; {
        // Parse PDF/document here
        return &quot;Document content extracted...&quot;
      }
    })
    contextModified = true
  }
  
  // 2. Add or edit context context
  processed.messages.push({
    id: generateId(),
    role: &apos;system&apos;,
    content: [{ type: &apos;text&apos;, text: &apos;Your memory here...&apos; }],
    metadata: { provider: processed.provider }
  })
  
  return { request: processed, contextModified: true }
}</code></pre><h3 id="error-handling">Error handling</h3><p>LLM providers each return their own distinct error formats, making consistent error handling tricky. With llm-bridge, you can translate errors from any provider into a single, consistent format, simplifying your error handling significantly.</p><pre><code class="language-JS">import { buildUniversalError, translateError } from &apos;llm-bridge&apos;

// Create a universal error
const error = buildUniversalError(
  &quot;rate_limit_error&quot;, 
  &quot;Rate limit exceeded&quot;,
  &quot;openai&quot;,
  { retryAfter: 60 }
)

// Translate to different provider formats
const anthropicError = translateError(error.universal, &quot;anthropic&quot;)
const googleError = translateError(error.universal, &quot;google&quot;)
</code></pre><h3 id="utilities">Utilities</h3><p>The library also ships helpers for the small but common things you may need when handling inputs and outputs for each of the APIs, like token counting.</p><h3 id="use-cases-and-examples">Use cases and examples</h3><ul><li>Translating between model calls <a href="https://github.com/supermemoryai/llm-bridge/blob/main/examples/basic-translation.ts?ref=blog.supermemory.ai">(For eg: Proxy Claude code requests to send them to Gemini instead)</a></li><li><a href="https://github.com/supermemoryai/llm-bridge/blob/main/examples/cost-optimizer.ts?ref=blog.supermemory.ai">Routing and optimizing costs</a></li><li>Analyzing images and <a href="https://github.com/supermemoryai/llm-bridge/blob/main/examples/image-analysis.ts?ref=blog.supermemory.ai">converting multi-modal into text to save money</a></li><li><a href="https://github.com/supermemoryai/llm-bridge/blob/main/examples/load-balancer.ts?ref=blog.supermemory.ai">Load balancing between different providers based on availability</a></li><li>Generating summaries using cheaper models</li></ul><figure class="kg-card kg-image-card"><img src="https://blog.supermemory.ai/content/images/2025/07/image.png" class="kg-image" alt="We solved AI API interoperability" loading="lazy" width="1244" height="486" srcset="https://blog.supermemory.ai/content/images/size/w600/2025/07/image.png 600w, https://blog.supermemory.ai/content/images/size/w1000/2025/07/image.png 1000w, https://blog.supermemory.ai/content/images/2025/07/image.png 1244w" sizes="(min-width: 720px) 720px"></figure><h3 id="dogfooding-at-supermemory">Dogfooding at supermemory</h3><p>We heavily use llm-bridge internally at supermemory to handle requests in our <a href="https://x.com/supermemoryai/status/1923122703009186217?ref=blog.supermemory.ai">Infinite Chat endpoint</a>, and many others, especially for conversational tasks. </p><p>This is, literally, our code, where the manipulation of data happens inside <code>async r =&gt; {}</code>. </p><figure class="kg-card kg-image-card"><img src="https://blog.supermemory.ai/content/images/2025/07/image-1.png" class="kg-image" alt="We solved AI API interoperability" loading="lazy" width="659" height="428" srcset="https://blog.supermemory.ai/content/images/size/w600/2025/07/image-1.png 600w, https://blog.supermemory.ai/content/images/2025/07/image-1.png 659w"></figure><p><br>We recognize any providers have OpenAI-compatible format support, but we still decided to build LLM-bridge because we, as supermemory, wanted to allow developers to use their preferred SDKs and format, and leave the worry of memory to us.</p><h3 id="vibe-coding">Vibe coding</h3><p>This project was vibe-coded using Claude Code. because no one wants to go through the pain of going through every single model provider :( </p><p>But we think that&apos;s a good thing. The TL;DR is that llm-bridge is almost <strong><em>fully</em></strong> <strong><em>covered</em></strong> with tests using <a href="https://vitest.dev/?ref=blog.supermemory.ai">vitest</a>, where very single unique implementation of llm-bridge is verifiably testable inside the <a href="https://github.com/supermemoryai/llm-bridge/tree/main/test?ref=blog.supermemory.ai">tests/ folder</a></p><h2 id="putting-it-all-together">Putting it all together</h2><p>When your app receives an LLM request, all you have to do is pass it through <code>llm-bridge</code>&#x2014;the rest happens automatically. The library handles converting requests from any provider into a universal representation, so you can seamlessly inspect, modify, or reroute them without worrying about vendor-specific details. </p><p>By standardizing API interactions into a single format, it becomes trivial to add failover, traffic splitting, analytics, or cost-based routing&#x2014;all without touching the client&apos;s existing integration. We built this to simplify our own workflows, and we&#x2019;re excited to see how it can streamline yours. Check out the <a href="https://github.com/supermemoryai/llm-bridge?ref=blog.supermemory.ai">repo</a>, try it out in your own stack, and let us know what you think!</p>]]></content:encoded></item><item><title><![CDATA[2 Approaches For Extending Context Windows in LLMs]]></title><description><![CDATA[<p>Transformer-based large language models have become the poster boys of modern AI, yet they still share one stark limitation: a finite context window. Once that window overflows, performance drops like a rock or the model forgets key details.</p><p>This guide walks through two complementary strategies that lift those limits:</p><ul><li><strong>Semantic</strong></li></ul>]]></description><link>https://blog.supermemory.ai/extending-context-windows-in-llms/</link><guid isPermaLink="false">686832bb857875000147196f</guid><dc:creator><![CDATA[Naman Bansal]]></dc:creator><pubDate>Fri, 04 Jul 2025 20:31:53 GMT</pubDate><media:content url="https://blog.supermemory.ai/content/images/2025/07/8.webp" medium="image"/><content:encoded><![CDATA[<img src="https://blog.supermemory.ai/content/images/2025/07/8.webp" alt="2 Approaches For Extending Context Windows in LLMs"><p>Transformer-based large language models have become the poster boys of modern AI, yet they still share one stark limitation: a finite context window. Once that window overflows, performance drops like a rock or the model forgets key details.</p><p>This guide walks through two complementary strategies that lift those limits:</p><ul><li><strong>Semantic Compression:</strong> Shrink a single, extremely long document so it slips inside an ordinary LLM window without architectural changes</li><li><strong>Infinite Chat with Supermemory:</strong> Keep multi-hour conversations coherent by retrieving only the most relevant history at every turn</li></ul><p>Along the way, you&#x2019;ll set up Python environments, run clustering and summarization pipelines, and slot a transparent proxy into any OpenAI-compatible SDK to help your team ship better products faster.</p><p><strong>Note:</strong> You can find the code in the <a href="https://github.com/namancoderpro/semantic-compression/tree/main?ref=blog.supermemory.ai">GitHub repo here.</a></p><h2 id="why-llms-struggle-with-length">Why LLMs Struggle With Length</h2><p>Self-attention, the <a href="https://arxiv.org/abs/1706.03762?ref=blog.supermemory.ai">mechanism</a> that gives transformers their uncanny language skills, scales quadratically with sequence length. In short, every time you double the input, the compute necessary to infer it quadruples. And while current foundational models can stretch their window from 128,000 tokens and up, sooner or later, the window dries up. When it does, you face an unpleasant choice:</p><ol><li>Truncate the context, losing historical context altogether</li><li>Summarize the context, maintaining some history, but potentially losing important details</li></ol><p>Even with a rolling summarizer, you&apos;re sending a huge amount of tokens back and forth to maintain a mere outline of your historical context.</p><p>A smarter answer is to adapt your prompts dynamically so the model sees everything it truly needs and nothing it doesn&#x2019;t. In this guide, we&apos;ll look at a couple of different approaches to this, the first of which is called &quot;Semantic Compression.&quot;</p><h2 id="extending-context-part-one-semantic-compression">Extending Context Part One: Semantic Compression</h2><p>What if you could drop a full-length, unabridged novel into a model&apos;s context window and get useful answers back without any hallucinations? This is exactly the kind of context extension that semantic compression makes possible.</p><p>The technique, outlined in <a href="https://aclanthology.org/2024.findings-acl.306.pdf?ref=blog.supermemory.ai">this paper</a>, adds a compression layer that squeezes text down to roughly one-sixth its original size.</p><figure class="kg-card kg-image-card"><img src="https://i.postimg.cc/hGZx1Y4H/Screenshot-2025-07-05-at-00-27-43.png" class="kg-image" alt="2 Approaches For Extending Context Windows in LLMs" loading="lazy" width="995" height="449"></figure><hr><p>TL;DR: How the algorithm in the paper works:</p><ol><li>Segment the input: Split the document into sentence blocks that fit a summarizer&#x2019;s 512-token limit.</li><li>Embed and graph: Turn each block into a MiniLM vector and build a similarity graph.</li><li>Spectral clustering: Detect topic boundaries so each cluster covers a coherent theme.</li><li>Run parallel summaries: Run a lightweight BART variant on every cluster concurrently.</li><li>Reassemble the chunks: Glue summaries back together in their original order.</li><li>Feed as context: Send the compressed text plus your question to any LLM.</li></ol><hr><p>Here&apos;s a more detailed technical breakdown of how it works:</p><p>The paper argues that most of natural language is incredibly redundant and wasteful. Multiple sentences often convey overlapping concepts. Entire paragraphs might elaborate on a single key point. The compression method identifies and preserves information-dense segments while removing repetitive portions, in effect, increasing the context window.</p><p>Often, real-world textual content has hierarchical structures, where each section explores a particular topic, and all the sections are somewhat mutually independent. These sections resemble <strong>cliques</strong> in graphs.</p><p>A clique is a group of nodes where everyone connects to everyone else. They form dense clusters of connections.</p><p>So, the algorithm starts by constructing a similarity graph from the text corpus. Then, several spectral clustering algorithms are implemented to identify the underlying cliques or topic structures.</p><p>Traditional clustering algorithms like k-means assume clusters are roughly spherical and similar in size. But real documents don&apos;t work that way. You might have a dense technical section followed by a sparse narrative passage. One subtopic might span three paragraphs, while another gets covered in a single dense paragraph.</p><p>Spectral clustering analyzes the eigenstructure of the similarity matrix. Basically, it finds the natural ways the graph wants to split apart. It can detect these irregular semantic boundaries that other clustering methods miss.</p><p>These chunks of text for each section are then processed in parallel using a pre-trained summarization model on them concurrently. The BART variant is specifically chosen for its ability to create abstractive summaries. It synthesizes new content that captures multiple concepts more efficiently.</p><p>The real technical achievement is maintaining semantic fidelity while achieving 6:1 compression ratios. Each summary sentence must encode roughly six times the semantic density of typical prose. This works because BART&apos;s denoising autoencoder training taught it to identify and preserve essential information while discarding redundant expressions.</p><p>Finally, these summarized chunks are then stitched together and passed to the LLM.</p><p>Experiments from the paper show that Llama 2 retains more than 90 percent accuracy on pass-key retrieval tasks, even when the source material balloons past 60,000 tokens. Below, we&apos;ll look at what it takes to build your own semantic compression layer to handle outsized prompt sizes.</p><h3 id="setting-up-the-environment">Setting Up the Environment</h3><p>Modern context-extension pipelines lean on a handful of mature Python libraries. Install the following:</p><pre><code class="language-bash">pip install torch sentencepiece sentence-transformers scikit-learn tqdm
</code></pre><p>This one-liner pulls in <strong>PyTorch</strong> for tensor math, <strong>Transformers</strong> for both the summarizer and the downstream LLM, <strong>Sentence-Transformers</strong> for MiniLM sentence embeddings, <strong>scikit-learn</strong> for spectral clustering, and <strong>tqdm</strong> for friendly progress bars.</p><p>Once the packages are in place, you&#x2019;re ready to ingest raw text.</p><h3 id="loading-and-normalizing-text">Loading and Normalizing Text</h3><p>Documents arrive in a variety of formats like PDFs, scraped HTML, or plain UTF-8 files. Normalizing them up front removes hidden Unicode quirks and errant whitespace that would otherwise derail token counts later.</p><p>Create a Python file and write the following code:</p><pre><code class="language-python">from pathlib import Path
import re, unicodedata

def load_text(path: Path) -&gt; str:
    raw = path.read_text(encoding=&quot;utf-8&quot;, errors=&quot;ignore&quot;)
    norm = unicodedata.normalize(&quot;NFKC&quot;, raw)
    return re.sub(r&quot;\s+&quot;, &quot; &quot;, norm).strip()
</code></pre><p>The helper reads the file, applies Unicode NFKC normalization so ligatures and full-width characters collapse into canonical forms, then squeezes every run of whitespace into a single space. The result is a clean, single-string corpus.</p><p>With a tidy text string in hand, you can safely start dividing it into manageable blocks.</p><h3 id="breaking-the-document-into-sentence-blocks">Breaking the Document into Sentence Blocks</h3><p>Self-contained sentence blocks become the nodes of a similarity graph. They must be short enough for a summarizer (&#x2248;512 tokens) yet long enough to preserve local context.</p><p>Add the following code to the file:</p><pre><code class="language-python">import nltk
nltk.download(&quot;punkt&quot;)
from transformers import AutoTokenizer

tok_sum = AutoTokenizer.from_pretrained(&quot;facebook/bart-large-cnn&quot;)

def sentence_blocks(text: str, limit=512):
    sents = nltk.sent_tokenize(text)
    block, blocks, start_idx = [], [], 0
    for sent in sents:
        tentative = &quot; &quot;.join(block + [sent])
        enc = tok_sum.encode(tentative)
        if len(enc) &gt; limit and block:
            blocks.append((&quot; &quot;.join(block), start_idx))
            start_idx += len(block)
            block = [sent]
        else:
            block.append(sent)
    if block:
        blocks.append((&quot; &quot;.join(block), start_idx))
    return blocks   # [(text, original_position)]
</code></pre><p>NLTK tokenizes sentences, while the BART tokenizer checks whether appending another sentence would overflow the 512-token guardrail. Each completed block is stored with its original position so you can later stitch summaries back together in order.</p><p>Now that the text is chunked, you can quantify how similar those chunks are to one another.</p><h3 id="building-a-similarity-graph">Building a Similarity Graph</h3><p>To discover topic boundaries, every block needs a numeric fingerprint. MiniLM embeddings supply that signature; cosine similarity converts it into an adjacency matrix ready for graph algorithms.</p><pre><code class="language-python">import torch
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

encoder = SentenceTransformer(&quot;all-MiniLM-L6-v2&quot;)

def similarity_graph(block_texts):
    with torch.inference_mode():
        emb = encoder.encode(
            block_texts,
            batch_size=64,
            convert_to_tensor=True
        )
    return cosine_similarity(emb.cpu())   # dense numpy array
</code></pre><p><code>SentenceTransformer</code> generates 384-dimension vectors, and <code>cosine_similarity</code> fills a dense matrix where each cell reflects semantic closeness between two blocks. This matrix is the substrate for spectral clustering, which is used for topic discovery.</p><h3 id="spectral-clustering-to-detect-topics">Spectral Clustering to Detect Topics</h3><p>Spectral clustering partitions the graph so that blocks inside the same cluster talk about the same thing, while blocks across clusters drift apart.</p><pre><code class="language-python">from math import ceil
from sklearn.cluster import SpectralClustering

def cluster_blocks(sim_matrix, block_tokens, target=450):
    total = sum(block_tokens)
    n_clusters = ceil(total / target)
    sc = SpectralClustering(
        n_clusters=n_clusters,
        affinity=&quot;precomputed&quot;,
        assign_labels=&quot;discretize&quot;,
        random_state=0
    )
    return sc.fit_predict(sim_matrix)   # cluster index per block
</code></pre><p>The code picks a cluster count by dividing total tokens by a &#x201C;comfort&#x201D; window (&#x2248;450). <code>SpectralClustering</code> then labels each block, grouping semantically tight neighborhoods together.</p><p>Clusters become the units you feed to the summarizer.</p><h3 id="summarizing-clusters-in-parallel">Summarizing Clusters in Parallel</h3><p>With coherent topics identified, you can shrink each one independently. Running multiple summaries concurrently makes the pipeline as fast as the GPU will allow.</p><pre><code class="language-python">from concurrent.futures import ThreadPoolExecutor
from transformers import pipeline

summarizer = pipeline(
    &quot;summarization&quot;,
    model=&quot;facebook/bart-large-cnn&quot;,
    batch_size=4   # CPU or MPS-compatible
)

def compress_cluster(text):
    return summarizer(
        text,
        max_length=256,
        min_length=64,
        do_sample=False
    )[0][&quot;summary_text&quot;]

def summarise_chunks(blocks, labels):
    clusters = {}
    for (txt, pos), lab in zip(blocks, labels):
        clusters.setdefault(lab, []).append((pos, txt))
    ordered = [&quot; &quot;.join(t for _, t in sorted(v)) 
               for v in (clusters[k] for k in sorted(clusters))]
    with ThreadPoolExecutor(max_workers=4) as pool:
        return list(pool.map(compress_cluster, ordered))
</code></pre><p>A <code>ThreadPoolExecutor</code> fans out four concurrent calls to the BART summarizer. Each cluster collapses into a 64 to 256-token synopsis, slashing the overall length by a factor of six to eight. All summaries come back ordered, ready for reassembly.</p><p>Next, we&apos;ll merge the compressed snippets into a single prompt.</p><h3 id="reassembling-and-prompting-the-llm">Reassembling and Prompting the LLM</h3><p>Putting the pieces together is as simple as concatenation, but with a big payoff: a formerly unwieldy document now fits inside a standard context window.</p><pre><code class="language-python">if __name__ == &quot;__main__&quot;:
    path = Path(&quot;input.txt&quot;)
    text = load_text(path)

    blocks = sentence_blocks(text)

    block_texts = [txt for txt, _ in blocks]
    block_tokens = [len(tok_sum.encode(t)) for t in block_texts]

    sim_matrix = similarity_graph(block_texts)
    labels = cluster_blocks(sim_matrix, block_tokens)

    compressed_chunks = summarise_chunks(blocks, labels)
    compressed_text = &quot;\n\n&quot;.join(compressed_chunks)

    question = &quot;Summarize the protagonist&apos;s character arc.&quot;

    prompt = f&quot;&lt;s&gt;[SYSTEM]\n{compressed_text}\n[/SYSTEM]\n[USER]\n{question}\n[/USER]\n&quot;

    inputs = tok(prompt, return_tensors=&quot;pt&quot;)
    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}

    out = model.generate(**inputs, max_new_tokens=512)

    print(tok.decode(out[0], skip_special_tokens=True))
</code></pre><p>The compressed text becomes a system message, the user&#x2019;s query follows, and the model answers as if it had read the entire original, because semantically, it has.</p><p>Also, add this line after the imports:</p><pre><code class="language-python">DEVICE = torch.device(&quot;mps&quot; if torch.backends.mps.is_available() else &quot;cpu&quot;)
</code></pre><p>I&apos;m on an M1 Mac, so the code falls back to my CPU since a CUDA GPU isn&apos;t available.</p><p>The complete code is available in the <a href="https://github.com/namancoderpro/semantic-compression/tree/main?ref=blog.supermemory.ai">GitHub repo here.</a></p><h2 id="extending-context-part-two-infinite-chat">Extending Context Part Two: Infinite Chat</h2><p>Semantic compression helps you manage massive inputs that would typically exceed a model&apos;s context window. But what if your context grows because users keep interacting? That&#x2019;s where Supermemory&apos;s <a href="https://supermemory.ai/docs/model-enhancement/context-extender?ref=blog.supermemory.ai">Infinite Chat</a> comes in.</p><p>Long conversations eventually overflow the largest context windows. Like semantic compression, Supermemory&apos;s Infinite Chat adds a contextual management layer in front of the model input.</p><p>Drop Supermemory&apos;s proxy in front of any OpenAI&#x2011;compatible API, store and index every interaction, and return only the ones that matter at request time.</p><h3 id="how-supermemorys-infinite-chat-works">How Supermemory&apos;s Infinite Chat Works</h3><p>Infinite chat applies a four&#x2011;stage retrieval pipeline every time a new message arrives.</p><p>First, it chunks the growing transcript into overlapping blocks. Chunking at sentence boundaries preserves semantics while giving the retriever fine&#x2011;grained control over what can be recalled later.</p><p>Afterwards, each chunk is embedded and stored in an index, allowing for fast similarity search at inference time.</p><p>Finally, when the running prompt nears its token budget, the proxy scores every stored chunk by a blend of relevance (how close the chunk&#x2019;s embedding sits to the new user prompt) and recency (a timestamp decay.)</p><p>The weighting keeps the assistant grounded in the present conversation while still able to reach for older but contextually rich information.</p><p>It&apos;s worth mentioning that infinite chat enforces a hard budget: it orders chunks by score, retains the top slices that still fit inside the target length, and rebuilds the prompt. Everything else is retained for if and when it becomes relevant again.</p><p>The entire operation adds only milliseconds of latency, yet can slash token spend by more than half during marathon chats.</p><h3 id="setting-up-infinite-chat">Setting Up Infinite Chat</h3><p>First, pull in the necessary imports:</p><pre><code class="language-python">import openai
import os
</code></pre><p>Next, configure your OpenAI client with the Supermemory Infinite Chat proxy:</p><pre><code class="language-python">openai.api_base = &quot;https://api.supermemory.ai/v3/https://api.openai.com/v1&quot;
openai.api_key = os.environ.get(&quot;OPENAI_API_KEY&quot;)  # Your regular OpenAI key
openai.default_headers = {
    &quot;x-api-key&quot;: os.environ.get(&quot;SUPERMEMORY_API_KEY&quot;),  # Your supermemory key
}
</code></pre><p>And finally, create a chat completion with unlimited context:</p><pre><code>response = openai.ChatCompletion.create(
  model=&quot;gpt-4o-mini&quot;,
  messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Your message here&quot;}]
)
</code></pre><p>Behind the scenes, the proxy stores messages in an embedding index, ranks relevance plus recency, and injects top snippets when the running context would otherwise blow up.</p><h2 id="where-to-go-from-here">Where to Go from Here</h2><p>Semantic Compression and Supermemory&apos;s Infinite Chat attack the context-limit problem from opposite ends: the former condenses massive static inputs while the latter curates sprawling, dynamic interactions.</p><p>Semantic Compression multiplies an LLM&#x2019;s effective window for single, monolithic documents; Infinite Chat keeps rolling dialogues sharp long after the model&apos;s context window would have faded.</p><p>Together, they form a practical toolkit for any team that wants to push language models beyond their native inference limits.</p><p>Go try Supermemory today. <a href="https://supermemory.ai/?ref=blog.supermemory.ai">It&apos;s free!</a></p>]]></content:encoded></item><item><title><![CDATA[LLM Costs Skyrocketing? Real Experts Weigh In]]></title><description><![CDATA[<p>In this blog, we&apos;re gonna walk through a fictional story, while learning how to optimize LLMs for cost, and the associated tradeoffs.</p><hr><p><strong>Tuesday, 10 June, 2:14 PM PST</strong></p><p>The billing alert hit. I was halfway through a product demo, nodding along to myself on Zoom, saying something</p>]]></description><link>https://blog.supermemory.ai/llm-costs-skyrocketing-real-experts-weigh-in/</link><guid isPermaLink="false">68614d0a8578750001471929</guid><category><![CDATA[Learning]]></category><dc:creator><![CDATA[Naman Bansal]]></dc:creator><pubDate>Tue, 01 Jul 2025 15:53:39 GMT</pubDate><media:content url="https://blog.supermemory.ai/content/images/2025/06/7.png" medium="image"/><content:encoded><![CDATA[<img src="https://blog.supermemory.ai/content/images/2025/06/7.png" alt="LLM Costs Skyrocketing? Real Experts Weigh In"><p>In this blog, we&apos;re gonna walk through a fictional story, while learning how to optimize LLMs for cost, and the associated tradeoffs.</p><hr><p><strong>Tuesday, 10 June, 2:14 PM PST</strong></p><p>The billing alert hit. I was halfway through a product demo, nodding along to myself on Zoom, saying something vaguely confident about &#x201C;intelligent LLM agents.&#x201D; On my second monitor, an email notification popped up:</p><p><em>&#x201C;LLM Spend Exceeded Threshold&#x201D;</em></p><p>That threshold was already 3x higher than last month.</p><p>After the call, I did what any rational and responsible engineer would do: opened the dashboard, stared at the numbers, and refreshed. Twice. Maybe it was a glitch. Just maybe&#x2026;</p><p>Enough denial. With a little bit of digging in, it became clear: our LLM usage per request had exploded. Tokens in. Money out.&#xA0;</p><p>So, I got to the task.&#xA0;</p><p>I wish I could say I fixed it that afternoon, but I didn&#x2019;t. Nevertheless, this is the story of how I used 5 different techniques to stop our token bill from killing us, and everything I learned about LLM cost optimization along the way.</p><h2 id="proper-context-management-only-load-what%E2%80%99s-needed">Proper Context Management: Only Load What&#x2019;s Needed</h2><p><strong>Wednesday, 11 June, 3:30 PM PST</strong></p><p>The first person I called was Nikhil, AI Strategy Architect @ <a href="https://www.techolution.com/?ref=blog.supermemory.ai"><u>Techolution</u></a>. His company uses LLMs to help enterprise customers modernize legacy code with millions of lines and decades-old COBOL and Fortran.&#xA0;</p><p>When we got on a call, he said:&#xA0;&#x201C;We don&#x2019;t load what we don&#x2019;t need.&#x201D;</p><p>&#x201C;What does that mean?&#x201D; I asked.</p><p>&#x201C;Well,&#x201D; he said, &#x201C;if we&#x2019;re modernizing a particular function, we don&#x2019;t send the whole codebase to the LLM. We build dependency graphs. So we only load the components that the function relies on, nothing else.&#x201D;</p><p>That makes sense. Sending everything into an LLM every time bloats the context, slows things down, and burns tokens, increasing the costs.</p><p>What Nikhil&#x2019;s team did instead was smart. Their system allowed them to figure out the parts of the codebase that depended on each other. Then, they chunked and indexed those relevant parts so the LLM would only receive context that mattered to that task.</p><p>He explained that this works because large language models don&#x2019;t &#x201C;understand&#x201D; code in the way we do. They pattern-match based on what they&#x2019;re given. So, if you feed them 3,000 lines of mostly irrelevant code, they&#x2019;ll waste time (and tokens) trying to make sense of it all.</p><p>This approach is backed by plenty of research. In particular,<a href="https://arxiv.org/abs/2407.02409?ref=blog.supermemory.ai"><u> Kabongo et al., 2024</u></a> suggest that <em>&#x201C;A targeted approach to context, where only task-relevant information is provided, is generally more beneficial for model performance than a more comprehensive one.&#x201D;</em></p><p>That same evening, I looked into our app&#x2019;s token logs again and realized we were doing the exact opposite. Every time a user asked a question, we were injecting the <em>entire</em> chat history. Didn&#x2019;t matter if the current question had nothing to do with the previous ten; we were loading it all.&#xA0;</p><p>While trying to figure out a solution to this, I <em>accidentally</em> landed on Twitter to ask some questions and learn from others. Obviously, I wasn&apos;t scrolling memes tsk tsk.</p><p>And then, I found out about Supermemory&#x2019;s <a href="https://supermemory.ai/docs/model-enhancement/context-extender?ref=blog.supermemory.ai"><u>Infinite Chat API</u></a>. It plugs into your existing LLM stack and allows you to extend your model&#x2019;s context window infinitely, while simultaneously optimizing long conversations using chunking, smart retrieval, and automatic token management to reduce LLM costs.</p><figure class="kg-card kg-image-card"><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXd7dFconefDcVgE6clR_PUftST55hrW57DBir45tJwyJNhD_L-U_rG8KUrevpHby2ouSAEwn_8xwElE2ppVmKdRkeHoNFUbkefhYwTCJ0K41P1bn_9RYvXoi1cQPjjPZ1C_GlTlyA?key=Qoh941QQBb2Ndy3uGBNIag" class="kg-image" alt="LLM Costs Skyrocketing? Real Experts Weigh In" loading="lazy" width="624" height="321"></figure><p>The best part? It took me only 4 minutes and 20 seconds to set up. It has a generous free plan and works with GPT 3.5, GPT-4, 4o, Claude 3 models, and any provider with an OpenAI-compatible endpoint.</p><h2 id="intelligent-prompt-engineering-ab-testing">Intelligent Prompt Engineering + A/B Testing</h2><p><strong>Wednesday, 11 June, 4:30 PM PST</strong></p><p>An hour later, once I&#x2019;d stopped injecting 10,000 tokens of unnecessary chat history into every API call, I figured we were safe.</p><p>Spoiler alert: we weren&#x2019;t. The prompts themselves were a mess.</p><p>Things like <em>&#x201C;You are a helpful assistant that&#x2026;&#x201D; </em>and <em>&#x201C;Write in a helpful, friendly tone&#x201D;</em>, which, okay, are fine once. But we were sending those sentences on every single request. Hundreds of times a day. We were paying LLMs to be reminded of their job. Repeatedly. </p><p>That&#x2019;s when I talked to Nikita Sherbani, CEO at <a href="https://www.aiscreen.io/?ref=blog.supermemory.ai"><u>AIScreen</u></a>. They use LLMs inside a digital signage platform for real-time copy suggestions and templated creative generation. Pretty different product, but the same underlying problem.</p><p><em>&#x201C;Our biggest cost spike,&#x201D; </em>he told me<em>, &#x201C;came from over-relying on high-token completion calls for relatively simple output. What worked? We rewrote prompts to be system-level pre-context instead of repeating instruction per request, cutting token use by ~38%.&#x201D;</em></p><p>It sounds obvious in hindsight: move static instructions to the system message, and stop sending them over and over again. We did the same. Shifted the tone, structure, and expectations to the system prompt.&#xA0;</p><p>Kept user instructions short and scoped. That alone helped trim the fat.</p><p>But there was still one lingering question: <em>Which version of the prompt is actually the best?</em></p><p>For that, I called Aryan. He works as an AI engineer at <a href="https://stellafoster.com/?ref=blog.supermemory.ai"><u>Health GC</u></a>, building AI voice care agents for elderly patients, which means their LLMs have to be fast, clear, and incredibly reliable. He told me they&#x2019;d been running A/B tests on prompts for months.</p><p>According to him, <em>&#x201C;Once we started treating the prompt like code by testing variations and reusing shared structures, our costs went down fast.&#x201D;</em></p><p>Instead of guessing, Aryan&#x2019;s team actively tests multiple prompt variants in production. They look at everything: output correctness, latency, token usage, and user satisfaction. Often, he calls his LLM using other LLMs and checks the response generated to test it against these thresholds.</p><h2 id="prompt-caching">Prompt Caching</h2><p><strong>Thursday, 12 June, 10:00 AM PST</strong></p><p>Prompt optimization helped, but we were still spending more than we should. Aryan had mentioned another quick hack we could use.</p><p>He explained it simply: most teams forget that their system prompt (the big template that defines behavior and style) doesn&#x2019;t have to be sent every time.</p><p>In their case, Health GC&#x2019;s AI voice agents for elderly care rely on a long, structured prompt with very small dynamic changes. And in the beginning, they were passing that entire prompt with every single request. Which meant they were basically paying to reintroduce the model to itself over and over again.</p><p>A lot of providers actually have built-in ways to avoid this. Gemini and Anthropic, for example, let you cache the system prompt with the model backend itself. Instead of repeating the entire text each time, you:</p><ul><li>Cache the prompt once at the start of the session</li><li>Pass only a reference variable or ID with each request</li><li>Include just the dynamic parts (like a user transcript or updated state)</li></ul><p>Aryan told me they switched to this approach and immediately saw costs drop. The tokens used for the static instructions were no longer billed the same way on each call.&#xA0;</p><p>Nicolas, Founder @ <a href="https://introwarm.com/?ref=blog.supermemory.ai"><u>Introwarm</u></a>, also implemented prompt caching. His tool analyzes LinkedIn profiles and company data to generate authentic, personalized email openers for sales teams. </p><p><em>&#x201C;[We] implemented a Redis layer that stores personalization insights for similar profiles/companies. If someone&apos;s personalizing emails to multiple people at the same company, we reuse company-specific insights and just vary the personal touches.&#x201D;&#xA0;</em></p><p>His smart caching had a 23% hit rate, saving him 15% on his monthly LLM costs.</p><p>We looked into our setup and realized we were doing exactly what they used to, so we followed their lead.&#xA0;</p><p>If your prompts are long templates with only a small variable changing each call, check whether your model provider supports prompt caching. It&#x2019;s the easiest optimization you&#x2019;ll ever make.</p><h2 id="structured-outputs">Structured Outputs</h2><p><strong>Thursday, 12 June, 11:00 AM PST</strong></p><p>Okay, I was done with most of the optimizations on the input side. Now, it was time to look at what the model was spitting out. By default, we were letting it generate free-form text: long paragraphs with different phrasings every time.</p><p>However, I realized that structured outputs are the better choice. Instead of asking the model to just &#x201C;answer,&#x201D; I asked it to return data in a specific schema:</p><ul><li>A JSON object with known keys</li><li>A numbered list of options</li><li>A simple numeric score</li></ul><p>This not only helps with downstream processing, but also cuts down on unnecessary tokens generated on output. A structured JSON is almost always shorter than an essay.</p><p>But there are also downsides to this approach that I found. LLMs aren&#x2019;t deterministic, so sometimes they&#x2019;ll slip up, especially with larger contexts. They&#x2019;ll return unexpected formats, which can lead to validation errors. Supermemory also switched some of their tasks from an LLM to purpose-built libraries to counter this in their product.</p><p>Well, I was on a spree! All 4 things I tried out worked almost perfectly. I was feeling damn confident, thinking to myself, &#x201C;I can build literally anything.&#x201D;</p><p>But then came the dark days.</p><h2 id="what-not-to-do">What Not To Do</h2><p><strong>Thursday, 12 June - Saturday, 14 June</strong></p><p><strong>12 June, 8:25 PM PST</strong></p><p>I decided to train our own open-source language model to save on per-token costs. On paper, this sounded smart. In practice, it meant provisioning multiple A100 GPUs, tuning datasets, and spending days just getting a stable training loop.</p><p>By the time we got the first version running, it barely produced coherent outputs. Instructions were inconsistent, completions were often unrelated to the prompt, and performance was nowhere near GPT-3.5 or Claude 3. The total cost in compute credits and engineering hours ended up far higher than simply using a managed API.</p><p>Nikhil echoes this same sentiment. According to him, <em>&#x201C;We had this genius idea to try out open-source models. Back then, Mistral was huge. Everyone was using Mistral for everything. So we tried creating our own fine-tuned version of Mistral. We put it on GCP, but the hosting and inference costs were ridiculous. Even quantizing didn&#x2019;t help much. In the end, we had to drop the idea. It was a good learning: if you want to build and deploy your own model, the associated costs are so high that it&#x2019;s usually better to leave inference to the big guys.&#x201D;</em></p><p><strong>Friday, 13 June, 11:00 AM PST</strong></p><p>Thought maybe I could get clever by quantizing a larger model for deployment on the edge. I read half a blog post about 4-bit quantization and convinced myself this would work.</p><p>It did run cheaper. But the drop in output quality was so bad we couldn&#x2019;t use it for anything customer-facing.</p><p>Abhisek Shah, CEO at <a href="https://testlify.com/?ref=blog.supermemory.ai"><u>Testlify</u></a>, also went through the same thing:</p><p><em>&#x201C;We initially tried quantizing larger models for deployment on edge, but the drop in output quality made it a no-go. Sometimes cheaper &#x2260; better.&#x201D;</em></p><p>One more Friday wasted, uff.</p><p><strong>Friday, 14 June, 7:30 PM PST</strong></p><p>Still determined to squeeze out more savings, I figured that batching requests was the next obvious win.</p><p>I set up serverless endpoints to handle the batch processing. On paper, it looked perfect: elastic scaling, no idle servers, automatic cost optimization.</p><p>But, in what can only be called life, it didn&#x2019;t work out as I thought it would.</p><p>Cold starts made latency swing all over the place. Sometimes a request would return in 300 milliseconds. Other times, it sat for ten seconds doing nothing. Because our batches were large, even a short delay meant we burned through overage credits faster than I could track them.</p><p>Hidden idle fees made it worse than a dedicated container. After two days of tweaking configurations and trying different scaling policies, I gave up.</p><h2 id="don%E2%80%99t-always-use-llms">Don&#x2019;t Always Use LLMs</h2><p><strong>Monday, 17 June, 10:00 AM PST</strong></p><p>Well, along the way, I found out that we were using LLMs for things that didn&#x2019;t really need them. And, we did this, admittedly, because it&#x2019;s easy and sounds cool.</p><p><em>&quot;AI-powered&quot;</em> is all the buzz today, isn&#x2019;t it?</p><p>But, sometimes, those operations can be done manually, saving a lot of cost and often leading to better outputs as well. Nikhil had said the same thing to me on the call a few days back, <em>&#x201C;It&#x2019;s almost ironic that you need more people in the planning stage to save money</em> <em>on AI later. Because if you don&#x2019;t plan, you&#x2019;ll end up using LLMs for things a human or a small deterministic script could do.&#x201D;</em></p><p>When he removed some of his code modernization workflows from his LLM processes and instead did them manually, he saw much lower costs.</p><p>You need to take a deep, hard look at your own pipeline and reflect on whether an LLM makes sense or not. That is where the answer lies for most small companies.</p><h2 id="conclusion">Conclusion</h2><p><strong>Today, 20 June, 11:00 PM PST</strong></p><p>Looking back, the learnings are pretty clear. It&#x2019;s a bunch of small, practical decisions: loading only the context we needed, writing better prompts, batching requests, and sometimes just using simpler tools instead of defaulting to an LLM.</p><p>If I had to do it again, I&#x2019;d start planning for this earlier. As Nikhil put it, it&#x2019;s almost funny how you need more people thinking ahead to avoid wasting money later.</p><p>And if you&#x2019;re in the same spot, don&#x2019;t feel like you have to build every piece yourself. <a href="http://supermemory.ai/?ref=blog.supermemory.ai"><u>Supermemory</u></a> ended up saving us a lot of time and cost. Their context extender alone reduced our usage by over a third, and it was easy to plug into our stack.</p><p>Hope this log helps you avoid some of the trial and error and keep your costs under control as you scale.</p><hr><p><em><strong>Disclaimer: </strong>I, the developer, am fake. The log of a developer working at an AI startup is a creative structure we undertook for experimentation and engagement. All the content and experts, and their opinions are real, though.</em></p>]]></content:encoded></item><item><title><![CDATA[Best Open-Source Embedding Models Benchmarked and Ranked]]></title><description><![CDATA[<p>If your AI agent is returning the wrong context, it&#x2019;s probably not your LLM, but your embedding model. Embeddings are the hidden engine behind retrieval-augmented generation (RAG) and memory systems. The better they are, the more relevant your results, and the smarter your app feels. </p><p>But here&#x2019;</p>]]></description><link>https://blog.supermemory.ai/best-open-source-embedding-models-benchmarked-and-ranked/</link><guid isPermaLink="false">685e35b085787500014718f3</guid><category><![CDATA[Learning]]></category><dc:creator><![CDATA[Naman Bansal]]></dc:creator><pubDate>Fri, 27 Jun 2025 16:54:26 GMT</pubDate><media:content url="https://blog.supermemory.ai/content/images/2025/06/6.webp" medium="image"/><content:encoded><![CDATA[<img src="https://blog.supermemory.ai/content/images/2025/06/6.webp" alt="Best Open-Source Embedding Models Benchmarked and Ranked"><p>If your AI agent is returning the wrong context, it&#x2019;s probably not your LLM, but your embedding model. Embeddings are the hidden engine behind retrieval-augmented generation (RAG) and memory systems. The better they are, the more relevant your results, and the smarter your app feels. </p><p>But here&#x2019;s the problem: there are dozens of open-source models out there, and you don&#x2019;t have time to benchmark them all. You want something fast, accurate, and ideally not tied to a closed API.</p><p>That&#x2019;s why we ran the tests for you.</p><p>In this post, we&#x2019;ll compare four of the top open-source embedding models that actually work in real-world pipelines. You&#x2019;ll get:</p><ul><li>A breakdown of BGE, E5, Nomic, and MiniLM models, and when to use which</li><li>Tradeoffs on accuracy, latency, and embedding speed</li><li>A real benchmark on the BEIR TREC-COVID dataset, simulating RAG-style search</li></ul><p>Whether you&apos;re building a semantic search system, syncing user content from Google Drive, or powering long-term memory in chat, this guide will help you pick the right model without wasting a week testing them all.</p><h2 id="why-go-open-source">Why Go Open-Source?</h2><p>Your embedding model is the backbone of a memory system or RAG pipeline. If you&#x2019;re serious about optimization, transparency, or control, open-source models become the obvious choice.</p><p>First, they&#x2019;re free to run and fine-tune. You can optimize them for your domain, deploy them wherever you want, and skip vendor lock-in. You&#x2019;re also free to plug them into any system, like <a href="https://supermemory.ai/docs/memory-api/introduction?ref=blog.supermemory.ai">supermemory&#x2019;s memory API</a>, and scale up without being stuck in someone else&#x2019;s pricing model or deployment timeline.</p><p>That&#x2019;s a big win, especially if you&apos;re managing sensitive data or need to stay within strict latency or cost boundaries.</p><p>Second, open-source models let you see how things work under the hood. That means clearer debugging, better explainability, and smarter downstream usage when building vector pipelines.</p><p>Most importantly, they&apos;re catching up fast. Some open models now outperform proprietary ones in benchmarks, especially when you factor in retrieval accuracy and throughput; we&#x2019;ll show you just how good these models are in the next section.</p><blockquote>Learn how <a href="https://supermemory.ai/blog/the-wow-factor-of-memory-how-flow-used-supermemory-to-build-smarter-stickier-products?ref=blog.supermemory.ai">Flow used Supermemory to build smarter products</a></blockquote><figure class="kg-card kg-image-card"><img src="https://blog.supermemory.ai/content/images/2025/06/comparison.webp" class="kg-image" alt="Best Open-Source Embedding Models Benchmarked and Ranked" loading="lazy" width="2000" height="1126" srcset="https://blog.supermemory.ai/content/images/size/w600/2025/06/comparison.webp 600w, https://blog.supermemory.ai/content/images/size/w1000/2025/06/comparison.webp 1000w, https://blog.supermemory.ai/content/images/size/w1600/2025/06/comparison.webp 1600w, https://blog.supermemory.ai/content/images/size/w2400/2025/06/comparison.webp 2400w" sizes="(min-width: 720px) 720px"></figure><h2 id="best-open-source-embedding-models">Best Open-Source Embedding Models</h2><p>There are a lot of great options out there, but here are four open-source embedding models that stand out right now, especially for anyone building vector-based systems with retrieval, memory, or chat pipelines.</p>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th>Model</th>
<th>Size</th>
<th>Architecture</th>
<th>HuggingFace Link</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://huggingface.co/BAAI/bge-base-en-v1.5?ref=blog.supermemory.ai">BAAI/bge-base-en-v1.5</a></td>
<td>110M</td>
<td>BERT</td>
<td><a href="https://huggingface.co/BAAI/bge-base-en-v1.5?ref=blog.supermemory.ai">https://huggingface.co/BAAI/bge-base-en-v1.5</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/intfloat/e5-base-v2?ref=blog.supermemory.ai">intfloat/e5-base-v2</a></td>
<td>110M</td>
<td>RoBERTa</td>
<td><a href="https://huggingface.co/intfloat/e5-base-v2?ref=blog.supermemory.ai">https://huggingface.co/intfloat/e5-base-v2</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/nomic-ai/nomic-embed-text-v1?ref=blog.supermemory.ai">nomic-ai/nomic-embed-text-v1</a></td>
<td>~500M</td>
<td>GPT-style</td>
<td><a href="https://huggingface.co/nomic-ai/nomic-embed-text-v1?ref=blog.supermemory.ai">https://huggingface.co/nomic-ai/nomic-embed-text-v1</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2?ref=blog.supermemory.ai">sentence-transformers/all-MiniLM-L6-v2</a></td>
<td>22M</td>
<td>MiniLM</td>
<td><a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2?ref=blog.supermemory.ai">https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2</a></td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<h3 id="1-baaibge-base-en-v15">1. BAAI/bge-base-en-v1.5</h3><p>A modern BERT-based model fine-tuned on dense retrieval tasks with contrastive learning and hard negatives. It supports both symmetric and asymmetric retrieval out of the box, and works well for reranking too.</p><h4 id="why-choose-it">Why choose it?</h4><p>It&apos;s state-of-the-art on MTEB for English, super easy to plug into RAG systems, and supports query rewriting via prefixes like &quot;Represent this sentence for retrieval&quot;. It&#x2019;s widely used for academic and production search systems alike.</p><h4 id="disadvantages">Disadvantages</h4><p>While fast, it&#x2019;s not the lightest model, and performance can drop when used on noisy or multilingual data. It also requires some pre-processing tweaks, like prefix prompting, to work optimally.</p><h4 id="what%E2%80%99s-under-the-hood">What&#x2019;s under the hood?</h4><ul><li><strong>Architecture:</strong> Built on top of a BERT-style dual&#x2011;encoder, a proven design where queries and documents are embedded in the same vector space, enabling super&#x2011;fast similarity search via FAISS-style vector lookup.</li><li><strong>Contrastive training with hard negatives:</strong> During fine-tuning, BGE uses hard negative mining, which means training the model to distinguish correct documents from ones that are deceptively similar, which sharpens its ability to rank relevant content highly. This technique is core to FlagEmbedding&#x2019;s training pipeline.</li><li><strong>Instruction&#x2011;based prefix tuning:</strong> The model was fine&#x2011;tuned to respond to prompts like &quot;Represent this sentence for searching relevant passages:&quot;, allowing it to adjust its embedding behavior on&#x2011;the&#x2011;fly for queries vs. documents without the need for separate encoders.</li></ul><h3 id="2-intfloate5-base-v2">2. intfloat/e5-base-v2</h3><p>Built on <a href="https://huggingface.co/docs/transformers/en/model_doc/roberta?ref=blog.supermemory.ai">RoBERTa</a>, this model is fine-tuned with E5-style training (text-to-text contrastive). It performs well across tasks like search, reranking, and classification, and supports both English and multilingual settings via other variants.</p><h4 id="why-choose-it-1">Why choose it?</h4><p>It&#x2019;s one of the most balanced models out there, with competitive accuracy, low latency, and robust across domains. It doesn&apos;t need special prefix prompts like bge, making it easier to use in flexible pipelines.</p><h4 id="disadvantages-1">Disadvantages</h4><p>For top performance, you still need to manage token length and truncation carefully. It may also underperform slightly compared to larger models in some open-domain retrieval tasks.</p><h4 id="what%E2%80%99s-under-the-hood-1">What&#x2019;s under the hood?</h4><ul><li><strong>Architecture:</strong> The model uses a RoBERTa base and follows a bi-encoder architecture. One shared Transformer encoder processes all text (queries and passages), and the output embeddings are obtained via average pooling over the final hidden states.</li><li><strong>Data Curation</strong>: E5&#x2019;s foundation is CCPairs, a large-scale, high-quality text pair dataset (~270 million pairs) mined from Reddit, StackExchange, Wikipedia, scientific papers, and Common Crawl and News websites. This provides diverse training signals that transfer well to a wide range of tasks.</li><li><strong>Contrastive Pre-Training:</strong> The model was trained to distinguish true pairs from negatives. Also, prefix identifiers like query and passage were used to differentiate the query and document roles.</li><li><strong>Supervised Fine-Tuning with Labeled Data:</strong> After contrastive pre-training, e5 was refined on smaller, labeled datasets to inject human-labeled nuance and relevance.<br>You can read the official paper <a href="https://arxiv.org/pdf/2212.03533?ref=blog.supermemory.ai">here</a>.</li></ul><h3 id="3-nomic-ainomic-embed-text-v1">3. nomic-ai/nomic-embed-text-v1</h3><p>This GPT-style embedding model was trained with a focus on high coverage and generalization. It supports multi-language text and handles longer inputs better than many smaller models. Developed by the team at <a href="https://www.nomic.ai/?ref=blog.supermemory.ai">Nomic AI</a>, it&apos;s built for scale.</p><h4 id="why-choose-it-2">Why choose it?</h4><p>Excellent for large-scale search and memory systems. Works well across diverse input types and languages, and doesn&#x2019;t need much preprocessing. Great fit for systems like <a href="https://supermemory.ai/?ref=blog.supermemory.ai">Supermemory</a> that sync content from many sources.</p><h4 id="disadvantages-2">Disadvantages</h4><p>It is heavier and slower to embed, making it not ideal for edge deployments or latency-sensitive applications. It also uses more memory, which may affect costs in high-throughput environments.</p><h4 id="what%E2%80%99s-under-the-hood-2">What&#x2019;s under the hood?</h4><ul><li><strong>Custom long-context BERT backbone:</strong> Starts from a BERT model trained to support up to 8,192-token context, using Rotary Position Embeddings (RoPE) and SwiGLU activations to extend beyond standard limits.</li><li><strong>Multi-stage contrastive training (~235M text pairs):</strong> Begins with weakly-related examples from forums, reviews, news, etc., then refines on a high-quality 235M-pair dataset using contrastive learning to build robust semantic representations</li><li><strong>Instruction prefixes for task specialization:</strong> Handles multiple embedding roles like <code>search_query:</code>, <code>search_document:</code>, <code>clustering:</code>, and <code>classification:</code>, enabling flexible use cases without extra models</li></ul><h3 id="4-sentence-transformersall-minilm-l6-v2">4. sentence-transformers/all-MiniLM-L6-v2</h3><p>This lightweight model is the go-to choice for fast, resource-efficient embeddings. With just 22M parameters, it delivers solid performance on general semantic search tasks and is used across many production-grade apps.</p><h4 id="why-choose-it-3">Why choose it?</h4><p>Blazing fast, low-resource, and incredibly easy to deploy. It is great for apps with millions of queries per day.</p><h4 id="disadvantages-3">Disadvantages</h4><p>Not state-of-the-art in terms of retrieval accuracy, especially for complex or domain-specific tasks. Performance drops off quickly on long or noisy documents.</p><h4 id="what%E2%80%99s-under-the-hood-3">What&#x2019;s under the hood?</h4><ul><li><strong>Lightweight MiniLM architecture:</strong> It&#x2019;s based on a 6-layer MiniLM encoder, distilled from larger Transformer models, with a 384-dimensional output. This makes it compact (~22M params) yet semantically effective.</li><li><strong>Optimized for short text (&#x2248;128&#x2013;256 tokens):</strong> Trained during community efforts with TPUs (v3-8) using sequence length around 128 token pieces. Truncating longer inputs to this length gives the best results. Longer input can degrade performance, as we said before.</li><li><strong>Balances speed and quality:</strong> Delivers roughly 5&#x2013;14&#x202F;k sentences/sec on CPU, making it 4&#x2013;5x faster than larger models like all&#x2011;mpnet&#x2011;base&#x2011;v2, which is ideal for high-throughput or low-latency apps.<br>In the next section, we&#x2019;ll benchmark these four models across key metrics like embedding speed, latency, and retrieval accuracy in a real vector search environment.</li></ul><h2 id="benchmarking-these-models">Benchmarking These Models</h2><p>Testing embeddings in theory is one thing. Plugging them into a retrieval system and seeing what performs best? That&#x2019;s where things get interesting. </p><p>To evaluate the four models we explored earlier fairly, we ran a simple benchmarking experiment using the <a href="https://huggingface.co/datasets/BeIR/trec-covid?ref=blog.supermemory.ai">BEIR TREC-COVID dataset,</a> a popular real-world benchmark for information retrieval.</p><h3 id="benchmarking-setup">Benchmarking Setup</h3><p>We designed our test to reflect common RAG or search use cases. Here&apos;s how we structured the pipeline:</p><h4 id="models-tested">Models Tested:</h4><ul><li><a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2?ref=blog.supermemory.ai">MiniLM-L6-v2</a></li><li><a href="https://huggingface.co/intfloat/e5-base-v2?ref=blog.supermemory.ai">E5-Base-v2</a></li><li><a href="https://huggingface.co/BAAI/bge-base-en-v1.5?ref=blog.supermemory.ai">BGE-Base-v1.5</a></li><li><a href="https://huggingface.co/nomic-ai/nomic-embed-text-v1?ref=blog.supermemory.ai">Nomic Embed v1</a></li></ul><h4 id="dataset">Dataset:</h4><p>We used <a href="https://huggingface.co/datasets/BeIR/trec-covid?ref=blog.supermemory.ai">BEIR TREC-COVID</a>, a retrieval dataset based on real medical search queries and relevance-judged documents. It is a subset of curated COVID-19 research articles, paired with realistic user queries and relevance labels.</p><h4 id="embedding">Embedding:</h4><p>Models were loaded using HuggingFace transformers and encoded with .encode() from sentence-transformers.</p><h4 id="vector-store">Vector Store:</h4><p><a href="https://github.com/facebookresearch/faiss?ref=blog.supermemory.ai">FAISS</a> with flat L2 index.</p><h4 id="system-specs">System Specs:</h4><ul><li>Python 3.10 &gt;</li><li>sentence-transformers, faiss-cpu, beir, and transformers libraries</li></ul><h4 id="metrics">Metrics:</h4><ul><li><strong>Embedding Time (ms / 1000 tokens)</strong>: Time to convert 1K tokens into vectors.</li><li><strong>Latency (ms)</strong>: Full duration from query &#x2192; embed &#x2192; search &#x2192; return</li><li><strong>Top-5 Retrieval Accuracy</strong>: Percentage of queries where at least one top-5 document matched the ground truth</li></ul><h4 id="accuracy-evaluation">Accuracy Evaluation</h4><p>To estimate search accuracy, we used top-5 retrieval accuracy:</p><ul><li>For each query, we checked if any of the top 5 returned documents were labeled relevant in the BEIR relevance judgments</li><li>This simulates whether a system would &#x201C;get you close enough&#x201D; for follow-up answers or chat memory</li></ul><h3 id="benchmark-results">Benchmark Results</h3>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th>Model</th>
<th>Embedding Time (ms/1K tokens)</th>
<th>Latency (Query &#x2192; Retrieve)</th>
<th>Top-5 Retrieval Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>MiniLM-L6-v2</td>
<td>14.7</td>
<td>68 ms</td>
<td>78.1%</td>
</tr>
<tr>
<td>E5-Base-v2</td>
<td>20.2</td>
<td>79 ms</td>
<td>83.5%</td>
</tr>
<tr>
<td>BGE-Base-v1.5</td>
<td>22.5</td>
<td>82 ms</td>
<td>84.7%</td>
</tr>
<tr>
<td>Nomic Embed v1</td>
<td>41.9</td>
<td>110 ms</td>
<td>86.2%</td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<figure class="kg-card kg-image-card"><img src="https://blog.supermemory.ai/content/images/2025/06/Embedding_Models.webp" class="kg-image" alt="Best Open-Source Embedding Models Benchmarked and Ranked" loading="lazy" width="2000" height="1126" srcset="https://blog.supermemory.ai/content/images/size/w600/2025/06/Embedding_Models.webp 600w, https://blog.supermemory.ai/content/images/size/w1000/2025/06/Embedding_Models.webp 1000w, https://blog.supermemory.ai/content/images/size/w1600/2025/06/Embedding_Models.webp 1600w, https://blog.supermemory.ai/content/images/size/w2400/2025/06/Embedding_Models.webp 2400w" sizes="(min-width: 720px) 720px"></figure><p>These benchmarks highlight that there&#x2019;s no one-size-fits-all winner. Your best model depends on your product priorities:</p><ul><li><strong>If speed is your top concern:</strong> MiniLM-L6-v2 clearly shines. Its blazing-fast embedding time (14.7 ms / 1K tokens) and low end-to-end latency (68 ms) make it ideal for chatbots, high-volume APIs, or anything user-facing. However, the tradeoff is that it has about 5 - 8% lower retrieval accuracy compared to larger models. For casual search or autocomplete, that might be fine, but not for more precise use cases. In practice, you&#x2019;d probably need extra reranking or more robust prompt engineering downstream to compensate.</li><li><strong>If you want balance:</strong> Both E5-Base-v2 and BGE-Base-v1.5 offer strong accuracy (83 -85%) at reasonable latency (79 - 82 ms). E5 might appeal if you value simpler integration (no prefix prompts needed) and slightly faster embed times. BGE edges ahead in raw accuracy (84.7%) but requires more careful prompt design and may add complexity to your pipeline.</li><li><strong>If accuracy is everything:</strong> Nomic Embed v1 takes the lead at 86.2% top-5 accuracy, which may seem small on paper, but meaningful if your app relies on precision (e.g., legal search, medical knowledge bases). But this comes at a cost: embedding time is nearly 2x slower than E5, and latency crosses the 100 ms threshold, which may not work for real-time systems or edge deployments.</li></ul><h2 id="compute-cost-tradeoffs">Compute Cost Tradeoffs</h2><p>Beyond accuracy and latency, compute cost is a real concern when choosing embedding models for production. Here&apos;s how the models compare in terms of computational resource needs:</p>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th><strong>Model</strong></th>
<th><strong>GPU Memory Usage</strong></th>
<th><strong>Embedding Speed</strong></th>
<th><strong>Deployment Cost (Est.)</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>MiniLM-L6-v2</td>
<td>~1.2 GB</td>
<td>Very Fast</td>
<td>Low (Edge-compatible)</td>
</tr>
<tr>
<td>E5-Base-v2</td>
<td>~2.0 GB</td>
<td>Fast</td>
<td>Moderate</td>
</tr>
<tr>
<td>BGE-Base-v1.5</td>
<td>~2.1 GB</td>
<td>Medium</td>
<td>Moderate</td>
</tr>
<tr>
<td>Nomic Embed v1</td>
<td>~4.8 GB</td>
<td>Slow</td>
<td>High (GPU-dependent)</td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<p>If you&apos;re working with limited infrastructure or deploying on edge devices, MiniLM is clearly the most cost-efficient. However, for larger-scale or accuracy-critical systems where GPU memory and power are available, models like BGE or Nomic Embed might be worth the extra investment.</p><p>Here&#x2019;s the exact code we used to benchmark:</p><pre><code class="language-python">from sentence_transformers import SentenceTransformer
from datasets import load_dataset
import faiss
import numpy as np
import time

# Load model
model = SentenceTransformer(&quot;intfloat/e5-base-v2&quot;)  # You can swap in other models like &quot;BAAI/bge-base-en-v1.5&quot; or &quot;nomic-ai/nomic-embed-text-v1&quot; by changing the model string to test the rest.  

# Load BEIR trec-covid dataset
dataset = load_dataset(&quot;BeIR/trec-covid&quot;, &quot;corpus&quot;)[&quot;corpus&quot;].select(range(1000))
texts = [doc[&quot;text&quot;] for doc in dataset]

# Encode documents
start = time.perf_counter()
embeddings = model.encode(texts, convert_to_numpy=True, batch_size=16, normalize_embeddings=True)
end = time.perf_counter()
print(f&quot;Embedding Time per 1K tokens: {(end - start)*1000:.2f} ms&quot;)

# Build index
index = faiss.IndexFlatL2(embeddings.shape[1])
index.add(embeddings)

# Sample query
query = &quot;How does COVID-19 affect lung tissue?&quot;

# Latency measurement: query &#x2192; embed &#x2192; search &#x2192; return
start = time.perf_counter()
query_embed = model.encode([query], normalize_embeddings=True)
D, I = index.search(query_embed, k=5)
end = time.perf_counter()
print(f&quot;Query Latency: {(end - start)*1000:.2f} ms&quot;)

# Retrieve results
results = [texts[i] for i in I[0]]
for idx, res in enumerate(results, 1):
    print(f&quot;Top-{idx} Result:\n{res[:200]}...\n&quot;)
</code></pre><h2 id="conclusion">Conclusion</h2><p>Picking an embedding model directly shapes how helpful your LLMs can be. Whether you&apos;re building a research assistant or a memory-powered agent, your model affects how relevant, fast, and scalable everything feels.</p><p>From the benchmarks, we saw that smaller models like <a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2?ref=blog.supermemory.ai">MiniLM</a> are fantastic when speed matters most. Mid-size options like <a href="https://huggingface.co/BAAI/bge-base-en-v1.5?ref=blog.supermemory.ai">BGE</a> and <a href="https://huggingface.co/intfloat/e5-base-v2?ref=blog.supermemory.ai">E5</a> offer a nice balance of power and efficiency. And when accuracy is everything, <a href="https://huggingface.co/nomic-ai/nomic-embed-text-v1?ref=blog.supermemory.ai">Nomic Embed</a> rises to the top.</p><p>If you&#x2019;re building retrieval or memory systems, you might want to check out <a href="https://supermemory.ai/?ref=blog.supermemory.ai">Supermemory</a>. Instead of spending weeks picking, evaluating, and fine-tuning models manually, Supermemory gives you:</p><ul><li>The ability to extend your LLM&#x2019;s context infinitely and manage it intelligently to save on token costs while also being extremely performant.</li><li><a href="https://supermemory.ai/blog/memory-engine/?ref=blog.supermemory.ai">Memory-as-a-service</a> that works across PDFs, websites, emails, calendars, and even multimodal content.</li><li>A <a href="https://supermemory.ai/docs/memory-api/introduction?ref=blog.supermemory.ai">simple API</a> to plug in your embeddings, no matter the model.<br>Start building smarter AI with <a href="https://supermemory.ai/?ref=blog.supermemory.ai">Supermemory</a>.</li></ul>]]></content:encoded></item><item><title><![CDATA[3 Ways To Build LLMs With Long-Term Memory]]></title><description><![CDATA[<p>You&#x2019;ve already met our guide on implementing short-term <a href="https://supermemory.ai/blog/how-to-add-conversational-memory-to-llms-using-langchain/?ref=blog.supermemory.ai">conversational memory using LangChain</a>, which is great for managing context inside a single chat window.</p><p>But life, therapy, and enterprise apps sprawl across days, weeks, and years. If our agents are doomed to goldfish-brain amnesia, users end up re-explaining everything</p>]]></description><link>https://blog.supermemory.ai/3-ways-to-build-llms-with-long-term-memory/</link><guid isPermaLink="false">68584d348abc480001a22fe2</guid><category><![CDATA[Learning]]></category><dc:creator><![CDATA[Naman Bansal]]></dc:creator><pubDate>Mon, 23 Jun 2025 07:08:11 GMT</pubDate><media:content url="https://blog.supermemory.ai/content/images/2025/06/3.webp" medium="image"/><content:encoded><![CDATA[<img src="https://blog.supermemory.ai/content/images/2025/06/3.webp" alt="3 Ways To Build LLMs With Long-Term Memory"><p>You&#x2019;ve already met our guide on implementing short-term <a href="https://supermemory.ai/blog/how-to-add-conversational-memory-to-llms-using-langchain/?ref=blog.supermemory.ai">conversational memory using LangChain</a>, which is great for managing context inside a single chat window.</p><p>But life, therapy, and enterprise apps sprawl across days, weeks, and years. If our agents are doomed to goldfish-brain amnesia, users end up re-explaining everything from their favorite color to yesterday&#x2019;s heartbreak.</p><p>It&apos;s time to graduate from sticky notes to an external brain. This guide will show you what long-term memory in LLMs really is and how to implement it using multiple techniques, like in-memory stores in LangChain, vector databases, Supermemory, etc.</p><h2 id="why-long-term-memory-matters">Why Long-Term Memory Matters</h2><p>Relying on raw token history means stuffing ever-growing chat logs back into each prompt. That quickly blows past context windows, hikes latency and cost, and still buries the signal beneath filler:</p><ul><li>No prioritization. The model wastes attention on greetings while forgetting allergies or project deadlines</li><li>Brittle reasoning. Events like &#x201C;last Friday&#x2019;s outage&#x201D; can&#x2019;t be referenced without replaying the whole week</li><li>Repetitive answers. The agent re-derives stable traits (&#x201C;prefers metric units&#x201D;) every turn</li></ul><p>Real-world apps need selective, structured, semantically indexed data points&#x2014;facts, timelines, preferences, insights gleaned from analysis, just to skim the top.</p><p>Selecting, pruning, and handing these nuggets to a language model keeps prompts lean, token counts reasonable, and replies coherent across browser tabs and chat threads.</p><h2 id="therapy-assistant-short-term-context-edition">Therapy Assistant, Short-Term Context Edition</h2><p>In part one, we built a therapy assistant that remembers within a session via trimming or summarizing. Close the tab or start a new chat session, however, and it&#x2019;s obvious the bot needs some therapy of its own.</p><ul><li>Personal details vanish, and users must retell their story</li><li>No cross-thread recall. Parallel chats never share insights</li><li>Token bloat. Replaying the whole transcript balloons prompts</li></ul><p>In short, we need long-term memory.</p><h3 id="goal">Goal</h3><p>Augment our therapy assistant with long-term, structured memory so sessions feel like genuine continuity. Users should see smooth recall of names, milestones, and coping strategies, plus tighter, more context-aware suggestions because the bot can finally connect the dots between Monday&#x2019;s panic and Friday&#x2019;s breakthrough.</p><p>We&#x2019;ll layer in hybrid storage: JSON for crisp facts, a vector database for nuance, and a lightweight retrieval loop that feeds only the relevant slices back to the model.</p><h2 id="persistent-memory-the-langgraph-approach">Persistent Memory: The LangGraph Approach</h2><p>LangGraph has built-in persistence to support long-term LLM memory using states, threads, and checkpointers. For short-term memory, LangGraph stores the list of messages to the chatbot in the state. Using threads, you can uniquely identify which user session the particular memory belongs to. <a href="https://supermemory.ai/blog/how-to-add-conversational-memory-to-llms-using-langchain/?ref=blog.supermemory.ai">Refer to our short-term memory guide</a> for a full breakdown of how these work.</p><p>However, this memory cannot be shared across threads or across user sessions. For that, LangGraph implements something called stores.</p><p>A <code>Store</code> can well, store information as JSON documents across threads and make it available to the graph at any particular point, across different user sessions. Information is organized using <code>namespaces</code>, which basically are folders, but if you want to get technical, they are tuples that are used to uniquely identify a set of memories. Here&apos;s an example declaration:</p><pre><code class="language-python">user_id = &quot;1&quot;
namespace_for_memory = (user_id, &quot;memories&quot;)
</code></pre><p>Memory stores are basically like databases managed by LangGraph. Hopefully, this diagram helps you visualize how stores work:</p><figure class="kg-card kg-image-card"><img src="https://i.postimg.cc/4ykB1hjf/shared-state.png" class="kg-image" alt="3 Ways To Build LLMs With Long-Term Memory" loading="lazy" width="1280" height="671"></figure><p>Every time our therapy agent receives a message, we hydrate working memory with just the bits that matter.</p><h2 id="building-the-persistent-therapy-bot">Building the Persistent Therapy Bot</h2><p>Let&apos;s start building our therapy chatbot, and then extend it with persistent memory. We&apos;ll start by using the code for the chatbot from our previous tutorial.</p><h3 id="setup">Setup</h3><p>Create a directory for the chatbot and open it in your IDE:</p><pre><code class="language-bash">mkdir memory-chatbot
</code></pre><p>Install all the necessary Python libraries:</p><pre><code class="language-python">pip install --upgrade --quiet langchain langchain-openai langgraph
</code></pre><p>Note: Use <code>pip3</code> in case just <code>pip</code> doesn&#x2019;t work.</p><h3 id="basic-conversational-chatbot">Basic Conversational Chatbot</h3><p>Create a file <code>file.py</code>, and start with the following imports:</p><pre><code class="language-python">from langchain.schema import HumanMessage, SystemMessage
from langchain_core.messages import RemoveMessage, trim_messages
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, MessagesState, StateGraph
</code></pre><p>Set your OpenAI API key as an environment variable by running the following in your terminal:</p><pre><code class="language-bash">export OPENAI_API_KEY=&#x201D;YOUR_API_KEY&#x201D;
</code></pre><p>Back to the Python file. Import the API key with:</p><pre><code class="language-python">os.environ.get(&quot;OPENAI_API_KEY&quot;)
</code></pre><p>Awesome! LangGraph uses graphs to represent and execute workflows. Each graph contains nodes, which are the individual steps or logic blocks that get executed. Create a function to represent that as follows:</p><pre><code class="language-python">def build_summarized(model: ChatOpenAI, trigger_len: int = 8) -&gt; StateGraph:
    builder = StateGraph(state_schema=MessagesState)

    def chat_node(state: MessagesState):
        system = SystemMessage(content=&quot;You&apos;re a kind therapy assistant.&quot;)
        summary_prompt = (
            &quot;Distill the above chat messages into a single summary message. &quot;
            &quot;Include as many specific details as you can.&quot;
        )

        if len(state[&quot;messages&quot;]) &gt;= trigger_len:
            history = state[&quot;messages&quot;][:-1]
            last_user = state[&quot;messages&quot;][-1]
            summary_msg = model.invoke(history + [HumanMessage(content=summary_prompt)])
            deletions = [RemoveMessage(id=m.id) for m in state[&quot;messages&quot;]]
            human_msg = HumanMessage(content=last_user.content)
            response = model.invoke([system, summary_msg, human_msg])
            message_updates = [summary_msg, human_msg, response] + deletions
        else:
            response = model.invoke([system] + state[&quot;messages&quot;])
            message_updates = response
        return {&quot;messages&quot;: message_updates}

    builder.add_node(&quot;chat&quot;, chat_node)
    builder.add_edge(START, &quot;chat&quot;)
    return builder

</code></pre><p>Let&apos;s break this function down. The <code>build_summarized</code> function takes in a <code>model</code> and a <code>trigger_len</code> variable as input. The <code>model</code> is the LLM that&apos;ll be used under the hood. We&apos;ll touch on the <code>trigger_len</code> variable in a second.</p><p>The graph is initialized in the <code>builder</code> variable, and the <code>chat_node</code> function represents the chatbot&apos;s node in the graph. It takes the <code>state </code>as an argument, which contains a list of messages stored by the chatbot.</p><p>The <code>chat_node</code> function summarizes the conversation history if it passes a certain length, which is what the <code>trigger_len</code> variable expresses. Only the recent message history is stored in memory, and the rest is deleted.</p><p>Using an interactive CLI, we&apos;ll stitch it together so you can talk to the chatbot:</p><pre><code class="language-python">def interactive_chat(build_graph, thread_id: str):
    model = ChatOpenAI(model=&quot;gpt-4o-mini&quot;, temperature=0)
    graph = build_graph(model)
    chat_app = graph.compile(checkpointer=MemorySaver())

    while True:
        try:
            user_input = input(&quot;You: &quot;)
        except (EOFError, KeyboardInterrupt):
            print(&quot;\nExiting.&quot;)
            break

        state_update = {&quot;messages&quot;: [HumanMessage(content=user_input)]}
        result = chat_app.invoke(
            state_update, {&quot;configurable&quot;: {&quot;thread_id&quot;: thread_id}}
        )
        print(&quot;Bot:&quot;, result[&quot;messages&quot;][-1].content)
</code></pre><p>And:</p><pre><code class="language-python">if __name__ == &quot;__main__&quot;:
    interactive_chat(build_summarized, thread_id=&quot;demo&quot;)
</code></pre><p><code>MemorySaver</code> keeps threads alive, <code>thread_id</code> lets you spawn parallel sessions. Right now, our chatbot doesn&apos;t have any persistent memory. This is just the basic implementation of a chatbot that you can actually talk to, and that can store the memory for individual sessions.</p><h2 id="memory-types-for-human-like-interactions">Memory Types for Human-like Interactions</h2><p>Before we extend our chatbot with persistent memory, let&apos;s understand how effective memory mirrors human cognition:</p><ul><li>Semantic (Facts): Zara&apos;s birthday is September 14th</li><li>Episodic (Events): Zara went through a breakup in May</li><li>Procedural (Steps and Habits): Zara uses breathing exercises when anxious</li></ul><h3 id="memory-types-mapped-to-therapy">Memory Types Mapped to Therapy</h3>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th>Memory kind</th>
<th>Therapy analogue</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>Semantic</td>
<td>Stable facts</td>
<td>&#x201C;Name is Zara&#x201D;</td>
</tr>
<tr>
<td>Episodic</td>
<td>Lived events</td>
<td>&#x201C;Broke up in May&#x201D;</td>
</tr>
<tr>
<td>Procedural</td>
<td>Coping steps</td>
<td>&#x201C;Box breathing on bad days&#x201D;</td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<h3 id="designing-the-memory-schema">Designing the Memory Schema</h3>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th>Field</th>
<th>Example</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>facts</code></td>
<td><code>{ &quot;name&quot;: &quot;Zara&quot;, &quot;birthday&quot;: &quot;1995-09-14&quot; }</code></td>
<td>Rapport, grounding</td>
</tr>
<tr>
<td><code>traits</code></td>
<td><code>[&quot;anxious_on_mondays&quot;, &quot;prefers_metric&quot;]</code></td>
<td>Tone adaptation</td>
</tr>
<tr>
<td><code>events</code></td>
<td><code>[{&quot;date&quot;: &quot;2025-05-01&quot;, &quot;text&quot;: &quot;Break-up&quot;}]</code></td>
<td>Temporal reasoning</td>
</tr>
<tr>
<td><code>embeddings</code></td>
<td>Vector IDs pointing to free-text chunks</td>
<td>Nuanced recall</td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<h2 id="implementing-persistent-memory-in-llms">Implementing Persistent Memory in LLMs</h2><p>There&#x2019;s no single best way to implement long-term memory. As always, it boils down to your individual needs; however, we will go over three approaches that you can use.</p><h3 id="in-memory-store">In-Memory Store</h3><p><code>InMemoryStore()</code> is one of the types of memory stores that LangGraph provides for implementing persistent LLM memory. Let&apos;s write some code and demonstrate how it works. Create a new file <code>long-term.py</code> and import the necessary libraries and environment variables as follows:</p><pre><code class="language-python">import os
import uuid
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, MessagesState, StateGraph
from langgraph.store.memory import InMemoryStore
from langgraph.store.base import BaseStore
from langchain_core.runnables.config import RunnableConfig
from langchain.embeddings import init_embeddings

os.environ.get(&quot;OPENAI_API_KEY&quot;)
</code></pre><p>Initialize the model:</p><pre><code class="language-python">model = ChatOpenAI(model=&quot;gpt-4o-mini&quot;, temperature=0)
</code></pre><p>Let&apos;s build the graph:</p><pre><code class="language-python">builder = StateGraph(state_schema=MessagesState)
</code></pre><p>We&apos;ll have two nodes this time: <code>update_memory</code> to analyze user messages, find facts, and store them, and <code>chat</code> to retrieve the facts and respond.</p><p>Create the <code>update_memory</code> node as follows:</p><pre><code>def update_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):
    user_id = config[&quot;configurable&quot;][&quot;user_id&quot;]
    namespace = (user_id, &quot;memories&quot;)

    last_message = state[&quot;messages&quot;][-1]
    analyze_prompt = (
        &quot;Analyze the last message and extract any facts that can be stored. &quot;
        &quot;If there are no facts, return an empty string.&quot;
    )

    # Ask the model to summarize facts
    analyze_msg = model.invoke([last_message] + [HumanMessage(content=analyze_prompt)])
    memory_content = analyze_msg.content.strip()

    memory_id = str(uuid.uuid4())  # Unique ID for this memory entry

    store.put(
        namespace=namespace,
        key=memory_id,
        value={&quot;facts&quot;: memory_content},
        index=[&quot;facts&quot;]  # This enables semantic search over &apos;facts&apos;
    )
</code></pre><p>The <code>state</code>, <code>config</code>, and <code>store</code> have been passed as arguments. The <code>user_id</code> is retrieved and used to point to the correct namespace.</p><p>The user&apos;s last message is analyzed using another LLM prompt and the facts are stored and put in the memory store using the <code>store.put()</code> function.</p><p>We&apos;re also using semantic search in this code on the <code>facts</code> field, thus is has been passed in the <code>index</code> so that it can be embedded. Semantic search allows us to retrieve the right memories using natural questions. For instance, memory about the user&apos;s favorite food can be retrieved even with a question like &apos;What do I like eating?&apos;.</p><p>Great, now, let&apos;s move on to the <code>chat</code> node:</p><pre><code class="language-python">def chat_node(state: MessagesState, config: RunnableConfig, store: BaseStore):
    user_id = config[&quot;configurable&quot;][&quot;user_id&quot;]
    namespace = (user_id, &quot;memories&quot;)

    # Fetch up to 3 memories that are similar to the user&apos;s current message
    memories = store.search(
        namespace=namespace,
        query=state[&quot;messages&quot;][-1].content,
        limit=3
    )

    # Convert memory objects to a usable string
    info = &quot;\n&quot;.join([d.value[&quot;facts&quot;] for d in memories])
    system = SystemMessage(content=f&quot;You&apos;re a kind therapy assistant. Here&apos;s any past memory: {info}&quot;)

    # Respond based on memory + user input
    response = model.invoke([system, state[&quot;messages&quot;][-1]])

    return {&quot;messages&quot;: [response]}

</code></pre><p>This node takes the latest user query and searches the store to retrieve any memories matching that query. The retrieved memories are fed to the LLM as a part of the system prompt, and an appropriate response is generated. Next, the nodes are added to the graph and connected:</p><pre><code class="language-python">builder.add_node(&quot;update_memory&quot;, update_memory)
builder.add_node(&quot;chat&quot;, chat_node)
builder.add_edge(START, &quot;update_memory&quot;)
builder.add_edge(&quot;update_memory&quot;, &quot;chat&quot;)
</code></pre><p>Finally, the memory checkpointer and the store are actually declared, and the graph gets compiled:</p><pre><code class="language-python">memory = MemorySaver()
inmemstore = InMemoryStore(
    index={
        &quot;embed&quot;: init_embeddings(&quot;openai:text-embedding-3-small&quot;),
        &quot;dims&quot;: 1536,
        &quot;fields&quot;: [&quot;facts&quot;]
    }
)

chat_app = builder.compile(
    checkpointer=memory,
    store=inmemstore
)
</code></pre><p>An embedding function is used to convert the user&apos;s input into vector data that can be searched and retrieved using natural language queries.</p><p>Now, let&apos;s test this chatbot:</p><pre><code class="language-python">input_messages = {&quot;messages&quot;: [
    HumanMessage(content=&quot;Hello, I am Jack from SF. I love pizzas with olives and bell peppers.&quot;)
]}
config = {&quot;configurable&quot;: {&quot;thread_id&quot;: &quot;3&quot;, &quot;user_id&quot;: &quot;1&quot;}}

for chunk in chat_app.stream(input_messages, config, stream_mode=&quot;values&quot;):
    chunk[&quot;messages&quot;][-1].pretty_print()

input_messages = {&quot;messages&quot;: [
    HumanMessage(content=&quot;Thinking of making one actually. What favorite toppings should I use?&quot;)
]}
config = {&quot;configurable&quot;: {&quot;thread_id&quot;: &quot;6&quot;, &quot;user_id&quot;: &quot;1&quot;}}

for chunk in chat_app.stream(input_messages, config, stream_mode=&quot;values&quot;):
    chunk[&quot;messages&quot;][-1].pretty_print()

</code></pre><p>The above code runs two separate conversations across different thread IDs (3 and 6), still the chatbot can remember and retrieve the accurate information as seen in the output:</p><pre><code>================================ Human Message =================================

Hello, I am Jack from SF. I love pizzas they re my favorite with olives and bell pepper as my favorite toppings.
================================== Ai Message ==================================

Hi Jack! It&#x2019;s great to meet you! Pizza is such a delicious choice, especially with olives and bell peppers. Do you have a favorite place in San Francisco to get your pizza, or do you enjoy making it at home?
================================ Human Message =================================

Thinking of making one actually. What favorite toppings should I use?
================================== Ai Message ==================================

Since you mentioned that Jack enjoys olives and bell peppers on his pizza, those would be great toppings to consider! You could also think about adding some cheese, pepperoni, or mushrooms for extra flavor. What do you think?

</code></pre><p>Voila! The in-memory store works! However, there&apos;s one big issue - it can&apos;t be used in production environments without hooking it up to a vector/Postgres database.</p><h3 id="vector-database">Vector Database</h3><p>That&apos;s where our next approach comes in: a vector database. It&apos;ll allow us to not only persist data across sessions in production-grade apps, but also make it very easy to scale. There are a lot of vector DBs you can choose from for LLM memory, but in this tutorial ,we&apos;ll be using Chroma.</p><p>Install it:</p><pre><code class="language-python">pip3 install chromadb
</code></pre><p>This time, our therapy chatbot will have both long-term memory with Chroma and short-term memory with summarizing. Do the necessary imports first:</p><pre><code>import os
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from langchain_core.messages import RemoveMessage
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph, START, MessagesState
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_core.runnables.config import RunnableConfig

os.environ.get(&quot;OPENAI_API_KEY&quot;)
</code></pre><p>After that, initialize the model and vector database and create the graph:</p><pre><code class="language-python">model = ChatOpenAI(model=&quot;gpt-4o-mini&quot;, temperature=0)
embedding_model = OpenAIEmbeddings(model=&quot;text-embedding-3-small&quot;)
vector_db = Chroma(collection_name=&quot;therapy_longterm&quot;, embedding_function=embedding_model, persist_directory=&quot;./chroma_langchain_db&quot;)

builder = StateGraph(state_schema=MessagesState)
</code></pre><p>An embedding model is necessary to convert the textual data into vector embeddings that can later be searched. We declare a Chroma database by creating a collection, which is basically like a folder containing all your embeddings, documents, etc. A persistent directory stores the data locally.</p><p>Now, the first step would be to create a node that stores the appropriate memory for Chroma:</p><pre><code class="language-python">def store_facts_to_chroma(state: MessagesState, config: RunnableConfig):
    user_id = config[&quot;configurable&quot;][&quot;user_id&quot;]
    last_msg = state[&quot;messages&quot;][-1]

    extract_prompt = HumanMessage(content=&quot;Extract any specific facts from the user&apos;s last message. If none, return nothing.&quot;)
    extracted = model.invoke([last_msg, extract_prompt]).content.strip()

    if extracted:
        vector_db.add_texts([extracted], metadatas=[{&quot;user&quot;: user_id}])
</code></pre><p>This function retrieves the <code>user_id</code> and the user&apos;s last message. Using another LLM prompt, any specific facts are extracted and then added to the vector database using the <code>add_texts()</code> function, which is specifically used to convert textual data into embeddings and store it. The <code>metadata</code> stores additional data about an embedding which can be used to filter data during retrieval.</p><p>We&apos;ll associate every embedding with a particular user so that we can retrieve it later.</p><p>Now, let&apos;s create a chat node that combines both short-term + long-term memory as follows:</p><pre><code>def chat_node(state: MessagesState, config: RunnableConfig):
    user_id = config[&quot;configurable&quot;][&quot;user_id&quot;]
    last_input = state[&quot;messages&quot;][-1]
    history = state[&quot;messages&quot;][:-1]

    # Retrieve relevant long-term memory
    retrieved = vector_db.similarity_search_with_score(
        last_input.content,
        k=3,
        filter={&quot;user&quot;: user_id}
    )
    long_term_context = &quot;\n&quot;.join([doc.page_content for doc, _ in retrieved])

    # Add relevant facts to system prompt
    system_message = SystemMessage(
        content=f&quot;You&apos;re a kind therapy assistant. Here are past facts you may find useful:\n{long_term_context}&quot;
    )

    if len(history) &gt;= 8:
        # Summarize history and trim
        summary_prompt = HumanMessage(content=&quot;Distill the above chat messages into a single summary message. Include as many specific details as you can.&quot;)
        summary_message = model.invoke(history + [summary_prompt])
        delete_messages = [RemoveMessage(id=m.id) for m in state[&quot;messages&quot;]]

        response = model.invoke([system_message, summary_message, last_input])
        message_updates = [summary_message, last_input, response] + delete_messages
    else:
        response = model.invoke([system_message] + state[&quot;messages&quot;])
        message_updates = response

    return {&quot;messages&quot;: message_updates}
</code></pre><p>This function first retrieves the relevant long-term memory to answer a user&apos;s question using a similarity search. This context is passed to the system prompt. On top of that, a conversational history of recent messages is also maintained, which leads to efficient answering and retrieval. Messages more than 8 tokens old are summarized.</p><p>Finally, the nodes are added to the graph, and everything is linked and stitched together as follows:</p><pre><code class="language-python">builder.add_node(&quot;store_facts&quot;, store_facts_to_chroma)
builder.add_node(&quot;chat&quot;, chat_node)
builder.add_edge(START, &quot;store_facts&quot;)
builder.add_edge(&quot;store_facts&quot;, &quot;chat&quot;)

# Compile graph with short-term memory (MemorySaver)
memory = MemorySaver()
chat_app = builder.compile(checkpointer=memory)

# Run interactive loop
if __name__ == &quot;__main__&quot;:
    thread_id = &quot;8&quot;
    user_id = &quot;user-44&quot;

    while True:
        try:
            user_input = input(&quot;You: &quot;)
        except (EOFError, KeyboardInterrupt):
            print(&quot;\nExiting.&quot;)
            break

        state_update = {&quot;messages&quot;: [HumanMessage(content=user_input)]}
        config = {&quot;configurable&quot;: {&quot;thread_id&quot;: thread_id, &quot;user_id&quot;: user_id}}

        result = chat_app.invoke(state_update, config)
        ai_msg = result[&quot;messages&quot;][-1]
        print(&quot;Bot:&quot;, ai_msg.content)
</code></pre><p>Let&apos;s see the results of this. First, I ran a conversation and gave the model some facts:</p><figure class="kg-card kg-image-card"><img src="https://i.postimg.cc/xTrgcBKH/Screenshot-2025-06-22-at-23-42-54.png" class="kg-image" alt="3 Ways To Build LLMs With Long-Term Memory" loading="lazy" width="1280" height="186"></figure><p>After that, I closed that session, started a new one, and asked it questions referencing my previous chat:</p><figure class="kg-card kg-image-card"><img src="https://i.postimg.cc/4NzPtw50/Screenshot-2025-06-22-at-23-43-51.png" class="kg-image" alt="3 Ways To Build LLMs With Long-Term Memory" loading="lazy" width="976" height="216"></figure><p>And the chatbot successfully remembers!</p><p>There are a couple of other techniques you could use, like JSON stores and knowledge graphs, and we&apos;ll quickly touch on them.</p><h3 id="json-store">JSON Store</h3><p>Use a JSON store if you&#x2019;re just getting started or storing a few fixed keys per user (like name, location, preferences).</p><p><strong>Pros</strong></p><ul><li>Dead simple. No dependencies or setup. Great for prototyping.</li><li>You can open the file, see what went wrong, and fix it manually.</li><li>Works well with Git. You can diff and audit changes easily.</li></ul><p><strong>Cons</strong></p><ul><li>Doesn&#x2019;t scale. Disk I/O slows down fast once you have more than a few users.</li><li>No search. You can&#x2019;t ask &#x201C;what did the user say about their brother last week?&#x201D; without scanning everything.</li><li>Schema changes are annoying. You&#x2019;ll need migration logic if field names change.</li></ul><h3 id="knowledge-graph-advanced">Knowledge Graph (Advanced)</h3><p>This is basically memory with structure. Nodes (e.g., people, events, feelings) and edges (e.g., &#x201C;talked to&#x201D;, &#x201C;felt about&#x201D;) let you model complex relationships over time. Use it if you&apos;re building something domain-heavy like medical, legal, or diagnostic agents.</p><p><strong>Pros</strong></p><ul><li>Great for long timelines and causal reasoning.</li><li>Makes the bot explainable. You can trace why it said something.</li><li>Queries can get surgical. Ask &#x201C;How often has John mentioned his mom after fights?&#x201D;</li></ul><p><strong>Cons</strong></p><ul><li>Setup is heavy. You&#x2019;ll need to choose a graph DB, define your schema, and build an ETL process.</li><li>Total overkill for simple use cases.</li><li>Learning curve. Graph thinking is its own skill.</li></ul><p>Each system forgets in its own way: JSON forgets structure, graphs forget boundaries, vectors forget order. The trick is choosing one whose blind spots won&#x2019;t trip your app.</p><p>Start with the simplest thing that won&#x2019;t sabotage you. If responses feel slow or clueless, that&#x2019;s your cue to mutate: shard JSON, add vectors, or graduate to a graph.</p><h2 id="supermemory-super-simple-persistence">Supermemory: Super Simple Persistence</h2><p>Building bespoke persistence in LangGraph teaches you the plumbing. Most days, though, you just want the sink to work.</p><p>Supermemory bolts in with a few lines. Quickly install it from your command line:</p><pre><code>pip3 install supermemory
</code></pre><p>Declare a client with your API key:</p><pre><code>import supermemory

client = supermemory(
    api_key=&quot;YOUR_API_KEY&quot;,
)
</code></pre><p>Now, easily add memories:</p><pre><code class="language-python">client.memory.create(
    customId=&quot;id1&quot;,
    content=&quot;documents related to python&quot;,
    metadata={
        &quot;category&quot;: &quot;datascience&quot;,
        &quot;tag_1&quot;: &quot;ai&quot;,
        &quot;tag_2&quot;: &quot;machine-learning&quot;,
    },
    containerTags=[&quot;user_123&quot;, &quot;project_xyz&quot;]
)
</code></pre><p>You can assign IDs, metadata, tags, etc. And retrieving is super simple as well:</p><pre><code class="language-python">
client.search.execute(
    q=&quot;machine learning concepts&quot;,
    limit=10
)
</code></pre><p>You can use NLP, and with automatic chunking, the retrieval is one of the fastest in the entire industry.<br>Apart from that, Supermemory offers multimodal search, sub-second retrieval, and a model-agnostic proxy that stretches context windows indefinitely!</p><h2 id="conclusion">Conclusion</h2><p>Awesome, your chatbots now have long-term memory as well. Now, take it to the next level by keeping a tight feedback loop: log prompts and retrievals, review mismatches, prune stale memories, and refactor schemas regularly. Keep a tab on the context and don&#x2019;t be sentimental about deleting what no longer serves.</p><p>Nail that, and a clear, relevant memory stack will turn your ordinary LLMs into extraordinary ones. However, LangGraph&apos;s solutions, like in-memory stores and traditional vector databases, start breaking at scale. That is where Supermemory&apos;s simple, yet infinitely scalable system comes in - test how easy it is to get started <a>today.</a></p>]]></content:encoded></item><item><title><![CDATA[How To Add Conversational Memory To LLMs Using LangChain]]></title><description><![CDATA[<p>Chatbots that don&#x2019;t remember conversations are very frustrating to work with. Users treat AI like a human and expect it to remember.</p><p>LangChain recently migrated to<a href="https://langchain-ai.github.io/langgraph/?ref=blog.supermemory.ai"> <u>LangGraph</u></a>, a new stateful framework for building multi-step, memory-aware LLM apps. So while the docs might still say &#x201C;LangChain memory,&#x201D;</p>]]></description><link>https://blog.supermemory.ai/how-to-add-conversational-memory-to-llms-using-langchain/</link><guid isPermaLink="false">6852ec478abc480001a22f0a</guid><category><![CDATA[Learning]]></category><dc:creator><![CDATA[Naman Bansal]]></dc:creator><pubDate>Thu, 19 Jun 2025 02:45:00 GMT</pubDate><media:content url="https://blog.supermemory.ai/content/images/2025/06/5.webp" medium="image"/><content:encoded><![CDATA[<img src="https://blog.supermemory.ai/content/images/2025/06/5.webp" alt="How To Add Conversational Memory To LLMs Using LangChain"><p>Chatbots that don&#x2019;t remember conversations are very frustrating to work with. Users treat AI like a human and expect it to remember.</p><p>LangChain recently migrated to<a href="https://langchain-ai.github.io/langgraph/?ref=blog.supermemory.ai"> <u>LangGraph</u></a>, a new stateful framework for building multi-step, memory-aware LLM apps. So while the docs might still say &#x201C;LangChain memory,&#x201D; what you&#x2019;re actually using under the hood is LangGraph.</p><p>In this guide, we&#x2019;ll walk through how to implement short-term conversational memory in LangChain using LangGraph.</p><p>We&#x2019;ll build a real-world chatbot and compare the two core approaches to memory in LangGraph: message trimming and summarizing (more on them later). You&#x2019;ll see how they differ, when to use what, and which one works best in a real use case.</p><p><strong>Note: </strong>Here&#x2019;s <a href="https://github.com/namancoderpro/memory-chatbot?ref=blog.supermemory.ai"><u>the link to the GitHub repo</u></a> containing the code snippets used in the article.</p><p><strong>But here&apos;s the reality: while these techniques work for demos and simple use cases, they quickly break down when you need reliable, scalable memory for real applications.</strong></p><h2 id="the-use-case-a-therapy-chatbot-with-memory">The Use Case: A Therapy Chatbot With Memory</h2><p>To keep this practical, we&#x2019;ll walk through a real-world example: a mental health chatbot. It&#x2019;s the perfect use case to test memory, because:</p><ul><li>Conversations are long and personal</li><li>Users often revisit old topics or emotions</li><li>The bot needs to remember specific details like names, moods, and events</li></ul><p>We&#x2019;ll use the same conversation flow across all memory types, so we can compare how each performs. By the end, you&#x2019;ll know exactly how to add memory to your LangChain app and which memory type makes the most sense for your use case.</p><h2 id="primer-what-is-conversational-memory">Primer: What is Conversational Memory?</h2><p>Conversational memory allows chatbots to remember the context of their conversation with the user and tailor their answers to new prompts accordingly for a more intelligent experience.</p><p>It&#x2019;s basically meant to model how you&#x2019;d talk to another person. Imagine trying to hold an hour-long conversation with someone who keeps forgetting everything you&#x2019;ve said. Not fun, right?</p><p>At an architectural level, memory in LLMs works like this:</p><ol><li><strong>Input and Output History</strong>: Every user message and model response is captured.</li><li><strong>Memory Storage</strong>: That conversation history is either:<ol><li>stored in the prompt window (ephemeral),</li><li>summarized and compressed (short-term memory), or</li><li>persisted in a separate, retrievable vector DB or external memory (long-term memory).</li></ol></li><li><strong>Retrieval Layer</strong>: At inference time, relevant pieces of past conversation are pulled in via raw replay, windowing, summarization, or vector similarity.</li><li><strong>Augmentation</strong>: That context is appended to the current prompt before sending to the LLM.</li></ol><figure class="kg-card kg-image-card"><img src="https://blog.supermemory.ai/content/images/2025/06/rep--1-.webp" class="kg-image" alt="How To Add Conversational Memory To LLMs Using LangChain" loading="lazy" width="2000" height="1125" srcset="https://blog.supermemory.ai/content/images/size/w600/2025/06/rep--1-.webp 600w, https://blog.supermemory.ai/content/images/size/w1000/2025/06/rep--1-.webp 1000w, https://blog.supermemory.ai/content/images/size/w1600/2025/06/rep--1-.webp 1600w, https://blog.supermemory.ai/content/images/size/w2400/2025/06/rep--1-.webp 2400w" sizes="(min-width: 720px) 720px"></figure><p>If we think of the most basic classification, then LLM memory can either be short-term or long-term. Here&#x2019;s the difference:</p>
<!--kg-card-begin: html-->
<table style="border:none;border-collapse:collapse;"><colgroup><col width="99"><col width="202"><col width="162"><col width="161"></colgroup><tbody><tr style="height:25pt"><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.38;text-align: center;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Type</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.38;text-align: center;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">What It Does</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.38;text-align: center;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">How It&#x2019;s Stored</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.38;text-align: center;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">When It&#x2019;s Used</span></p></td></tr><tr style="height:39.25pt"><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.38;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Short-term</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.38;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Tracks the current conversation window</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.38;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Prompt buffer, summarization</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.38;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Most chatbot interactions</span></p></td></tr><tr style="height:39.25pt"><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.38;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Long-term</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.38;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Remembers facts and context across sessions</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.38;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Vector store, DBs, JSON, etc.</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.38;margin-top:12pt;margin-bottom:12pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Persistent user knowledge, histories</span></p></td></tr></tbody></table>
<!--kg-card-end: html-->
<p>While this architecture is clever, it&apos;s still fundamentally limited. Managing threads manually becomes a nightmare at scale, and the memory patterns are too rigid for complex, evolving user relationships.</p><h2 id="langgraph-memory-architecture-short-term-vs-long-term">LangGraph Memory Architecture: Short-Term vs Long-Term</h2><p>LangGraph introduces a new way of handling memory that&#x2019;s far more powerful than the old ConversationBufferMemory-style classes you might be used to. It supports both short-term and long-term memory through state management and memory stores.</p><figure class="kg-card kg-image-card"><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXdDAWjG1VMPSSjc8NLe2mtrQYoheIyAr_In-26GbrpSfqMlnMC7heNmoJ-Uur1ZEE7nCdTyEtnd6n5FXZPQ4Rpy8hhLegIgI6xGW7D1ACkTsU3mSYLo_S9o5AxfurS6fgPzlygPbQ?key=D_xLASwNMHAmhwMRUaWFmA" class="kg-image" alt="How To Add Conversational Memory To LLMs Using LangChain" loading="lazy" width="624" height="201"></figure><p>LangGraph has a built-in persistence layer that allows it to remember what&#x2019;s going on. It has a few key concepts like state, threads, and checkpoints.</p><p>Think of building a chatbot with LangGraph as writing in a notebook. The state refers to the notebook page where you&apos;re recording the actual conversation: what&#x2019;s been said, what tools were used, what decisions were made.&#xA0;</p><p>Every time something happens in your app, this state, or notebook page, gets updated with what happened.</p><p>Now, imagine that every user, or session, gets their own notebook. That&#x2019;s what a thread is: an isolated session that stores several different states and has a unique ID to identify it.&#xA0;</p><p>If a user returns tomorrow, you just give LangGraph the same thread ID, and it picks up right where you left off, flipping back to the last used page.</p><p>Now, let&#x2019;s say you want to bookmark key moments in your notebook so you can return to them. That&#x2019;s what checkpoints are: a snapshot of the state at a specific time.&#xA0;</p><p>LangGraph automatically creates a checkpoint every time you invoke a graph with a thread ID.</p><h3 id="how-langchain-implements-short-term-memory">How LangChain Implements Short-Term Memory</h3><p>You basically store the conversation history in the state, usually as a list of messages. Each time a user sends a message or the LLM generates a response, it is appended to this list, which is the short-term memory of the LLM. You can also store documents, uploaded files, and other metadata in the graph&#x2019;s state so that the LLM has access to the full context.</p><p>There are two main ways to maintain this list of messages:</p><ul><li><strong>Message Buffering: </strong>Keeps the last <em>k </em>messages in memory</li><li><strong>Summarization: </strong>Replace older history with a summary</li></ul><figure class="kg-card kg-image-card"><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXcvy-TuEeVfx0DiSE5IRoNNz15qkoRL3vAmRBPzbuYxf2pWnue-XyRXIb9yjT-xeLVCLmXV6HXip5w3x4Ka5W9DFgBKelNa50L0a2jgb2XI2h8nckCzhxuqZe4UabQG3FJ_SNy2?key=D_xLASwNMHAmhwMRUaWFmA" class="kg-image" alt="How To Add Conversational Memory To LLMs Using LangChain" loading="lazy" width="594" height="200"></figure><p>Now, we&#x2019;ll implement these in code for our therapy chatbot.</p><h2 id="adding-conversational-memory-to-our-llm">Adding Conversational Memory To Our LLM</h2><p>First, we&#x2019;ll start by storing all the messages in our therapy chatbot&#x2019;s context, and then we&#x2019;ll move on to advanced techniques like message trimming and summarizing.</p><h3 id="basic-setup">Basic Setup</h3><p>Let&#x2019;s start with the setup first. Create a directory for the chatbot and open it in your IDE:<br>
<br></p>
<pre><code class="language-bash">mkdir memory-chatbot
</code></pre>
<p>Install all the necessary Python libraries:<br>
<br></p>
<pre><code class="language-python">pip install --upgrade --quiet langchain langchain-openai langgraph
</code></pre>
<br>
<p><strong>Note:</strong> Use <code>pip3</code> in case just <code>pip</code> doesn&#x2019;t work.</p>
<h3 id="basic-memory">Basic Memory</h3><p>Let&#x2019;s start building the chatbot. Create a <code>file.py</code> file in your folder and import the necessary libraries:</p>
<br> 
<pre><code class="language-python">from langchain_openai import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph, START, MessagesState
import os
</code></pre>
<br> 
<p>You&#x2019;ll start by setting your OpenAI API key as an environment variable. Navigate to your <code>memory-chatbot</code> folder and run the following command in the terminal to set the environment variable:</p>
<br> 
<pre><code class="language-bash">export OPENAI_API_KEY=&#x201D;YOUR_API_KEY&#x201D;
</code></pre>
<br> 
<p>Here are the steps to <a href="https://help.openai.com/en/articles/4936850-where-do-i-find-my-openai-api-key?ref=blog.supermemory.ai">find your OpenAI API key.</a></p>
<p>Back to <code>file.py</code>. Retrieve the environment variable:<br>
<br></p>
<pre><code class="language-python">os.environ.get(&quot;OPENAI_API_KEY&quot;)
</code></pre>
<br> 
<p>Now, initialize your model:<br>
<br></p>
<pre><code class="language-python">model = ChatOpenAI(model=&quot;gpt-4o-mini&quot;, temperature=0)
</code></pre>
<br> 
<p>The next step while building an LLM with LangGraph is to initialize the graph that represents the workflow that gets carried out. Here&#x2019;s how:</p>
<br>
<pre><code class="language-python">builder = StateGraph(state_schema=MessagesState)
</code></pre>
<br> 
<p>Since the chatbot stores the chat history, this code defines <code>MessagesState</code> as the schema, which means the state contains a list of messages.</p>
<p>Now, if you think about it, graphs contain nodes that carry out the actual logic. Our main logic is going to be the chatbot that acts like a therapist, asks questions, generates responses, and stores them in its memory.</p>
<p>Thus, we&#x2019;ll now define a function that carries out this logic as follows:</p>
<br>
<pre><code class="language-python">def chat_node(state: MessagesState):
&#xA0;&#xA0;&#xA0;system_message = SystemMessage(content=&quot;You&apos;re a kind therapy assistant.&quot;)
&#xA0;&#xA0;&#xA0;history = state[&quot;messages&quot;]
&#xA0;&#xA0;&#xA0;prompt = [system_message] + history
&#xA0;&#xA0;&#xA0;response = model.invoke(prompt)
&#xA0;&#xA0;&#xA0;return {&quot;messages&quot;: response}
</code></pre>
<br> 
<p>The node is passed the state, which is the chat history. Prompts to LLMs contain a system instruction (developer instructions on how the LLM must behave) and a human message.</p>
<p>The code instructs the LLM to act as a therapy assistant, and our prompt contains both the system message and the complete list of messages since we&#x2019;re passing the entire context to it. This prompt is sent to OpenAI, which returns a response.</p>
<p>Let&#x2019;s add this node to our graph and define it as the start node as follows:</p>
<br> 
<pre><code class="language-python">builder.add_node(&quot;chat&quot;, chat_node)
builder.add_edge(START, &quot;chat&quot;)
</code></pre>
<br> 
<p>Next, write the following:</p>
<br> 
<pre><code class="language-python">memory = MemorySaver()
chat_app = builder.compile(checkpointer=memory)
</code></pre>
<br> 
<p>The <code>MemorySaver()</code> stores the state in memory, and the <code>builder.compile</code> compiles and executes the graph with the memory checkpointer.<br>
If you go back to our breakdown of LangChain&#x2019;s architecture, you&#x2019;ll remember that LangChain used a unique thread identifier to set a unique ID for each conversation and store its memory separately. Declare that in your code:</p>
<br> 
<pre><code class="language-python">thread_id = &quot;1&quot;
</code></pre>
<br> 
<p>Finally, let&#x2019;s allow the user to enter messages and update our app&#x2019;s state accordingly with the following code:<br>
<br></p>
<pre><code class="language-python">while True:
&#xA0;&#xA0;&#xA0;user_input = input(&quot;You: &quot;)
&#xA0;&#xA0;&#xA0;state_update = {&quot;messages&quot;: [HumanMessage(content=user_input)]}
&#xA0;&#xA0;
&#xA0;&#xA0;&#xA0;result = chat_app.invoke(
&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;state_update,
&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;{&quot;configurable&quot;: {&quot;thread_id&quot;: thread_id}}
&#xA0;&#xA0;&#xA0;)
&#xA0;&#xA0;
&#xA0;&#xA0;&#xA0;print(result)
&#xA0;&#xA0;&#xA0;ai_msg = result[&quot;messages&quot;][-1]
&#xA0;&#xA0;&#xA0;print(&quot;Bot:&quot;, ai_msg.content)
</code></pre>
<br>
<p>This code takes the user input, wraps it in the <code>HumanMessage</code> class, and appends the list of messages stored. After that, the graph is invoked with <code>chat_app.invoke</code>. The result is then printed.</p>
<p>Upon executing the code, we get the following results:</p>
<br> 
<pre><code class="language-bash">You: Hi I&apos;m John from San Francisco
Bot: Hi John! It&apos;s nice to meet you. How are you doing today?
You : I&apos;m doing okay, but I need some help managing my anxiety.Bot: I&#x2019;m glad you reached out, John. Managing anxiety can be challenging, but there are strategies that can help. Can you share a bit more about what specifically triggers your anxiety or how it manifests for you?
</code></pre>
<br> 
<p>If we print the actual <code>messages</code> response, then we notice the following after the bot&#x2019;s first response:</p>
<br> 
<pre><code class="language-bash">{&apos;messages&apos;: [HumanMessage(content=&quot;Hi I&apos;m John from San Francisco&quot;,...), AIMessage(content=&quot;Hi John! It&apos;s nice to meet you. How are you doing today?&quot;, &#x2026;), HumanMessage(content=&quot;I&apos;m doing okay, but I need some help managing my anxiety.&quot;, &#x2026;), AIMessage(content=&apos;I&#x2019;m glad you reached out, John. Managing anxiety can be challenging, but there are strategies that can help. Can you share a bit more about what specifically triggers your anxiety or how it manifests for you?&apos;, &#x2026;)]}
</code></pre>
<p>What you can notice is that the <code>messages</code> key contains all the chats. The bot maintains a memory by storing all the previous chats and responding to new questions accordingly. Every time a question is asked, this entire list of messages is sent as part of the prompt.</p>
<p>This approach is pretty simple and gives us a workable chatbot, but it has some severe drawbacks. LangGraph replays all the messages stored in memory for every new message, and the full system prompt + message history is sent to the LLM as input.</p>
<p>LLMs have a finite token limit. Eventually, the prompt becomes too large to fit into the model&#x2019;s context window, which leads to a shit ton of errors.</p>
<p>On top of that, more tokens = more cost = higher OpenAI bills. Our therapy chatbot would have long conversations, so that would become super expensive, super fast. Lastly, token-heavy prompts would also degrade performance.</p>
<p>That&#x2019;s where the next approach comes in: message trimming.</p>
<h3 id="memory-with-message-trimming">Memory With Message Trimming</h3><p>Message trimming only stores a certain <code>k</code> number of messages in the memory, which prevents memory overload, stays within token limits, and focuses only on recent context.</p>
<p>To implement it, create a new file <code>trimmed.py</code> and add the following code to it:</p>
<pre><code class="language-python">from langchain_openai import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph, START, MessagesState
from langchain_core.messages import trim_messages
import os
os.environ.get(&quot;OPENAI_API_KEY&quot;)
# Initialize model
model = ChatOpenAI(model=&quot;gpt-4o-mini&quot;, temperature=0)
trimmer = trim_messages(strategy=&quot;last&quot;, max_tokens=2, token_counter=len)
# Create the graph
builder = StateGraph(state_schema=MessagesState)&#xA0;&#xA0;&#xA0;
def chat_node(state: MessagesState):
&#xA0;&#xA0;&#xA0;trimmed_messages = trimmer.invoke(state[&quot;messages&quot;])
&#xA0;&#xA0;&#xA0;system_message = SystemMessage(content=&quot;You&apos;re a kind therapy assistant.&quot;)
&#xA0;&#xA0;&#xA0;prompt = [system_message] + trimmed_messages
&#xA0;&#xA0;&#xA0;response = model.invoke(prompt)
&#xA0;&#xA0;&#xA0;return {&quot;messages&quot;: response}
builder.add_node(&quot;chat&quot;, chat_node)
builder.add_edge(START, &quot;chat&quot;)
# Compile graph with MemorySaver
memory = MemorySaver()
chat_app = builder.compile(checkpointer=memory)
thread_id = &quot;2&quot;
while True:
&#xA0;&#xA0;&#xA0;user_input = input(&quot;You: &quot;)
&#xA0;&#xA0;&#xA0;state_update = {&quot;messages&quot;: [HumanMessage(content=user_input)]}
&#xA0;&#xA0;
&#xA0;&#xA0;&#xA0;result = chat_app.invoke(
&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;state_update,
&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;{&quot;configurable&quot;: {&quot;thread_id&quot;: thread_id}}
&#xA0;&#xA0;&#xA0;)
&#xA0;&#xA0;
&#xA0;&#xA0;&#xA0;print(result)
&#xA0;&#xA0;&#xA0;ai_msg = result[&quot;messages&quot;][-1]
&#xA0;&#xA0;&#xA0;print(&quot;Bot:&quot;, ai_msg.content)
</code></pre>
<p>You&#x2019;ll notice most of this is the same as the previous file. Except, now we&#x2019;re importing the <code>trim_messages</code> function:</p>
<pre><code class="language-python">from langchain_core.messages import trim_messages
</code></pre>
<p>And, we&#x2019;re declaring a <code>trimmer</code> at the beginning of our code:</p>
<pre><code class="language-python">trimmer = trim_messages(strategy=&quot;last&quot;, max_tokens=10, token_counter=len)
</code></pre>
<p>This <code>trimmer</code> counts every message as 1 token, and only keeps the last 10 tokens (5 pairs of human + AI conversation).</p>
<p>The <code>chat_node</code> function has also been edited:<br>
<br></p>
<pre><code class="language-python">def chat_node(state: MessagesState):
&#xA0;&#xA0;&#xA0;trimmed_messages = trimmer.invoke(state[&quot;messages&quot;])
&#xA0;&#xA0;&#xA0;system_message = SystemMessage(content=&quot;You&apos;re a kind therapy assistant.&quot;)
&#xA0;&#xA0;&#xA0;prompt = [system_message] + trimmed_messages
&#xA0;&#xA0;&#xA0;response = model.invoke(prompt)
&#xA0;&#xA0;&#xA0;return {&quot;messages&quot;: response}
</code></pre>
<br>
<p>First, the messages are trimmed by invoking the trimmer on the current state, and then those trimmed messages are passed to the prompt. Thus, the entire state is not passed, saving tokens.</p>
<p>Now, if we replay our previous conversation, it goes as follows:</p>
<br>
<pre><code class="language-bash">You: hi im john from sf
Bot: Hi John! It&apos;s nice to meet you. How are you doing today?

You: im doing okay, you?&#xA0;&#xA0;
Bot: I&#x2019;m glad to hear you&#x2019;re doing okay! I&#x2019;m here and ready to help with anything you need. Is there something specific on your mind today?

You: yeah dealing with some general anxiety
Bot: I&#x2019;m sorry to hear that you&#x2019;re feeling anxious. It&#x2019;s completely normal to experience anxiety from time to time. Would you like to talk about what&#x2019;s been causing your anxiety, or would you prefer some tips on managing it?

You: idk whats causing it - stressed about work i think
Bot: That makes sense. Work-related stress can definitely contribute to feelings of anxiety. It might help to break down what aspects of work are causing you stress. Is it the workload, deadlines, interactions with colleagues, or something else? Sometimes identifying specific triggers can make it easier to manage those feelings.

You: yeah i think it&apos;s a mix of them all&#xA0;&#xA0;
Bot: That sounds really overwhelming. It&#x2019;s tough when multiple factors contribute to stress. Have you had a chance to take breaks or practice any self-care during busy times? Even small moments of relaxation can help alleviate some of that anxiety.

You: yeah i guess that makes sense. i should take a break right?
Bot: Absolutely! Taking a break can be really beneficial. It gives your mind a chance to reset and can help reduce feelings of anxiety. Even a short walk, some deep breathing exercises, or just stepping away from your workspace for a few minutes can make a difference. What do you think you might do for your break?

You: what&apos;s my name?
Bot: I&#x2019;m sorry, but I don&#x2019;t have access to personal information like your name unless you share it with me. If you&#x2019;d like to tell me your name or anything else, feel free!
</code></pre>
<br>
<p>As you can see, when I ask the what&#x2019;s my name question, the previous 5 memories don&#x2019;t contain it, thus the chatbot forgets. It&#x2019;s an improvement over the basic model, but it still has drawbacks:</p>
<ul>
<li>Trimming may lead to loss of crucial context, which is not a part of the last 10 tokens.</li>
<li>Trimming isn&#x2019;t intelligent, so there&#x2019;s an arbitrary information cutoff.</li>
<li>Even with trimming, token creep might happen if the input/output is extremely long.</li>
</ul>
<p>The next approach, summarization of old context, builds upon these.</p>
<h3 id="memory-with-summarization">Memory With Summarization</h3><figure class="kg-card kg-image-card"><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXf_PdVuCaPCEO9h51RXhJXfkEdCMyt_Yn-fUVEFK9afS_hh_midNqAIgpcazIa8yW3c9nnK8BYqIUz-STFzAsCBcURLZztLH_hbdMFRW9WRQSWMuxCYS4yWAeTucgW074rnTx_Gew?key=D_xLASwNMHAmhwMRUaWFmA" class="kg-image" alt="How To Add Conversational Memory To LLMs Using LangChain" loading="lazy" width="609" height="242"></figure><p>Summarization allows the LLM to summarize all conversations before the current one, thus reducing token usage while also ensuring minimal context loss.</p>
<p>Create a new file <code>summarized.py</code> and write the following code in it:<br>
<br></p>
<pre><code class="language-python">from langchain_openai import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage
from langchain_core.messages import RemoveMessage
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph, START, MessagesState
import os
os.environ.get(&quot;OPENAI_API_KEY&quot;)
# Initialize model
model = ChatOpenAI(model=&quot;gpt-4o-mini&quot;, temperature=0)
# Create graph
builder = StateGraph(state_schema=MessagesState)&#xA0;&#xA0;&#xA0;
def chat_node(state: MessagesState):
&#xA0;&#xA0;&#xA0;system_message = SystemMessage(content=&quot;You&apos;re a kind therapy assistant.&quot;)
&#xA0;&#xA0;&#xA0;history = state[&quot;messages&quot;][:-1]
&#xA0;&#xA0;&#xA0;if len(history) &gt;= 8:
&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;last_human_message = state[&quot;messages&quot;][-1]
&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;summary_prompt = (
&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&quot;Distill the above chat messages into a single summary message. &quot;
&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&quot;Include as many specific details as you can.&quot;
&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;)
&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;summary_message = model.invoke(history + [HumanMessage(content=summary_prompt)])
&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;delete_messages = [RemoveMessage(id=m.id) for m in state[&quot;messages&quot;]]
&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;human_message = HumanMessage(content=last_human_message.content)
&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;response = model.invoke([system_message, summary_message, human_message])
&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;message_updates = [summary_message, human_message, response] + delete_messages
&#xA0;&#xA0;&#xA0;else:
&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;message_updates = model.invoke([system_message] + state[&quot;messages&quot;])
&#xA0;&#xA0;&#xA0;return {&quot;messages&quot;: message_updates}
builder.add_node(&quot;chat&quot;, chat_node)
builder.add_edge(START, &quot;chat&quot;)
# Compile graph with MemorySaver
memory = MemorySaver()
chat_app = builder.compile(checkpointer=memory)
thread_id = &quot;3&quot;
while True:
&#xA0;&#xA0;&#xA0;user_input = input(&quot;You: &quot;)
&#xA0;&#xA0;&#xA0;state_update = {&quot;messages&quot;: [HumanMessage(content=user_input)]}
&#xA0;&#xA0;
&#xA0;&#xA0;&#xA0;result = chat_app.invoke(
&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;state_update,
&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;{&quot;configurable&quot;: {&quot;thread_id&quot;: thread_id}}
&#xA0;&#xA0;&#xA0;)
&#xA0;&#xA0;
&#xA0;&#xA0;&#xA0;print(result)
&#xA0;&#xA0;&#xA0;ai_msg = result[&quot;messages&quot;][-1]
&#xA0;&#xA0;&#xA0;print(&quot;Bot:&quot;, ai_msg.content)
</code></pre>
<br> 
<p>This has some more significant changes. Firstly, you&#x2019;re importing the <code>RemoveMessages</code> function, which wasn&#x2019;t there previously.<br>
In terms of logic, there are changes in the <code>chat_node</code> function. The summarization logic only kicks in when the total conversation has gone over 8 messages.</p>
<p>If that happens, first the history is retrieved, except the latest HumanMessage, which is why we&#x2019;ve used the [:-1] operator. Then, the model is invoked with a prompt to summarize the content, and the <code>history</code> variable is passed along with the instructions.</p>
<p>After that, all the summarized messages are removed from the state to save on tokens. The latest human message is wrapped again inside the <code>HumanMessage()</code> class, and then the model is invoked again with the summary and the human message, but this time with the therapy chatbot system message.</p>
<p>Finally, the model response is returned with the new information.<br>
If the history is less than 8 messages, then normal conversation chaining is used. This makes the chatbot efficient for shorter chats as well.</p>
<p>Upon running some test instructions, the following is seen:<br>
<br></p>
<pre><code class="language-bash">You: hi im jack from sf
Bot: Hi Jack! It&apos;s nice to meet you. How are you doing today?

You: im okay, wbu?
Bot: I&#x2019;m doing well, thank you! I&#x2019;m here to help you with anything you need. Is there something specific on your mind today?

You: just a bit of general anxiety
Bot: I&#x2019;m sorry to hear that you&#x2019;re feeling anxious. It&#x2019;s completely normal to experience anxiety from time to time. Would you like to talk about what&#x2019;s been causing your anxiety, or would you prefer some tips on how to manage it?

You: probably work is causing it
Bot: Work can definitely be a significant source of stress and anxiety. Would you like to share more about what aspects of work are making you feel anxious? Sometimes just talking about it can help.

You: yeah i think it&apos;s about how fast the deadlines are now
Bot: It sounds like the pressure of tight deadlines is really weighing on you. That can definitely contribute to feelings of anxiety. Would you like to talk about what specific aspects of the deadlines are causing you the most stress? Sometimes breaking it down can help us find ways to manage it better.
</code></pre>
<br> 
<p>The conversation flows as normal, however if you look at the full message output, after the 8 messages, we receive the following output in the console:<br>
<br></p>
<pre><code class="language-bash">{&apos;messages&apos;: [AIMessage(content=&apos;**Jack from San Francisco mentioned he was feeling okay but experiencing some general anxiety, which he attributed to work-related stress. He expressed a desire to discuss the causes of his anxiety or seek tips for managing it.**&apos;,...), HumanMessage(content=&quot;yeah i think it&apos;s about how fast the deadlines are now&quot;,...), AIMessage(content=&apos;It sounds like the pressure of tight deadlines is really weighing on you. That can definitely contribute to feelings of anxiety. Would you like to talk about what specific aspects of the deadlines are causing you the most stress? Sometimes breaking it down can help us find ways to manage it better.&apos;, &#x2026;)]}
</code></pre>
<p>As you&#x2019;ll see, everything else from before was summarized, while the two latest messages are stored in memory. This is the best approach we&#x2019;ve explored until now, but as always, there&#x2019;s always room for improvement.</p>
<p>You see:</p>
<ul>
<li>Summaries are only as good as the prompt + model, so the quality&#x2019;s a bit unreliable.</li>
<li>Hard deletes can be risky. If the summarization is off, you&#x2019;ve lost the original data.</li>
</ul>
<h2 id="evaluating-both-the-techniques">Evaluating both the techniques</h2><p>For evaluation of the trimming and summarizing chatbots, we created 6 generic questions/responses we&#x2019;d pass into the chatbots to see how they&#x2019;d perform. We&#x2019;re also measuring them across 6 quantitative metrics:</p>
<!--kg-card-begin: html-->
<table style="border:none;border-collapse:collapse;"><colgroup><col width="108"><col width="199"><col width="148"><col width="168"></colgroup><tbody><tr style="height:39.25pt"><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.38;text-align: center;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Metric</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.38;text-align: center;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Description</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.38;text-align: center;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Scoring (0&#x2013;1)</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.38;text-align: center;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Notes</span></p></td></tr><tr style="height:52.75pt"><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.38;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Fact Retention</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.38;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Mentions or uses previous facts correctly (name, job, etc.)</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.38;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">1 = correct use; 0 = wrong/missing</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.38;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">-</span></p></td></tr><tr style="height:39.25pt"><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.38;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Entity Count</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.38;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Number of correctly named entities reused</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.38;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">0 to worse chatbot; 1 to the better one. If tied, 1 to both.</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.38;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Proxy for detail richness</span></p></td></tr><tr style="height:52.75pt"><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.38;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Latency / Turn Depth</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.38;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">How far back memory is used (e.g., Turn 1 fact used in Turn 6)</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.38;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">0 -1</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.38;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Higher is better</span></p></td></tr><tr style="height:39.25pt"><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.38;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Token Usage</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.38;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Total tokens used in the conversation</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.38;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">0 - 1</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.38;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Counted via OpenAI or LangSmith tools</span></p></td></tr><tr style="height:39.25pt"><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.38;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Response Length</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.38;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Average word/token count per AI reply</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.38;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">0 -1&#xA0;</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.38;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Spot unnecessarily long answers</span></p></td></tr><tr style="height:52.75pt"><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.38;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Repetition Penalty</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.38;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Counts of repeated generic phrases</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.38;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">-1 or 0</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.38;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Penalize fluff like &#x201C;I understand how you feel.&#x201D;</span></p></td></tr></tbody></table>
<!--kg-card-end: html-->
<p>Our 6 prompts are as follows:</p>
<!--kg-card-begin: html-->
<table style="border:none;border-collapse:collapse;"><colgroup><col width="384"></colgroup><tbody><tr style="height:25pt"><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><ul style="margin-top:0;margin-bottom:0;padding-inline-start:48px;"><li dir="ltr" style="list-style-type:disc;font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;" aria-level="1"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;" role="presentation"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">&#x201C;Hi, I&#x2019;m John. I&#x2019;ve been feeling overwhelmed lately.&#x201D;</span></p></li></ul></td></tr><tr style="height:39.25pt"><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><ul style="margin-top:0;margin-bottom:0;padding-inline-start:48px;"><li dir="ltr" style="list-style-type:disc;font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;" aria-level="1"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;" role="presentation"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">&#x201C;It started after I got promoted to product manager..&#x201D;</span></p></li></ul></td></tr><tr style="height:39.25pt"><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><ul style="margin-top:0;margin-bottom:0;padding-inline-start:48px;"><li dir="ltr" style="list-style-type:disc;font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;" aria-level="1"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;" role="presentation"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">&#x201C;I usually go for a run in the morning, but I&#x2019;ve stopped doing that recently.&#x201D;</span></p></li></ul></td></tr><tr style="height:39.25pt"><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><ul style="margin-top:0;margin-bottom:0;padding-inline-start:48px;"><li dir="ltr" style="list-style-type:disc;font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;" aria-level="1"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;" role="presentation"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">&#x201C;My therapist told me to start journaling, but I haven&#x2019;t.&#x201D;</span></p></li></ul></td></tr><tr style="height:25pt"><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><ul style="margin-top:0;margin-bottom:0;padding-inline-start:48px;"><li dir="ltr" style="list-style-type:disc;font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;" aria-level="1"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;" role="presentation"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">&#x201C;Also, I had a fight with my brother Jack yesterday.&#x201D;</span></p></li></ul></td></tr><tr style="height:39.25pt"><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><ul style="margin-top:0;margin-bottom:0;padding-inline-start:48px;"><li dir="ltr" style="list-style-type:disc;font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;" aria-level="1"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;" role="presentation"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">&#x201C;Can you help me make a plan to feel more in control again?&#x201D;</span></p></li></ul></td></tr></tbody></table>
<!--kg-card-end: html-->
<h3 id="trimming-set-at-8-tokens">Trimming (set at 8 tokens)</h3><pre><code class="language-bash">You: &#x201C;Hi, I&#x2019;m John. I&#x2019;ve been feeling overwhelmed lately.&#x201D;
Bot: Hi John, it&#x2019;s great to meet you. I&#x2019;m really sorry to hear that you&#x2019;ve been feeling overwhelmed. Would you like to share a bit more about what&#x2019;s been going on? Sometimes talking about it can help.

You: &#x201C;It started after I got promoted to product manager..&#x201D;
Bot: That sounds like a big change, John. Congratulations on your promotion! It&#x2019;s understandable to feel overwhelmed with new responsibilities. What aspects of your new role are feeling particularly challenging for you?

You: &#x201C;I usually go for a run in the morning, but I&#x2019;ve stopped doing that recently.&#x201D;
Bot: It sounds like running has been an important part of your routine. Exercise can be a great way to manage stress and clear your mind. What do you think has made it difficult for you to keep up with your morning runs lately?

You: &#x201C;My therapist told me to start journaling, but I haven&#x2019;t.&#x201D;
Bot: It&#x2019;s okay, John. Starting something new like journaling can be tough, especially when you&#x2019;re feeling overwhelmed. Sometimes it helps to set small, manageable goals. Would you be open to trying just a few minutes of journaling each day, or even just jotting down a few thoughts when you feel up to it? It can be a great way to process your feelings.

You: &#x201C;Also, I had a fight with my brother Jack yesterday.&#x201D;
Bot: I&#x2019;m sorry to hear that, John. Conflicts with family can be really tough to navigate. Would you like to talk about what happened with Jack? Sometimes discussing the situation can help you find some clarity or a way to approach it.

You: &#x201C;Can you help me make a plan to feel more in control again?&#x201D;
Bot: Absolutely, John. It&#x2019;s great that you want to take steps to feel more in control. Here&#x2019;s a simple plan we can start with:
1. **Set Small Goals**: Choose one or two small, achievable goals each week. For example, you might aim to go for a short run a couple of times or write in your journal for just five minutes.
2. **Establish a Routine**: Try to create a daily routine that includes time for exercise, journaling, and some relaxation. Consistency can help you feel more grounded.
3. **Communicate with Jack**: If you feel ready, consider reaching out to your brother to talk about the fight. A calm conversation can help clear the air and strengthen your relationship.
4. **Practice Mindfulness**: Incorporate a few minutes of mindfulness or deep breathing into your day. This can help reduce stress and bring you back to the present moment.
5. **Reflect Weekly**: At the end of each week, take a moment to reflect on what went well and what you might want to adjust for the following week.
How does that sound to you? Would you like to modify any part of this plan?
</code></pre>
<p>Tokens used: 1.618K completion tokens</p><h3 id="summarizing-set-at-8-messages">Summarizing (set at 8 messages)</h3><p>Here&#x2019;s how it responded:</p><pre><code class="language-bash">You: &#x201C;Hi, I&#x2019;m John. I&#x2019;ve been feeling overwhelmed lately.&#x201D;
Bot: Hi John, it&#x2019;s nice to meet you. I&#x2019;m really glad you reached out. Feeling overwhelmed can be tough to deal with. Would you like to share more about what&#x2019;s been on your mind or what&#x2019;s been making you feel this way?

You: &#x201C;It started after I got promoted to product manager..&#x201D;
Bot: That sounds like a significant change, John. Congratulations on your promotion! It&#x2019;s understandable to feel overwhelmed with new responsibilities. What aspects of your new role are feeling particularly challenging for you?

You: &#x201C;I usually go for a run in the morning, but I&#x2019;ve stopped doing that recently.&#x201D;
Bot: It sounds like running was an important part of your routine and a way for you to manage stress. It&#x2019;s common for changes in our lives, like a new job, to disrupt our usual habits. How do you feel about not being able to run lately? Do you think getting back to it might help you feel a bit better?

You: &#x201C;My therapist told me to start journaling, but I haven&#x2019;t.&#x201D;
Bot: Journaling can be a really helpful tool for processing your thoughts and feelings, but it can be hard to get started, especially when you&#x2019;re feeling overwhelmed. It&#x2019;s okay if you haven&#x2019;t begun yet. Maybe you could start with just a few minutes a day or even jot down a few thoughts when you feel inspired. What do you think might help you get started with journaling?

You: &#x201C;Also, I had a fight with my brother Jack yesterday.&#x201D;
Bot: I&apos;m sorry to hear that you had a fight with your brother, Jack. Conflicts with loved ones can be really tough to navigate. How are you feeling about the situation? Do you want to talk about what happened? Sometimes expressing your thoughts and feelings can help you process the experience.

You: &#x201C;Can you help me make a plan to feel more in control again?&#x201D;
Bot: Absolutely, I&#x2019;d be happy to help you create a plan to feel more in control. Here&#x2019;s a simple outline you can follow:
1. **Reintroduce Morning Runs:**
&#xA0;&#xA0;&#xA0;- **Set a Schedule:** Choose specific days and times for your runs. Start with two or three days a week.
&#xA0;&#xA0;&#xA0;- **Start Small:** If you&#x2019;re feeling overwhelmed, begin with shorter runs or even brisk walks to ease back into it.
2. **Begin Journaling:**
&#xA0;&#xA0;&#xA0;- **Set Aside Time:** Dedicate a few minutes each day, perhaps in the morning or before bed, to write down your thoughts and feelings.
&#xA0;&#xA0;&#xA0;- **Prompts:** If you&#x2019;re unsure what to write about, consider prompts like &#x201C;What am I grateful for today?&#x201D; or &#x201C;What challenges did I face, and how did I handle them?&#x201D;
3. **Address the Conflict with Jack:**
&#xA0;&#xA0;&#xA0;- **Reflect:** Take some time to think about what led to the fight and how you feel about it.
&#xA0;&#xA0;&#xA0;- **Communicate:** When you feel ready, consider reaching out to Jack to talk about what happened. Express your feelings and listen to his perspective.
4. **Practice Mindfulness:**
&#xA0;&#xA0;&#xA0;- **Breathing Exercises:** Incorporate deep breathing or meditation into your daily routine to help manage stress.
&#xA0;&#xA0;&#xA0;- **Mindful Moments:** Take a few moments throughout the day to check in with yourself and acknowledge your feelings.
5. **Set Realistic Goals:**
&#xA0;&#xA0;&#xA0;- **Daily Goals:** Identify one or two small goals each day that can help you feel accomplished, whether it&#x2019;s going for a run, journaling, or reaching out to a friend.
6. **Seek Support:**
&#xA0;&#xA0;&#xA0;- **Talk to Someone:** Whether it&#x2019;s a friend, family member, or therapist, sharing your feelings can provide relief and perspective.
How does this plan sound to you? Would you like to adjust anything or add more specific details?
</code></pre>
<p>Tokens used: 1,940 tokens</p><p>Evaluation:</p>
<!--kg-card-begin: html-->
<table style="border:none;border-collapse:collapse;"><colgroup><col width="92"><col width="129"><col width="79"><col width="114"><col width="210"></colgroup><tbody><tr style="height:25pt"><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;text-align: center;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Metric</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;text-align: center;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Description</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;text-align: center;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Trimming</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;text-align: center;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Summarization</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;text-align: center;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Notes</span></p></td></tr><tr style="height:67pt"><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Fact Retention</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Correct recall/use of facts from earlier turns</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">1</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">1</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Both bots correctly referenced all six facts (John, promotion, running, journaling, Jack, and feeling overwhelmed).</span></p></td></tr><tr style="height:67pt"><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Entity Count</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Number of correctly named entities reused</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">1</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">1</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Both mentioned John, Jack, running, journaling, etc. Summarization didn&#x2019;t sacrifice detail.</span></p></td></tr><tr style="height:67pt"><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Latency / Turn Depth</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">How far back a memory is reused (e.g., Turn 1 info in Turn 6)</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">0</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">1</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Summarization reused context from all 5 previous turns in the final response, trimming lost older context earlier.</span></p></td></tr><tr style="height:80.5pt"><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Token Usage</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Total tokens used</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">1</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">0</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Trimming uses fewer tokens due to shorter prompts and hard message cuts; summarization adds summarizer calls and more verbose responses.</span></p></td></tr><tr style="height:80.5pt"><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Avg. Response Length</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Average length of bot replies</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">1</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">0</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Trimming&#x2019;s responses are shorter, while summarization is more verbose; it can be less desirable for token-sensitive applications.</span></p></td></tr><tr style="height:80.5pt"><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Repetition Penalty</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Penalty for repeated generic phrases (lower = better)</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">-1</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">0</span></p></td><td style="border-left:solid #ffffff 1pt;border-right:solid #ffffff 1pt;border-bottom:solid #ffffff 1pt;border-top:solid #ffffff 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;"><p dir="ltr" style="line-height:1.2;margin-top:0pt;margin-bottom:0pt;"><span style="font-size:12pt;font-family:Arial,sans-serif;color:#ffffff;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Trimming led to more generic fallback lines (&#x201C;That&#x2019;s understandable.&#x201D;); summarization preserved specificity despite verbosity.</span></p></td></tr></tbody></table>
<!--kg-card-end: html-->
<p>Final scores are 3-3 for both models, tied. While both trimming and summarization scored equally in our evaluation, the tie doesn&#x2019;t imply that the two strategies are interchangeable.</p><p>Trimming excels in token economy and snappy responses. It&#x2019;s simple, lightweight, and great for short or transactional conversations. But once the dialogue gets deeper or longer, it starts forgetting context, leading to generic or disconnected replies, which earns it a repetition penalty.</p><p>Summarization, on the other hand, maintains coherent long-form memory. It preserves emotional and factual continuity over time, allowing for more natural conversations. But this comes at a cost: higher token usage and longer responses, which might not be ideal for production environments with strict token budgets.</p><h2 id="supermemory">Supermemory</h2><p>With Supermemory, you can add memory with just one line of code.</p><pre><code class="language-javascript">import OpenAI from &quot;openai&quot;

const client = new OpenAI({  
  baseUrl: &quot;https://api.supermemory.ai/v3/https://api.openai.com/v1/&quot;
}, {
  headers: {
    &quot;x-supermemory-user-id&quot;: &quot;dhravya&quot;
    }
})</code></pre><p>That&apos;s it.</p><p>Supermemory automatically ingests and manages the context, using a combination of Graphs and Vector store. Read more about it in this blog - <a href="https://supermemory.ai/blog/memory-engine/?ref=blog.supermemory.ai">https://supermemory.ai/blog/memory-engine/</a></p><h2 id="conclusion">Conclusion</h2><p>This article covered everything from how conversational memory works to implementing it in LangChain, using both trimming and summarizing, and then evaluating them for their particular use cases.</p><p>However, it starts to fall apart as complexity grows. <strong>If you&apos;re building apps and AI agents that need to remember across sessions, adapt to users, or scale without manual patchwork, Supermemory&apos;s Memory API might be a better solution for you. Check it out </strong><a href="http://supermemory.ai/?ref=blog.supermemory.ai"><strong><u>here.</u></strong></a></p><p></p>]]></content:encoded></item><item><title><![CDATA[The Wow Factor of Memory - How Flow Used Supermemory To Build Smarter, Stickier Products]]></title><description><![CDATA[<p><strong>Overview: </strong><a href="https://flowapp.so/?ref=blog.supermemory.ai" rel="noreferrer">Flow</a> is a note-taking app built around a bold vision: to create a more <em>personal, context-aware</em> writing experience powered by AI. At the heart of this mission is memory. Flow uses Supermemory as a persistent memory layer, enabling users to ask questions about any of their notes, retrieve key</p>]]></description><link>https://blog.supermemory.ai/the-wow-factor-of-memory-how-flow-used-supermemory-to-build-smarter-stickier-products/</link><guid isPermaLink="false">684dde828abc480001a22ee4</guid><category><![CDATA[Case Study]]></category><dc:creator><![CDATA[Dhravya Shah]]></dc:creator><pubDate>Sat, 14 Jun 2025 20:50:54 GMT</pubDate><media:content url="https://blog.supermemory.ai/content/images/2025/06/new.png" medium="image"/><content:encoded><![CDATA[<img src="https://blog.supermemory.ai/content/images/2025/06/new.png" alt="The Wow Factor of Memory - How Flow Used Supermemory To Build Smarter, Stickier Products"><p><strong>Overview: </strong><a href="https://flowapp.so/?ref=blog.supermemory.ai" rel="noreferrer">Flow</a> is a note-taking app built around a bold vision: to create a more <em>personal, context-aware</em> writing experience powered by AI. At the heart of this mission is memory. Flow uses Supermemory as a persistent memory layer, enabling users to ask questions about any of their notes, retrieve key information, and generate writing suggestions, even across different documents. It creates a note-taking experience that feels tailored, contextual, and deeply personal.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfme7z94i9V0GLbRuJExpfBit8upCVicwsIsIX8V-yRkoHbhSWD4PUpcGY7apBOrfSXbG7VTwg3B8M3-lPb1fySsIOMLvY1tStUfPFybIDxb75wBwNSpHSzhPdZ7f3WorCv6EA0eA?key=SykFvh_Xii5PxwueaZNPPA" class="kg-image" alt="The Wow Factor of Memory - How Flow Used Supermemory To Build Smarter, Stickier Products" loading="lazy" width="624" height="376"><figcaption><i><em class="italic" style="white-space: pre-wrap;">Flow&#x2019;s UI - Notes and Chat</em></i></figcaption></figure><p>There&apos;s a certain <strong>&quot;time to wow moment&quot;</strong> involved with every app. The higher it is, the higher users convert and use it more.<br>Supermemory brings this &quot;Time to wow moment&quot; down - When LLMs magically know everything about you, the experience is more natural, and product more delightful.<br><br>In this blog, we talk about how Flow uses supermemory to build better products, and how the &quot;Wow factor of memory&quot; is real.</p><hr><p>When Daniel set out to build Flow, he knew memory would be at the core of his product.</p><p><em>&#x201C;Having a good memory layer or like having some sort of remembering system, is what I always wanted&#x2026; memory was the only important part for the product to go public.&#x201D;</em></p><p>But actually getting that to work? Not easy.</p><p>Flow started out by integrating Mem0, a memory API that promised contextual recall. But it didn&#x2019;t deliver. Daniel faced multiple issues:</p><ol><li><strong>Existing memory tools couldn&#x2019;t handle complex, high-volume input.</strong> Flow users were pasting full documents, and existing solutions broke down when large chunks of text were involved, making it difficult to build a seamless writing experience.&#xA0;</li><li><strong>Multi-document context was essential, and missing. </strong>Flow&#x2019;s vision was to let users ask a question on one document and get an answer based on something they wrote somewhere else. That meant memory had to persist across documents, but existing systems lacked this capability. Daniel tried patching together solutions, but they just added latency, cost, and technical debt.</li><li><strong>API support was &#x201C;poor&#x201D;. </strong>No explanations needed there, really.</li></ol><p>Daniel&#x2019;s vision finally came into focus when he first stumbled upon the open-source project behind Supermemory, where people were using it to store and retrieve Twitter threads with simple prompts. That small use case sparked a bigger realization: <em>what if Flow could give users the same kind of recall, not just within one note, but across everything they&apos;d ever written?</em></p><p>He decided to integrate Supermemory. It took him one day to implement it, and instantly, things felt different.</p><ol><li><strong>Flow could finally handle massive, messy, real-world context. </strong>With Supermemory, Flow&#x2019;s customers could confidently pass large volumes of content like notes, PDFs, videos, and more, without breaking the experience. Everything just worked. This flexibility lets users bring their entire knowledge base into Flow, transforming it from a notes app to a second brain.</li><li><strong>A persistent memory layer that unlocked his product&#x2019;s purpose. </strong>Supermemory&#x2019;s memory API created a persistent, reliable memory layer that carried context across chats, across notes, across time. Now, users could reference something they wrote days or weeks ago, even in different documents, and Flow would instantly recall it. That was the &#x201C;holy shit&#x201D; moment.</li></ol><p><em>&#x201C;<strong>My users, they&apos;re like, &#x2018;Oh wow! I can dump basically everything - videos, audios, images, etc. I would pay for this just because of how much context I can put in it!</strong>&#x2019;, and Supermemory&#x2019;s been the cornerstone behind helping us achieve that.&#x201D;</em></p><ol start="3"><li><strong>It&#x2019;s the best support he&#x2019;s ever seen. </strong>Daniel was one of the earliest builders to integrate Supermemory&#x2019;s API, and that meant a lot of debugging in the early days. But he never felt alone. From schema design to integration questions, the Supermemory team was in the loop every day, helping him ship a reliable experience.</li></ol><p><em>&#x201C;Initially, it was a fair bit of debugging and calls, but with Supermemory, the support is the best I&#x2019;ve ever seen.&#x201D;</em></p><figure class="kg-card kg-embed-card kg-card-hascaption"><iframe src="https://player.vimeo.com/video/1093603806?app_id=122963" width="426" height="240" frameborder="0" allow="autoplay; fullscreen; picture-in-picture; clipboard-write; encrypted-media; web-share" title="video-excerpt"></iframe><figcaption><p dir="ltr"><span style="white-space: pre-wrap;">Daniel&apos;s thoughts on how Supermemory has helped Flow</span></p></figcaption></figure><p>With Supermemory, Flow became a better product and a better business. Daniel saw results both in the backend and in the way users engaged with Flow.</p><ul><li><strong>User retention hit 40%.</strong> Daniel&#x2019;s favorite metric is &#x201C;Users coming back for 3+ days consecutively&#x201D;. Supermemory&#x2019;s context layer helped make Flow feel sticky, and users kept returning to <em>continue conversations</em>. That sense of continuity drove Flow&#x2019;s retention to 40%.</li><li><strong>Lower token usage = significantly reduced infrastructure cost. </strong>With Supermemory handling persistent context, token usage dropped dramatically. Users stopped pasting long documents and context, and the AI stopped reprocessing what it already knew. For Daniel, that meant a noticeable dip in infra bills.</li><li><strong>60% reduction in backend requests. </strong>Instead of stringing together five different API calls for memory retrieval, user context, document fetching, embeddings, and response generation, Flow just needs to call Supermemory&#x2019;s API, and everything gets done.</li></ul><p>The day Daniel announced the <strong>Flow x Supermemory</strong> partnership, signups jumped by 50%. But, perhaps, the most meaningful outcome wasn&#x2019;t a number.&#xA0;</p><p>It was a shift in Daniel&#x2019;s confidence.&#xA0;</p><p>Supermemory made Flow feel robust, smart, and valuable, giving Daniel the confidence to start thinking about monetization.</p><p>&#x201C;I&#x2019;m considering changing the model to pay-first <em>just</em> because of how good the memory is.&#x201D;</p><p>And maybe, that is the <strong><em>wow factor of memory.</em></strong> It benefits everyone involved - customers get a much better UX, your developers aren&#x2019;t left scratching their heads, and founders can create real value.</p><p>Daniel didn&#x2019;t have to change his product to fit the limits of memory. Supermemory finally met the bar he had always had in mind.</p><p>Want to build like Flow? Check out <a href="http://docs.supermemory.ai/?ref=blog.supermemory.ai"><u>Supermemory&#x2019;s docs</u></a> and start building with memory today. </p>]]></content:encoded></item><item><title><![CDATA[The UX and technicalities of awesome MCPs]]></title><description><![CDATA[<p>Last month, we launched the Supermemory MCP, mostly to test our own infrastructure and get some initial traction. It blew up.</p><p>To my absolute surprise, the initial launch itself got <strong><em>half a million impressions (!!!)</em></strong>. Then, we launched and got <a href="https://www.producthunt.com/products/supermemory?launch=universal-memory-mcp&amp;ref=blog.supermemory.ai" rel="noreferrer">#2 on ProductHunt</a> too. Seems like people absolutely <strong>loved</strong> the concept</p>]]></description><link>https://blog.supermemory.ai/the-ux-and-technicalities-of-awesome-mcps/</link><guid isPermaLink="false">6844c510ac6a62000160ffb7</guid><category><![CDATA[Engineering]]></category><dc:creator><![CDATA[Dhravya Shah]]></dc:creator><pubDate>Sun, 08 Jun 2025 05:13:09 GMT</pubDate><media:content url="https://blog.supermemory.ai/content/images/2025/06/nice.png" medium="image"/><content:encoded><![CDATA[<img src="https://blog.supermemory.ai/content/images/2025/06/nice.png" alt="The UX and technicalities of awesome MCPs"><p>Last month, we launched the Supermemory MCP, mostly to test our own infrastructure and get some initial traction. It blew up.</p><p>To my absolute surprise, the initial launch itself got <strong><em>half a million impressions (!!!)</em></strong>. Then, we launched and got <a href="https://www.producthunt.com/products/supermemory?launch=universal-memory-mcp&amp;ref=blog.supermemory.ai" rel="noreferrer">#2 on ProductHunt</a> too. Seems like people absolutely <strong>loved</strong> the concept and it was a massively successful side-project. In this blog, we&apos;ll talk about the reasons why this MCP did really well - and learnings along the way.</p><figure class="kg-card kg-embed-card"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">i made a universal memory mcp.<br><br>- carry your memories to every llm you use (not just chatgpt) - works with windsurf, claude, etc. <br>- almost unlimited memories<br>- no login or paywall required<br>- one single command to install<br><br>try it out now - instruction and demo below &#x1F447; <a href="https://t.co/k3lMOdqVSE?ref=blog.supermemory.ai">pic.twitter.com/k3lMOdqVSE</a></p>&#x2014; Dhravya Shah (@DhravyaShah) <a href="https://twitter.com/DhravyaShah/status/1912544775536066772?ref_src=twsrc%5Etfw&amp;ref=blog.supermemory.ai">April 16, 2025</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></figure><h2 id="what-did-we-do-differently">What did we do differently?</h2><p>If you look at any popular MCP server right now, the ultimate realizations are:</p><ul><li>MCPs are only used by developers right now.</li><li>Authentication always seems to get in the way</li><li>It&apos;s a terrible pain to install MCPs today.</li><li>MCP clients always want to behave differently.</li><li>Long-standing connections (SSE) keeps breaking.</li><li>There&apos;s a huge usecase problem. </li></ul><p>To get through each of these hurdles, we chose a set of user experience and technical decisions, to ensure that the MCP is really <em>newbie proof</em> (from both a user perspective and not-well-done MCP clients)</p><h2 id="1-choose-sse-instead-of-stdio">1) Choose SSE instead of STDIO</h2><p>At the time of building the MCP, most servers were using STDIO. But, the reality is, my mom could <em>never ever </em>use something that needs a terminal command to run.<br>I realized that there will be more and more MCP clients that would add SSE support, especially on Web-based clients like Claude.ai. In April there were barely any famous web-based clients (none that I can remember), so this was a huge bet.</p><p>My previous MCP servers like <a href="https://github.com/supermemoryai/apple-mcp?ref=blog.supermemory.ai" rel="noreferrer">apple-mcp</a>, which also got popularity, were completely local-only. This felt like a good bet to make because MCP SSE servers can be made to work over STDIO as well.</p><h2 id="2-no-auth-at-all">2) No auth at all. </h2><p>Next step was authentication: There&apos;s a lot of fuss in the &quot;Auth with MCP&quot; space, and in my opinion, none of them offer a good user experience. The main issue in my opinion is, again, the variability of how the clients work.</p><p>So, we decided to nuke auth completely - and generate a unique URL for every user that comes to the website. In fact, we made it so easy that all that the user has to do is <a href="https://mcp.supermemory.ai/?ref=blog.supermemory.ai">https://mcp.supermemory.ai</a>, and the ONLY thing that they see on the screen is a huge URL to copy.</p><p>From a technical perspective, we use react-router loader to:</p><ol><li>Check if <code>userId</code> is in session</li><li>If it is, query the database for that userId</li><li>Else, generate a random ID</li><li>Commit it to the session</li></ol><pre><code class="language-Typescript">export async function loader({ request, context }: Route.LoaderArgs) {
    const cookies = request.headers.get(&quot;Cookie&quot;)
    const session = await getSession(cookies)

    if (session.has(&quot;userId&quot;)) {
        const userId = session.get(&quot;userId&quot;)!
        // ... get user memories
        return data({
            message: &quot;Welcome back!&quot;,
            userId,
            memories,
        })
    }

    session.set(&quot;userId&quot;, nanoid())
    const userId = session.get(&quot;userId&quot;)!

    return data(
        ...
        {
            headers: {
                &quot;Set-Cookie&quot;: await commitSession(session, {
                    expires: new Date(&quot;9999-12-31&quot;),
                }),
            },
        },
    )
}</code></pre><p>There are obviously some huge drawbacks to this approach:</p><ul><li>If the user clears the cookies, they will lose their memories</li><li>If anyone finds out the URL for the user, they might be able to access it.</li></ul><p>These drawbacks were fine for us, because the user would &quot;persist&quot; their URL by Copy+pasting it to an MCP client anyways. Which means they would always store it <em>somewhere</em>. <br>And the url path parameter acted as an API key. <br><br>This also means that the MCP server itself would have to handle the userId on a <em>dynamic</em> url.<br>But we&apos;ll get there.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.supermemory.ai/content/images/2025/06/image-2.png" class="kg-image" alt="The UX and technicalities of awesome MCPs" loading="lazy" width="2000" height="1156" srcset="https://blog.supermemory.ai/content/images/size/w600/2025/06/image-2.png 600w, https://blog.supermemory.ai/content/images/size/w1000/2025/06/image-2.png 1000w, https://blog.supermemory.ai/content/images/size/w1600/2025/06/image-2.png 1600w, https://blog.supermemory.ai/content/images/2025/06/image-2.png 2042w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">How our authentication works</span></figcaption></figure><p>This did make things a <em>little</em> challenging - like how do you handle different client connections again on different domains?<br>Most MCP server libraries are built for one URL only- so we &quot;wrapped&quot; our functions into a <code>createSupermemory</code> &quot;meta mcp&quot; so, technically, a dynamic server is generated for every single unique user.</p><pre><code>        server.get(&quot;/sse&quot;, async (c) =&gt; {
            const userId = c.get(&quot;userId&quot;)

            return streamSSE(c, async (stream) =&gt; {
                this.transport?.connectWithStream(stream)

                await bridge({
                    mcp: muppet(createSuperMemory(userId, c.env), {
                        name: &quot;Supermemory MCP&quot;,
                        version: &quot;1.0.0&quot;,
                    }),
                    transport: c.env.transport,
                    // logger: logger,
                })
            })
        })</code></pre><p>This does not really have any performance implications, but just better UX because the URL will look like <code>mcp.supermemory.ai/userid/sse</code> and <code>/userid/messages</code></p><p>Beautiful, as things should be.</p><h2 id="3-made-an-installation-cli-to-make-it-dead-simple">3) Made an installation CLI to make it dead simple.</h2><p>Over a quick afternoon, I vibecoded this quick tool to install MCPs with a single CLI command. </p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/supermemoryai/install-mcp?ref=blog.supermemory.ai"><div class="kg-bookmark-content"><div class="kg-bookmark-title">GitHub - supermemoryai/install-mcp: A simple CLI to install MCP servers into any client</div><div class="kg-bookmark-description">A simple CLI to install MCP servers into any client - supermemoryai/install-mcp</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://blog.supermemory.ai/content/images/icon/pinned-octocat-093da3e6fa40.svg" alt="The UX and technicalities of awesome MCPs"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">supermemoryai</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://blog.supermemory.ai/content/images/thumbnail/install-mcp" alt="The UX and technicalities of awesome MCPs" onerror="this.style.display = &apos;none&apos;"></div></a></figure><p>This meant that I could provide a CLI command for whichever MCP client had a local-only config.<br>Interestingly, a lot of cool people contributed to the CLI like <a href="https://nmn.gl/?ref=blog.supermemory.ai">Namanya</a>, <a href="https://kentcdodds.com/?ref=blog.supermemory.ai">Kent C. Dodds</a>, and more, to add more clients, and make the installation UX even better! </p><figure class="kg-card kg-image-card"><img src="https://blog.supermemory.ai/content/images/2025/06/image-3.png" class="kg-image" alt="The UX and technicalities of awesome MCPs" loading="lazy" width="1522" height="832" srcset="https://blog.supermemory.ai/content/images/size/w600/2025/06/image-3.png 600w, https://blog.supermemory.ai/content/images/size/w1000/2025/06/image-3.png 1000w, https://blog.supermemory.ai/content/images/2025/06/image-3.png 1522w" sizes="(min-width: 720px) 720px"></figure><p>Users no longer had to &quot;Edit JSON Config&quot; files to install a simple MCP server.<br>It was always Copy+paste. Either the URL to direct SSE connections, or the CLI command.</p><p>Simply beautiful. As I like it.</p><figure class="kg-card kg-image-card"><img src="https://blog.supermemory.ai/content/images/2025/06/image-4.png" class="kg-image" alt="The UX and technicalities of awesome MCPs" loading="lazy" width="2000" height="663" srcset="https://blog.supermemory.ai/content/images/size/w600/2025/06/image-4.png 600w, https://blog.supermemory.ai/content/images/size/w1000/2025/06/image-4.png 1000w, https://blog.supermemory.ai/content/images/size/w1600/2025/06/image-4.png 1600w, https://blog.supermemory.ai/content/images/2025/06/image-4.png 2148w" sizes="(min-width: 720px) 720px"></figure><p><br>This is what I mean by good UX. You need to go out of your way to give a good user experience. Software is flaky and broken, so the easier you make it for people to <em>not</em> fuck up, the better.<br><strong>Supermemory does all this because we care. Even for our non-revenue making projects :) </strong></p><h2 id="4-provide-prompts">4) Provide prompts.</h2><p>The supermemory MCP also provides a prompt built-in for clients that really, really don&apos;t want to listen. <br>For example, when we launched, the MCP didn&apos;t work on Claude because it really wanted to use a resource instead of a tool. Which meant that we had to write an unhinged prompt &#x1F606;</p><figure class="kg-card kg-image-card"><img src="https://blog.supermemory.ai/content/images/2025/06/image-5.png" class="kg-image" alt="The UX and technicalities of awesome MCPs" loading="lazy" width="892" height="460" srcset="https://blog.supermemory.ai/content/images/size/w600/2025/06/image-5.png 600w, https://blog.supermemory.ai/content/images/2025/06/image-5.png 892w" sizes="(min-width: 720px) 720px"></figure><p>Anyways, by prompts, I mean actual MCP protocol &quot;prompts&quot;. So that users can actually click on &quot;use Supermemory Prompt&quot; whenever they explicitly need to use memory </p><figure class="kg-card kg-image-card"><img src="https://blog.supermemory.ai/content/images/2025/06/image-6.png" class="kg-image" alt="The UX and technicalities of awesome MCPs" loading="lazy" width="1298" height="658" srcset="https://blog.supermemory.ai/content/images/size/w600/2025/06/image-6.png 600w, https://blog.supermemory.ai/content/images/size/w1000/2025/06/image-6.png 1000w, https://blog.supermemory.ai/content/images/2025/06/image-6.png 1298w" sizes="(min-width: 720px) 720px"></figure><p>This is not perfect, but all we could do to make MCP clients unfuck themselves :) </p><h2 id="5-host-your-mcp-servers-on-cloudflare">5) Host your MCP servers on Cloudflare.</h2><p>SSE is a weird protocol. You have to have an <strong><em>extremely</em></strong> long-running connection for the messages to actually work. Memory absolutely requires this because clients may send one memory in 30 minutes, or 6 hours, you never know. </p><p>Fortunately, the smart folks over at Cloudflare have been working on a way to run &quot;Durable&quot; objects / connections for years now. And MCP happens to be the perfect use case for something like this. Initially when we launched the MCP, users constantly complained about &quot;Connection lost&quot; and stuff like that.<br>I was also lucky to work on this tech, during my time at Cloudflare before starting this company. So, I trust the <a href="https://agents.cloudflare.com/?ref=blog.supermemory.ai" rel="noreferrer">Agents SDK</a> and genuinely think that there&apos;s no better platform to put MCPs on.</p><p>Worse still, most platforms charge for Execution time (total time running compute, including waits) - not CPU time (only time actually using CPU), but for MCPs you almost always have to just &quot;wait&quot; for messages to do anything with it. <br>Again, fortunately Cloudflare only charges for CPU time which means I can have unimaginably long connections, and that <em>definitely</em> happened.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.supermemory.ai/content/images/2025/06/image-7.png" class="kg-image" alt="The UX and technicalities of awesome MCPs" loading="lazy" width="1148" height="1508" srcset="https://blog.supermemory.ai/content/images/size/w600/2025/06/image-7.png 600w, https://blog.supermemory.ai/content/images/size/w1000/2025/06/image-7.png 1000w, https://blog.supermemory.ai/content/images/2025/06/image-7.png 1148w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">While the actual CPU time was a few milliseconds only (mostly MCP transport stuff), the total connection times were MILLIONS of milliseconds each.</span></figcaption></figure><p>Bonkers &#x1F605;</p><h2 id="6-use-cases">6) Use-cases.</h2><p>On a more non-technical side, I believe that whenever you sell something, lead with examples. All the demos of the supermemory MCP were things that <em>I</em> personally use it for, every day. In fact, my personal URL has 400+ memories about me already - projects I&apos;m building, my preferences, my girlfriend&apos;s name, and a lot more. &#x1F603;<br>I made &amp; posted videos of me actually using the product, and whenever something interesting happens, I would post it on Twitter as well!</p><h1 id="the-result-growth">The result: growth.</h1><p>It turns out, more MCP clients started supporting direct SSE connections - include Claude, <a href="https://x.com/DhravyaShah/status/1930464949677240703?ref=blog.supermemory.ai">https://x.com/DhravyaShah/status/1930464949677240703</a> - And instantly after Claude launched integrations, people started looking for interesting integrations to add. <br>Supermemory was one of them, on day 1.</p><p>Today, it might be the most used memory MCP - but I can&apos;t tell for sure &#x1F605;<br>But all this could not be possible without all the background work...</p><figure class="kg-card kg-embed-card"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">supermemory MCP, which started off as just a small, random sideproject, <br>built as a &quot;customer&quot; of the supermemory API, <br>...is now used by 63,000 people all around the world. DAILY.<br><br>It might as well be the most used memory MCP in the market right now.<br>How? Blog soon. <a href="https://t.co/3IlVLEztna?ref=blog.supermemory.ai">pic.twitter.com/3IlVLEztna</a></p>&#x2014; Dhravya Shah (@DhravyaShah) <a href="https://twitter.com/DhravyaShah/status/1931484491048169883?ref_src=twsrc%5Etfw&amp;ref=blog.supermemory.ai">June 7, 2025</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></figure><h2 id="the-background-work">The background work</h2><p>We were able to make &amp; ship supermemory MCP in ~5 hours of actual work time. Just look at the repository - it&apos;s the simplest thing you&apos;ll see in your life. It&apos;s a react router app that makes fetch calls to the Supermemory API.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/supermemoryai/supermemory-mcp?ref=blog.supermemory.ai"><div class="kg-bookmark-content"><div class="kg-bookmark-title">GitHub - supermemoryai/supermemory-mcp: Your memories are in ChatGPT... But nowhere else. Universal Memory MCP makes your memories available to every single LLM. No logins or paywall. One command to set it up.</div><div class="kg-bookmark-description">Your memories are in ChatGPT... But nowhere else. Universal Memory MCP makes your memories available to every single LLM. No logins or paywall. One command to set it up. - supermemoryai/supermemory&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://blog.supermemory.ai/content/images/icon/pinned-octocat-093da3e6fa40-1.svg" alt="The UX and technicalities of awesome MCPs"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">supermemoryai</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://blog.supermemory.ai/content/images/thumbnail/supermemory-mcp" alt="The UX and technicalities of awesome MCPs" onerror="this.style.display = &apos;none&apos;"></div></a></figure><p>The MCP was made as a &quot;Customer&quot; of the base API, that we have been working for months on. If you want to read more about that, we wrote a blog about it too! </p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://supermemory.ai/blog/memory-engine/?ref=blog.supermemory.ai"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Architecting a memory engine inspired by the human brain</div><div class="kg-bookmark-description">Language is at the heart of intelligence, but what truly powers meaningful interaction is memory &#x2014; the ability to accumulate, recall, and contextualize information over time. Large Language Models (LLMs) have mastered language, but memory remains their Achilles&#x2019; heel. Every leap in context window size is quickly outpaced by real-world demands:</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://blog.supermemory.ai/content/images/icon/SuperM_LinkedIn-Github-Twitter_ProfilePicture--1--1.png" alt="The UX and technicalities of awesome MCPs"><span class="kg-bookmark-author">supermemory - Blog</span><span class="kg-bookmark-publisher">Dhravya Shah</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://blog.supermemory.ai/content/images/thumbnail/blog-cover.png" alt="The UX and technicalities of awesome MCPs" onerror="this.style.display = &apos;none&apos;"></div></a></figure><p>But the TL;DR is that we&apos;ve been obsessing over memory for a while now, topping almost every benchmark and objectively the best way to build chat apps / agents with memory today</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.supermemory.ai/content/images/2025/06/image-9.png" class="kg-image" alt="The UX and technicalities of awesome MCPs" loading="lazy" width="1622" height="1250" srcset="https://blog.supermemory.ai/content/images/size/w600/2025/06/image-9.png 600w, https://blog.supermemory.ai/content/images/size/w1000/2025/06/image-9.png 1000w, https://blog.supermemory.ai/content/images/size/w1600/2025/06/image-9.png 1600w, https://blog.supermemory.ai/content/images/2025/06/image-9.png 1622w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Supermemory vs next best provider, in the LOCOMO benchmark.</span></figcaption></figure><p>There&apos;s a lot to build, and we&apos;re just getting started. We&apos;ll continue obsessing over user and developer experience, and continue on our vision of building the best memory engine on the planet.</p><p>Star the <a href="https://github.com/supermemoryai/supermemory-mcp?ref=blog.supermemory.ai">MCP repository</a>, start <a href="https://docs.supermemory.ai/?ref=blog.supermemory.ai" rel="noreferrer">using supermemory</a> and leave your thoughts on Twitter (tag <a href="https://x.com/supermemoryai?ref=blog.supermemory.ai" rel="noreferrer">@supermemoryai</a>) </p>]]></content:encoded></item><item><title><![CDATA[Architecting a memory engine inspired by the human brain]]></title><description><![CDATA[<p></p><p>Language is at the heart of intelligence, but what truly powers meaningful interaction is memory &#x2014; the ability to accumulate, recall, and contextualize information over time. </p><p>Large Language Models (LLMs) have mastered language, but memory remains their Achilles&#x2019; heel. Every leap in context window size is quickly outpaced by</p>]]></description><link>https://blog.supermemory.ai/memory-engine/</link><guid isPermaLink="false">6841b48e57dca90001167567</guid><category><![CDATA[Engineering]]></category><dc:creator><![CDATA[Dhravya Shah]]></dc:creator><pubDate>Thu, 05 Jun 2025 19:20:31 GMT</pubDate><media:content url="https://blog.supermemory.ai/content/images/2025/06/blog-cover.png" medium="image"/><content:encoded><![CDATA[<img src="https://blog.supermemory.ai/content/images/2025/06/blog-cover.png" alt="Architecting a memory engine inspired by the human brain"><p></p><p>Language is at the heart of intelligence, but what truly powers meaningful interaction is memory &#x2014; the ability to accumulate, recall, and contextualize information over time. </p><p>Large Language Models (LLMs) have mastered language, but memory remains their Achilles&#x2019; heel. Every leap in context window size is quickly outpaced by real-world demands: users upload massive documents, have lengthy conversations, and expect seamless recall of preferences and history. The result? LLMs that forget, hallucinate, and frustrate users to the point of wanting to just start over with a clean slate.</p><p>At Supermemory, the idea wasn&#x2019;t always to help models remember better. It started off as a way to make *you* remember your bookmarks. <br>Then, it was a cost and accuracy efficient way to easily implement RAG in your product. <br>The more experience we had with this pain point, the more we were led to tackle context, and that&#x2019;s what it currently stands as &#x2014; a drop-in memory layer for your LLMs that is extremely scalable, with sub-400ms latency and enterprise-grade reliability.</p><p>Long context does not really solve this either - as found in popular <a href="https://arxiv.org/abs/2502.05167?ref=blog.supermemory.ai" rel="noreferrer">papers </a> and proven time and time again in real-world use-cases.<br><br>But why?</p><h3 id="what-makes-memory-for-llms-so-hard">What Makes Memory for LLMs So Hard?</h3><p>Building a memory layer for AI isn&#x2019;t just about one more layer of data storage. You need to optimize for five uncompromising requirements:</p><ul><li><strong>High Recall &amp; Precision:</strong>&#xA0;Always surface the right information &#x2014; even across years of chat history or thousands of documents, and filtering out irrelevant, outdated, or noisy data to keep responses accurate.</li><li><strong>Low Latency:</strong>&#xA0;Memory shouldn&#x2019;t slow you down. It needs to work fast, especially at scale.<br><br>This is particularly a challenge - because all current solutions for memory are <em>not</em> built for scale.<ul><li><em>Vector Databases</em>: Either get too expensive, or too slow as they grow. There are new, proprietary, server-less options now - but we&apos;ll get into this shortly</li><li><em>Graph</em>: To add every node, or for every query, one typically has to traverse factors or magnitudes more edges than nodes.</li><li><em>Key-value:</em> The entire KV pair has to fit inside the context length of models. Which moves the problem from one context length to another.</li></ul></li><li><strong>Ease of Integration:</strong>&#xA0;Developers need APIs and SDKs that require minimal changes to integrate &#x2014; not weeks of onboarding or complex migrations.<br><br>Managing embeddings, migrating between them, doing research for new improvements, etc. is usually not the primary business goal for apps needing memory. Lots of engineering hours are wasted.</li><li><strong>Semantic &amp; Non-Literal Queries:</strong>&#xA0;Memory must understand nuances, metaphors, and ambiguity &#x2014; not just literal matches. Humans don&#x2019;t search against a corpus of data with search terms they kinda know.<br><br>What is &quot;Non-literal match&quot;, here&apos;s an example</li></ul><figure class="kg-card kg-image-card"><img src="https://blog.supermemory.ai/content/images/2025/06/image.png" class="kg-image" alt="Architecting a memory engine inspired by the human brain" loading="lazy" width="1504" height="1030" srcset="https://blog.supermemory.ai/content/images/size/w600/2025/06/image.png 600w, https://blog.supermemory.ai/content/images/size/w1000/2025/06/image.png 1000w, https://blog.supermemory.ai/content/images/2025/06/image.png 1504w" sizes="(min-width: 720px) 720px"></figure><p>Most solutions optimize for some of these, but fall short on others &#x2014; especially when it comes to semantic understanding and scaling to billions of data points - For example, a user asking questions about all their internet life may ask more &#x201C;general&#x201D; questions that require knowledge of the entire dataset, not just ability to fuzzy search keywords.</p><h2 id="the-supermemory-approach-human-memory-at-scale">The Supermemory Approach: Human Memory at Scale</h2><p>Your brain doesn&apos;t store everything perfectly as you see it&#x2014;and that&apos;s actually a feature, not a bug. It forgets the mundane, emphasizes what you&apos;ve used recently, and rewrites memories based on current context. Our architecture works the same way, but engineered for AI at scale.</p><p><strong>Smart Forgetting &amp; Decay</strong></p><p>Just like you naturally forget where you parked three weeks ago but remember yesterday&apos;s important meeting, our system applies intelligent decay. Less relevant information gradually fades while important, frequently-accessed content stays sharp. No more drowning in irrelevant context.</p><p><strong>Recency &amp; Relevance Bias</strong></p><p>That thing you just talked about? It gets priority. That document you reference constantly? It stays top-of-mind. We mirror your brain&apos;s natural tendency to surface what&apos;s actually useful right now, not just what&apos;s technically &quot;relevant&quot; to a search query.</p><p><strong>Context Rewriting &amp; Broad Connections</strong></p><p>Your brain doesn&apos;t just file away facts&#x2014;it rewrites them based on new experiences and draws unexpected connections. Our system does the same, continuously updating summaries and finding links between seemingly unrelated information. That random insight from last month might be exactly what you need for today&apos;s problem.</p><p><strong>Hierarchical Memory Layers</strong></p><p>Like how you have working memory, short-term memory, and long-term storage, we use Cloudflare&apos;s infrastructure to create memory layers that match how you actually think. Hot, recent stuff stays instantly accessible (we personally use <a href="https://developers.cloudflare.com/kv/?ref=blog.supermemory.ai" rel="noreferrer">KV</a>). Deeper memories get retrieved when you need them, not before.</p><h2 id="building-on-top-of-the-engine">Building on top of the engine</h2><p>Supermemory isn&#x2019;t just another vector database or RAG toolkit. It&#x2019;s a universal memory layer that gives your LLMs the power of infinite context, with near-instant plug-and-play integration. Add our endpoint to your existing AI provider, plug in your API key, and boom &#x2014; you&#x2019;re done.</p><p>On top of this engine, we&apos;ve been building some interesting products and experiences -</p><ul><li><strong>Memory as a service: </strong>Storing and querying multimodal data, at scale, with support for external connectors and sync with Google Drive, Notion, OneDrive, etc. It&apos;s a few API calls - just /add, /connect, /search <a href="https://docs.supermemory.ai/api-reference/manage-memories/add-memory?ref=blog.supermemory.ai">https://docs.supermemory.ai/api-reference/manage-memories/add-memory</a> </li></ul><figure class="kg-card kg-image-card"><img src="https://blog.supermemory.ai/content/images/2025/06/SM-2-gradient.png" class="kg-image" alt="Architecting a memory engine inspired by the human brain" loading="lazy" width="1280" height="720" srcset="https://blog.supermemory.ai/content/images/size/w600/2025/06/SM-2-gradient.png 600w, https://blog.supermemory.ai/content/images/size/w1000/2025/06/SM-2-gradient.png 1000w, https://blog.supermemory.ai/content/images/2025/06/SM-2-gradient.png 1280w" sizes="(min-width: 720px) 720px"></figure><ul><li><strong>The Supermemory MCP: </strong> Model-interoperable MCP server that lets users carry their memories, and chats, through LLM apps without losing context. This is actually built on top of our own Memory as a service</li></ul><figure class="kg-card kg-embed-card"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">&#x1F4A5; Supermemory MCP can now be used on Claude dot ai!<br><br>Now you can ideate on chat apps, <br>add things to your universal memory <br>...and then use them in your code editors.<br><br>it&apos;s YOUR portable memory. <a href="https://t.co/4ju054uQz4?ref=blog.supermemory.ai">https://t.co/4ju054uQz4</a></p>&#x2014; supermemory (@Supermemoryai) <a href="https://twitter.com/Supermemoryai/status/1930465720325771420?ref_src=twsrc%5Etfw&amp;ref=blog.supermemory.ai">June 5, 2025</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></figure><p></p><p>And our latest launch, called Infinite Chat API, manages memories inline with the conversation history to only send what&apos;s needed to the model providers. Leading to less token usage, cost savings, better latencies and better quality responses. <br>You heard that right. You can use it <em>today</em> with just <a href="https://docs.supermemory.ai/infinite-chat?ref=blog.supermemory.ai">one line of code!</a></p><figure class="kg-card kg-embed-card"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">Introducing: Infinite Chat API &#x1F4A5;<br><br>Extend the context length of ANY model <br>...while saving 90% of your tokens and cost AND improving performance!<br><br>One single line to switch. Available to use now <a href="https://t.co/hDfAuu2skD?ref=blog.supermemory.ai">pic.twitter.com/hDfAuu2skD</a></p>&#x2014; supermemory (@Supermemoryai) <a href="https://twitter.com/Supermemoryai/status/1923122703009186217?ref_src=twsrc%5Etfw&amp;ref=blog.supermemory.ai">May 15, 2025</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></figure><p>We&apos;ve been constantly improving this with our latest benchmarks, work and progress in the field.</p><figure class="kg-card kg-image-card"><img src="https://blog.supermemory.ai/content/images/2025/06/image-1.png" class="kg-image" alt="Architecting a memory engine inspired by the human brain" loading="lazy" width="2000" height="1333" srcset="https://blog.supermemory.ai/content/images/size/w600/2025/06/image-1.png 600w, https://blog.supermemory.ai/content/images/size/w1000/2025/06/image-1.png 1000w, https://blog.supermemory.ai/content/images/size/w1600/2025/06/image-1.png 1600w, https://blog.supermemory.ai/content/images/2025/06/image-1.png 2400w" sizes="(min-width: 720px) 720px"></figure><p></p><p></p><p>Memory is a huge missing piece on the road to AGI. With Supermemory, you can finally build products that remember, reason, and respond like never before.</p><p>If you believe in the mission, <a href="https://x.com/i/jobs/1928194391946186862?ref=blog.supermemory.ai">we&apos;re hiring</a>. If you want better memory for your LLMs and apps - you can use all these products <em>today</em>. Check out our <a href="https://docs.supermemory.ai/?ref=blog.supermemory.ai">docs</a>, <a href="https://mcp.supermemory.ai/?ref=blog.supermemory.ai" rel="noreferrer">MCP</a>, and get an API key on our <a href="https://console.supermemory.ai/?ref=blog.supermemory.ai" rel="noreferrer">dashboard</a></p>]]></content:encoded></item></channel></rss>